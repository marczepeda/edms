'''
Module: fastq.py
Author: Marc Zepeda
Created: 2024-08-05
Description: Fastq processing and analysis 

Usage:
[Supporting methods for sequences]
- fuzzy_substring_search: retuns a dataframe containing all substrings in 'text' that resemble 'pattern' within the Levenshtein 'max_distance'.

[Input/Output]
- revcom_fastqs(): write reverse complement of fastqs to a new directory
- unzip_fastqs(): Unzip gzipped fastqs and write to a new directory
- comb_fastqs(): Combines one or more (un)compressed fastqs files into a single (un)compressed fastq file

[Nanopore]
- savemoney_samples(): create savemoney samples.csv for nanopore mixed WPS

[Quantify epeg/ngRNA or Signature abundance]
- [Motif search: mU6, ..., & target sequence flanks]
    - count_motif(): returns a dataframe with the sequence motif location per read and abundance for every fastq file in a directory
    - plot_motif(): generate plots highlighting motif mismatches, locations, and sequences
- [Region/read alignments: spacer, ..., & ngRNA-epegRNA]
    - mismatch_alignments(): compute & save mismatch number and position per alignment; enables checkpoints
    - perform_alignments(): perform alignments on fastq reads using PairwiseAligner and compute mismatches using mismatch_alignments()
    - plot_alignments(): generate line & distribution plots from fastq alignments dictionary
    - count_region(): align read region from fastq directory to the annotated library with mismatches; plot and return fastq alignments dictionary
    - count_alignments(): align reads from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary
    - plot_paired(): generate stacked bar plots from paired_regions() dataframe
    - paired_regions(): quantify, plot, & return (un)paired regions that aligned to the annotated library
- [Signature]
    - count_signatures(): generate signatures from fastq read region alignments to WT sequence; count signatures, plot and return fastq signatures dataframe

[Quantify edit outcomes]
- trim_filter(): trim and filter fastq sequence based on quality scores
- get_fastqs(): get fastq files from directory and store records in dataframes in a dictionary
- region(): gets DNA and AA sequence for records within flanks
- [Supporting methods for genotype()]
    - find_AA_edits(): find amino acid edits compared to wildtype sequence
    - trim(): trim the sequence to a multiple of 3.
    - format_alignment(): formats two sequences for alignment display & return the middle.
    - find_indel(): aligns two sequences and returns the indel edit.
- genotype(): assign genotypes to sequence records
- outcomes(): returns edit count & fraction per sample (tidy format)
- genotyping(): quantify edit outcomes workflow
- abundances(): quantify desired edits count & fraction per sample
- editing_per_library(): Determine editing relative library abundance

[UMI methods]
- extract_umis(): extract UMIs using umi_tools
- trim_motifs(): trimming motifs with cutadapt
- make_sams(): generates alignments saved as a SAM files using bowtie2
- make_bams(): converts SAM files to BAM files using samtools
- bam_umi_tags(): copy UMI in read ID to RX tag in BAM files using fgbio
- group_umis(): group BAM files by UMI using fgbio
- consensus_umis(): generate consensus sequences from grouped UMIs using fgbio
- bam_to_fastq(): convert BAM files to fastq files using samtools
# Note: Consider adding fgbio FilterConsensusReads & fgbio ReviewConsensusVariants

[Supporting methods for plots]
- aa_props: dictionary of AA properties with citations (Generated by ChatGPT)
- edit_change(): split edit column to before, after, and amino acid number; determine aa property changes
- make_change_table(): generate HTML table of amino acid property changes
- make_label_info(): include additional info for labels including aa property changes and patient information
- add_label_info(): AA properties for conservation (change to); plus additional info from cBioPortal, UniProt, PhosphoSitePlus, PDB if specified

[Plot methods]
- cat: creates categorical graphs
- stack(): creates stacked bar plot
- vol(): creates volcano plot
- torn(): creates tornado plot
- corr(): creates correlation plot
- heat(): creates heatmap plot
'''

# Import packages
from Bio.Seq import Seq
from Bio import SeqIO, SeqUtils
from Bio.Align import PairwiseAligner
from Bio.SeqRecord import SeqRecord
import gzip
import os
import re
import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.cm as cm
import matplotlib.colors as mcolors
from matplotlib.ticker import MaxNLocator
import mpld3
from scipy.stats import ttest_ind
import Levenshtein
from typing import Literal
import math
import subprocess
import itertools

from ..bio.signature import parse_signature_literal, signature_from_alignment, expand_signature_units, is_reference_match_with_n_extra_nt_or_less
from ..gen import io, tidy as t, plot as p, stat as st
from ..data import uniprot, pdb
from ..utils import memory_timer, mkdir, load_resource_csv
from .. import config

# Supporting methods for sequences
def fuzzy_substring_search(text: str, pattern: str, max_distance: int):
    """
    fuzzy_substring_search: retuns a dataframe containing all substrings in 'text' that resemble 'pattern' within the Levenshtein 'max_distance'.
    
    Parameters:
    text (str): text to search within
    pattern (str): text to search for
    max_distance (int): maximum Levenshtein distance of 'pattern' in text substring

    Dependencies: Levenshtein, pandas
    """
    # Initialize lists that will be stored in the output dataframe
    windows = []
    distances = []
    starts_i = []
    ends_i = []

    # Iterate through 'text' using windows with equal length to 'pattern'
    len_pat = len(pattern)
    for i in range(len(text) - len_pat + 1):
        window = text[i:i + len_pat]
        distance = Levenshtein.distance(window, pattern)
        if distance <= max_distance: # Save windows that resemble 'pattern' within the Levenshtein 'max_distance'
            windows.append(window)
            distances.append(distance)
            starts_i.append(int(i))
            ends_i.append(int(i + len_pat))

    # Return final dataframe
    return pd.DataFrame({'pattern': [pattern]*len(windows),
                         'window': windows,
                         'distance': distances,
                         'start_i': starts_i,
                         'end_i': ends_i})

# Nanopore
def savemoney(fastq_dir: str='./fastq', fasta_dir: str='./fasta', pt: str='.', 
              out_dir: str=None, out_file: str='samples.csv', return_df: bool=False) -> pd.DataFrame:
    '''
    savemoney(): create savemoney samples.csv for nanopore mixed WPS

    Parameters:
    fastq_dir (str, optional): path to fastq directory (contains .fastq files, not .fastq.gz files; Default: './fastq')
    fasta_dir (str, optional): path to fasta directory (contains .fasta files; Default: './fasta')
    pt (str, optional): working directory when running savemoney (Default: "." = current directory)
    out_dir (str, optional): path to output directory (Default: None)
    out_file (str, optional): name of output file (Default: 'samples.csv')
    return_df (bool, optional): return dataframe (Default: False)

    Dependencies: pandas, os
    '''
    mkdir(out_dir) # Ensure the output directory exists

    # Get fastq & fasta files
    fastq_files = [fastq_file for fastq_file in os.listdir(fastq_dir) if fastq_file.endswith(".fastq")]
    fasta_files = [fasta_file for fasta_file in os.listdir(fasta_dir) if fasta_file.endswith(".fasta")]

    # Sort files naturally
    fastq_files = sorted(fastq_files, key=t.natural_key)
    fasta_files = sorted(fasta_files, key=t.natural_key)

    # Determine number of fasta files per fastq file
    n_int = int(len(fasta_files)/len(fastq_files))
    n_rem = len(fasta_files)%len(fastq_files)
    n_ls = [n_int+1]*n_rem + [n_int]*(len(fastq_files)-n_rem)

    # Create fastq filename and group name lists
    fastq_fname_ls = []
    group_name_ls = []
    for i,n in enumerate(n_ls):
        fastq_fname_ls += [f'{os.path.join(pt, fastq_files[i])}']*n
        group_name_ls += [fastq_files[i]]*n
    
    # Create ref filename list
    ref_fname_ls = [f'{os.path.join(pt, fasta_file)}' for fasta_file in fasta_files ]

    # Create, save and return samples dataframe
    df = pd.DataFrame({'group_name':group_name_ls,
                       'fastq_fname':fastq_fname_ls,
                       'ref_fname':ref_fname_ls})
    if out_dir is not None and out_file is not None: io.save(dir=out_dir, file=out_file, obj=df)
    if return_df: return df

# Input/Output
def revcom_fastqs(in_dir: str, out_dir: str):
    ''' 
    revcom_fastqs(): write reverse complement of fastqs to a new directory
    
    Parameters:
    in_dir (str): directory with fastq files
    out_dir (str): new directory with reverse complement fastq files
    
    Dependencies: Bio.SeqIO, gzip, os, & Bio.Seq.Seq
    '''
    mkdir(out_dir) # Ensure the output directory exists

    for filename in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory

        if filename.endswith(".fastq.gz"):
            input_fastq_gz = os.path.join(in_dir, filename)
            output_fastq_gz = os.path.join(out_dir, filename)
            print(f"Processing {filename}...")
            with gzip.open(input_fastq_gz, "rt") as infile, gzip.open(output_fastq_gz, "wt") as outfile:
                for record in SeqIO.parse(infile, "fastq"):
                    reverse_complement_seq = record.seq.reverse_complement() # Compute the reverse complement of the sequence
                    reverse_complement_record = record[:] # Create a new record with the reverse complement sequence
                    reverse_complement_record.seq = reverse_complement_seq # Write the new record to the output file
                    SeqIO.write(reverse_complement_record, outfile, "fastq")
            print(f"Saved reverse complement to {output_fastq_gz}")
        
        elif filename.endswith(".fastq"):
            input_fastq = os.path.join(in_dir, filename)
            output_fastq = os.path.join(out_dir, filename)
            print(f"Processing {filename}...")
            with open(input_fastq, "r") as infile, open(output_fastq, "w") as outfile:
                for record in SeqIO.parse(infile, "fastq"):
                    reverse_complement_seq = record.seq.reverse_complement() # Compute the reverse complement of the sequence
                    reverse_complement_record = record[:] # Create a new record with the reverse complement sequence
                    reverse_complement_record.seq = reverse_complement_seq # Write the new record to the output file
                    SeqIO.write(reverse_complement_record, outfile, "fastq")
            print(f"Saved reverse complement to {output_fastq}")

def unzip_fastqs(in_dir: str, out_dir: str):
    ''' 
    unzip_fastqs(): Unzip gzipped fastqs and write to a new directory

    Parameters:
    in_dir (str): directory with compresesd fastq files
    out_dir (str): new directory with uncompressed fastq files
    
    Dependencies: gzip & os
    '''
    mkdir(out_dir) # Ensure the output directory exists

    for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
        print(f"Processing {in_file}...")
        if in_file.endswith(".fastq.gz"):
            with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                with open(os.path.join(out_dir,in_file.split('.fastq.gz')[0]+'.fastq'), 'wt') as out:
                    for line in handle:
                        out.write(line)

def comb_fastqs(in_dir: str, out_dir: str, out_file: str):
    ''' 
    comb_fastqs(): Combines one or more (un)compressed fastqs files into a single (un)compressed fastq file

    Parameters:
    in_dir (str): directory with fastq files
    out_dir (str): new directory with combined fastq file
    out_file (str): Name of output fastq file (Needs .fastq or .fastq.gz suffix)
    
    Dependencies: gzip & os
    '''
    mkdir(out_dir) # Ensure the output directory exists

    if out_file.endswith(".fastq.gz"):
        with gzip.open(os.path.join(out_dir,out_file), 'wt') as out:
            for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
                print(f"Processing {in_file}...")
                if in_file.endswith(".fastq.gz"):
                    with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                        for line in handle:
                            out.write(line)
                
                elif in_file.endswith(".fastq"):
                    with open(os.path.join(in_dir,in_file), 'r') as handle:
                        for line in handle:
                            out.write(line)
    
    elif out_file.endswith(".fastq"):
        with open(os.path.join(out_dir,out_file), 'wt') as out:
            for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
                print(f"Processing {in_file}...")
                if in_file.endswith(".fastq.gz"):
                    with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                        for line in handle:
                            out.write(line)
                
                elif in_file.endswith(".fastq"):
                    with open(os.path.join(in_dir,in_file), 'r') as handle:
                        for line in handle:
                            out.write(line)

    else: print('out_file needs .fastq or .fastq.gz suffix')

# Quantify epeg/ngRNA abundance
### Motif search: mU6,...
def count_motif(fastq_dir: str, pattern: str, out_dir: str, motif: str="motif", 
                max_distance:int=0, max_reads:int=0, meta: pd.DataFrame | str=None,
                return_df:bool=False) -> pd.DataFrame:
    ''' 
    count_motif(): returns a dataframe with the sequence motif location with mismatches per read for every fastq file in a directory

    Parameters:
    fastq_dir (str): path to fastq directory
    pattern (str): search for this sequence motif
    out_dir (str): path to save directory
    motif (str, optional): motif name (Default: 'motif')
    max_distance (int, optional): max Levenstein distance for seq in fastq read (Default: 0)
    meta (DataFrame | str, optional): meta dataframe (or file path) must have 'fastq_file' column (Default: None)
    return_df (bool, optional): return dataframe (Default: False)

    Dependencies: pandas, gzip, os, Bio, fuzzy_substring_search() & memory_timer()
    '''
    # Initialize timer; memory & stats reporting
    memory_timer(reset=True)
    memories = []
    stats = []

    # Create a dataframe to store motif abundance
    df = pd.DataFrame(columns=['fastq_file','read','motif','pattern','window','start_i','end_i','distance'])
    for fastq_file in os.listdir(fastq_dir): # Find all .fastq.gz & .fastq files in the fastq directory
        
        print(f"Processing {fastq_file}...") # Keep track of sequence motifs & reads
        has_motif = 0
        missing_motif = 0
        reads = pd.DataFrame() 

        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            with gzip.open(os.path.join(fastq_dir,fastq_file), 'rt') as handle:
                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    
                    # Find all substrings that resemble 'pattern' within the Levenshtein 'max_distance'.
                    read = fuzzy_substring_search(text=str(record.seq),pattern=pattern,max_distance=max_distance)
                    
                    # Retain the substring with the smallest Levenshtein distance to 'pattern'. 
                    if len(read)==0: # No substring
                        missing_motif += 1
                        read.loc[0] = [pattern, "N"*len(pattern), -1, -1, -1]
                    else: # Found substring(s)
                        has_motif += 1
                        read = read[read['distance']==min(read['distance'])] # Smallest Levenshtein distance
                        read = read.iloc[:1] # Isolate first instance 
                    read['read'] = [r+1] # Add read index
                    reads = pd.concat([reads,read]).reset_index(drop=True)

                    # Processing status and down sample to reduce computation
                    if len(reads)%10000==0: 
                        print(f"Processed {len(reads)} reads")
                    if (len(reads)+1>max_reads) & (max_reads!=0): 
                        break
                 
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            with open(os.path.join(fastq_dir,fastq_file), 'r') as handle:
                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads 

                    # Find all substrings that resemble 'pattern' within the Levenshtein 'max_distance'.
                    read = fuzzy_substring_search(text=str(record.seq),pattern=pattern,max_distance=max_distance)
                    
                    # Retain the substring with the smallest Levenshtein distance to 'pattern'. 
                    if len(read)==0: # No substring
                        missing_motif += 1
                        read.loc[0] = [pattern, "N"*len(pattern), -1, -1, -1]
                    else: # Found substring(s)
                        has_motif += 1
                        read = read[read['distance']==min(read['distance'])] # Smallest Levenshtein distance
                        read = read.iloc[:1] # Isolate first instance 
                    read['read'] = [r+1] # Add read index
                    reads = pd.concat([reads,read]).reset_index(drop=True)

                    # Processing status and down sample to reduce computation
                    if len(reads)%10000==0: 
                        print(f"Processed {len(reads)} reads")
                    if (len(reads)+1>max_reads) & (max_reads!=0): 
                        break
        
        else: # Not a fastq file
            print("Not a fastq file")
            continue         
        
        # Update memories & stats: # of reads with(out) the region
        print(f'{fastq_name}:\t{len(reads)} reads\t=>\t{has_motif} has motif;\t{missing_motif} missing motif')
        stats.append((fastq_name,len(reads),has_motif,missing_motif))
        memories.append(memory_timer(task=f"{fastq_file} (motif)"))
        
        # Append metadata...
        reads['fastq_file'] = [fastq_file]*len(reads)
        reads['motif'] = [motif]*len(reads)
         
        if meta is not None: # ...and merge with meta
            if type(meta)==str: # Get from file path if needed
                meta = io.get(pt=meta)
            
            if 'fastq_file' not in list(meta.columns): # Check for 'fastq_file' column
                print(f"Warning: Did not merge with meta.\nmeta needs 'fastq_file' column.\nDetected columns: {list(meta.columns)}")
            else: # Merge on 'fastq_file' column
                reads = pd.merge(left=meta,right=reads,on='fastq_file')

        # Improve dataframe column formatting 
        reads = reads.astype({'start_i': int,'end_i': int,'read': int,'distance':int})
        reads['mismatches'] = [f">{max_distance}" if d==-1 else int(d) for d in reads['distance']]
        reads['location'] = [(start_i,end_i) if start_i!=-1 else "Absent" for (start_i,end_i) in t.zip_cols(df=reads,cols=['start_i','end_i'])]

        # Save & append fastq dataframe to final dataframe
        print('Save & append fastq dataframe to final dataframe')
        io.save(dir=os.path.join(out_dir,motif),file=f'{fastq_name}.csv',obj=reads) # Save checkpoint
        df = pd.concat([df,reads]).reset_index(drop=True) # save to final dataframe
    
    # Save & return
    memories.append(memory_timer(task='count_motif()'))
    io.save(dir=os.path.join(out_dir,f'.count_{motif}'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_stats.csv',
            obj=pd.DataFrame(stats, columns=['file','reads','reads_w_motif','reads_wo_motif']))
    io.save(dir=os.path.join(out_dir,f'.count_{motif}'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))
    io.save(dir=out_dir,file=f'{motif}.csv',obj=df)
    if return_df: return df # Return dataframe (optional)

def plot_motif(df: pd.DataFrame | str, out_dir: str=None, plot_suf='.pdf',numeric: str='count',
               id_col: str='fastq_file', id_axis: str='fastq', stack_figsize: tuple=(7,3), heat_figsize: tuple=None,
               cutoff_frac:float=0.01, return_df:bool=False, show:bool=True) -> tuple[pd.DataFrame]:
    '''
    plot_motif(): generate plots highlighting motif mismatches, locations, and sequences
    
    Parameters:
    df (dataframe | str): count_motif() dataframe (or file path)
    out_dir (str, optional): output directory
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    numeric (str, optional): 'count' or 'fraction' can be the numeric column for plots (Default: 'count')
    id_col (str, optional): id column name (Default: 'fastq_file')
    id_axis (str, optional): replace id column name on plots (Default: 'fastq')
    stack_figsize (tuple, optional): stacked bar plot figure size (Default: (7,3))
    heat_figsize (tuple, optional): heatmap figure size (Default: None)
    cutoff_frac (float, optional): y-axis values needs be greater than (e.g. 0.01) fraction
    return_df (bool, optional): return dataframe (Default: False)
    show (bool, optional): show plots (Default: True)

    Dependencies: count_motifs(), plot
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Check numeric column
    if numeric not in ['count','fraction']:
        raise(ValueError(f"numeric can only be 'count' or 'fraction'; not {numeric}"))

    # Get value_counts() dataframes...
    # ...remove unwanted columns
    mismatches_cols = [c for c in list(df.columns) if c not in ['read','window','start_i','end_i','location']]
    locations_cols = [c for c in list(df.columns) if c not in ['read','window','distance','mismatches']]
    windows_cols = [c for c in list(df.columns) if c not in ['read','start_i','end_i','distance','mismatches','location']]

    # ...generate dataframes
    df_mismatches = df[mismatches_cols].value_counts().reset_index()
    df_locations = df[locations_cols].value_counts().reset_index()
    df_windows = df[windows_cols].value_counts().reset_index()
    
    # ...save (optional)
    if out_dir is not None:
        io.save(dir=out_dir,file=f"mismatches.csv",obj=df_mismatches)
        io.save(dir=out_dir,file=f"locations.csv",obj=df_locations)
        io.save(dir=out_dir,file=f"windows.csv",obj=df_windows)

    # ...define cut() based on cutoff_frac
    def cut(df_vc: pd.DataFrame, col: str, off: bool=True):
        '''
        cut(): apply cutoff_fraction to value_counts() dataframe grouped by id

        Parameters:
        df_vc (dataframe): value_counts() dataframe
        col (str): column name that will be overwritten with cutoff_frac
        off (bool, optional): apply cutoff (Default: True)
        '''
        df_vc_cutoff = pd.DataFrame() # Initialize output dataframe
        for id in list(df_vc[id_col].value_counts().keys()): # Iterate through ids
            # Group by id and calculate fraction
            df_vc_id = df_vc[df_vc[id_col]==id]
            df_vc_id['fraction'] = df_vc_id['count']/sum(df_vc_id['count'])
            if off: # Apply cutoff_fract
                # Append greater than cutoff_frac
                df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id[df_vc_id['fraction']>=cutoff_frac]]).reset_index(drop=True)
                # Group less than cutoff_frac and append
                df_vc_id_other = df_vc_id[df_vc_id['fraction']<cutoff_frac].reset_index(drop=True)
                if df_vc_id_other.empty==False:
                    df_vc_id_other['count']=sum(df_vc_id_other['count'])
                    df_vc_id_other['fraction']=sum(df_vc_id_other['fraction'])
                    df_vc_id_other[col]=f'<{cutoff_frac*100}%'
                    df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id_other.iloc[:1]]).reset_index(drop=True)
            else: # Do not apply cutoff_fract
                df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id]).reset_index(drop=True)  

        # Return output dataframe
        return df_vc_cutoff
    
    #...apply cut()
    df_mismatches = cut(df_vc=df_mismatches,col='mismatches',off=False)
    df_locations = cut(df_vc=df_locations,col='location')
    df_windows = cut(df_vc=df_windows,col='window')

    # Adjust heat_figsize if not specified
    if heat_figsize is None:
        heat_figsize = (len(df_windows[id_col].unique()),len(df_windows['window'].unique()))

    # Plots
    if numeric=='count':
        p.stack(df=df_mismatches,x=id_col,y=numeric,cols='mismatches', y_axis='Reads',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,palette_or_cmap='tab20',
                dir=out_dir,file=f"mismatches{plot_suf}",show=show)
        
        p.stack(df=df_locations,x=id_col,y=numeric,cols='location', y_axis='Reads',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,palette_or_cmap='tab20',
                dir=out_dir,file=f"locations{plot_suf}",show=show)
        
        p.heat(df=df_windows,x=id_col,y='window',vars='motif',vals=numeric,x_axis=id_axis,y_ticks_font='Courier New',
               figsize=heat_figsize,x_ticks_rot=45,title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}",sq=False,
               dir=out_dir,file=f"windows{plot_suf}",show=show)
    
    else: # fraction
        p.stack(df=df_mismatches,x=id_col,y=numeric,cols='mismatches', y_axis='Reads fraction',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,palette_or_cmap='tab20',
                dir=out_dir,file=f"mismatches{plot_suf}",show=show)
        
        p.stack(df=df_locations,x=id_col,y=numeric,cols='location', y_axis='Reads fraction',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,palette_or_cmap='tab20',
                dir=out_dir,file=f"locations{plot_suf}",show=show)
        
        p.heat(df=df_windows,x=id_col,y='window',vars='motif',vals=numeric,x_axis=id_axis,y_ticks_font='Courier New',
               figsize=heat_figsize,x_ticks_rot=45,title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}",sq=False,
               dir=out_dir,file=f"windows{plot_suf}",vals_dims=(0,1),show=show)
    
    # Return value_counts() dataframes (optional)
    if return_df: return (df_mismatches,df_locations,df_windows)

### Region/read alignments: spacer,..., & ngRNA-epegRNA
def mismatch_alignments(align_col: str, out_dir: str, fastq_name: str,
                        fastq_df_ref: pd.DataFrame, dc_alignments: dict, dc_aligned_reads: dict,
                        dc_alignments_mismatch_pos: dict, dc_alignments_mismatch_num: dict, 
                        return_df: bool=False) -> pd.DataFrame:
    '''
    mismatch_alignments(): Compute & save mismatch number and position per alignment; enables checkpoints

    Parameters:
    align_col (str): align column name in the annotated library reference file
    out_dir (str): directory for output files
    fastq_name (str): name of fastq file
    fastq_df_ref (dataframe): annotated reference library dataframe
    dc_alignments (dict): dictionary of alignments
    dc_aligned_reads (dict): dictionary of aligned reads
    dc_alignments_mismatch_pos (dict): dictionary of mismatch positions
    dc_alignments_mismatch_num (dict): dictionary of mismatch numbers
    return_df (bool, optional): return dataframe (Default: False)
    
    Dependencies: count_region(), count_alignments(), perform_alignments(), pandas, tidy
    '''
    # Merge alignment dictionaries into a fastq dataframe
    print('Merge alignment dictionaries into a fastq dataframe')
    df_alignments = pd.DataFrame(dc_alignments.items(),columns=[align_col,'alignments'])
    df_aligned_reads = pd.DataFrame(dc_aligned_reads.items(),columns=[align_col,'reads_aligned'])
    df_alignments_mismatch_num = pd.DataFrame(dc_alignments_mismatch_num.items(),columns=[align_col,'mismatch_num'])
    df_alignments_mismatch_pos = pd.DataFrame(dc_alignments_mismatch_pos.items(),columns=[align_col,'mismatch_pos']) 
    df_fastq = pd.merge(left=fastq_df_ref,right=df_alignments,on=align_col)
    df_fastq['alignments_fraction'] = [alignments/reads_processed for (alignments,reads_processed) in t.zip_cols(df=df_fastq,cols=['alignments','reads_processed'])]
    df_fastq = pd.merge(left=df_fastq,right=df_aligned_reads,on=align_col)
    df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_num,on=align_col)
    df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_pos,on=align_col)
    
    # Calculate mismatch num & position per alignment
    print('Calculate mismatch num & position per alignment')
    mismatch_num_per_alignment_ls = []
    mismatch_pos_per_alignment_ls = []
    for (ref,mismatch_pos,mismatch_num,alignments) in t.zip_cols(df=df_fastq,cols=[align_col,'mismatch_pos','mismatch_num','alignments']):
        if alignments==0:
            mismatch_num_per_alignment_ls.append(0)
            mismatch_pos_per_alignment_ls.append({pos:0 for pos in range(1,len(ref)+1)})
        else:
            mismatch_num_per_alignment_ls.append(mismatch_num/alignments)
            mismatch_pos_per_alignment_ls.append({pos:mismatch_pos.count(pos)/alignments for pos in range(1,len(ref)+1)})
    df_fastq['mismatch_num_per_alignment'] = mismatch_num_per_alignment_ls
    df_fastq['mismatch_pos_per_alignment'] = mismatch_pos_per_alignment_ls
    
    # Save & return fastq dataframe
    print('Save & return fastq dataframe')
    io.save(dir=out_dir,file=f'{fastq_name}.csv',obj=df_fastq)
    if return_df: return df_fastq

def perform_alignments(align_col: str, out_dir: str, fastq_name: str, fastq_df_ref: pd.DataFrame,
                       aligner: PairwiseAligner, seqs: list, memories: list, align_ckpt: int, exact: bool) -> pd.DataFrame:
    '''
    perform_alignments(): perform alignments on fastq reads using PairwiseAligner and compute mismatches using mismatch_alignments()

    Parameters:
    align_col (str): align column name in the annotated library reference file
    out_dir (str): directory for output files
    fastq_name (str): name of fastq file
    fastq_df_ref (dataframe): annotated reference library dataframe
    aligner (PairwiseAligner): pairwise aligner object
    seqs (list): list of sequences to align
    memories (list): list of memory timers
    align_ckpt (int): alignment checkpoint interval
    exact (bool): perform exact matching only
    
    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, Bio.PairwiseAligner, count_region(), count_alignments(), perform_alignments(), io, tidy
    '''
    print('Perform alignments')
    dc_alignments = {ref:0 for ref in fastq_df_ref[align_col]}
    dc_aligned_reads = {ref:[] for ref in fastq_df_ref[align_col]}
    dc_alignments_mismatch_num = {ref:0 for ref in fastq_df_ref[align_col]}
    dc_alignments_mismatch_pos = {ref:[] for ref in fastq_df_ref[align_col]}
    
    for s,seq in enumerate(seqs): # Iterate though sequences
        if s==0: # Initial alignment status
            print(f'{s+1} out of {len(seqs)}') 
        elif s%align_ckpt==0: # Alignment status; save checkpoint
            print(f'{s+1} out of {len(seqs)}')
            mismatch_alignments(align_col=align_col, out_dir=out_dir, fastq_name=fastq_name, fastq_df_ref=fastq_df_ref,
                                dc_alignments=dc_alignments, dc_aligned_reads=dc_aligned_reads,
                                dc_alignments_mismatch_pos=dc_alignments_mismatch_pos, dc_alignments_mismatch_num=dc_alignments_mismatch_num)
            memories.append(memory_timer(task=f"{fastq_name} (align {s+1} out of {len(seqs)})"))

        if seq is None: # Missing region or empty read
            continue

        found_exact = False 
        for i,ref in enumerate(fastq_df_ref[align_col]): # Iterate though reference sequences (exact matches)
            if str(ref) == str(seq[0:len(ref)]): # Exact match
                aligned_i = aligner.align(ref, seq[0:len(ref)])[0].aligned[0] # trim ngs sequence to reference sequence & align
                ref_i = fastq_df_ref.iloc[i][align_col]
                dc_alignments[ref_i] = dc_alignments[ref_i]+1
                dc_aligned_reads[ref_i].append(s)

                found_exact = True
                break

        if not exact: # Allow non-exact matches
            if not found_exact: # No exact match found; perform alignments
                seq_alignments_scores = []
                seq_alignments_aligned = []

                for ref in fastq_df_ref[align_col]: # Iterate though reference sequences
                    seq_alignment = aligner.align(ref, seq[0:len(ref)]) # trim ngs sequence to reference sequence & align
                    seq_alignments_scores.append(seq_alignment[0].score) # Save highest alignment score
                    seq_alignments_aligned.append(seq_alignment[0].aligned[0]) # Save alignment matches

                # Isolate maximum score alignment
                i = seq_alignments_scores.index(max(seq_alignments_scores))
                ref_i = fastq_df_ref.iloc[i][align_col]
                aligned_i = seq_alignments_aligned[i]
                dc_alignments[ref_i] = dc_alignments[ref_i]+1
                dc_aligned_reads[ref_i].append(s)
        
        else: # Exact matches only
            if not found_exact: # No exact match found; skip
                continue

        # Find & quantify mismatches (Change zero-indexed to one-indexed)
        mismatch_pos = []
        if len(aligned_i) == 1: 
            (a1,b1) = aligned_i[0]
            if (a1==0)&(b1==len(ref_i)-1): mismatch_pos.extend([])
            elif a1==0: mismatch_pos.extend([k+1 for k in range(b1+1,len(ref_i))])
            elif b1==len(ref_i)-1: mismatch_pos.extend([k+1 for k in range(0,a1-1)])
            else: mismatch_pos.extend([j+1 for j in range(0,a1-1)] + [k+1 for k in range(b1+1,len(ref_i))])
        else:
            for j in range(len(aligned_i)-1):
                (a1,b1) = aligned_i[j]
                (a2,b2) = aligned_i[j+1]
                if (j==0)&(a1!=0): mismatch_pos.extend([k+1 for k in range(0,a1-1)])
                if (j==len(aligned_i)-2)&(b2!=len(ref_i)-1): mismatch_pos.extend([k+1 for k in range(b2+1,len(ref_i))])
                mismatch_pos.extend([k+1 for k in range(b1+1,a2-1)])
        dc_alignments_mismatch_num[ref_i] = dc_alignments_mismatch_num[ref_i] + len(mismatch_pos)
        dc_alignments_mismatch_pos[ref_i] = dc_alignments_mismatch_pos[ref_i] + mismatch_pos

    # Perform mismatch alignments and return fastq dataframe
    return mismatch_alignments(align_col=align_col, out_dir=out_dir, fastq_name=fastq_name, fastq_df_ref=fastq_df_ref,
                               dc_alignments=dc_alignments, dc_aligned_reads=dc_aligned_reads,
                               dc_alignments_mismatch_pos=dc_alignments_mismatch_pos, dc_alignments_mismatch_num=dc_alignments_mismatch_num,
                               return_df=True)
    
def plot_alignments(fastq_alignments: dict | str, align_col: str, id_col: str,
                    out_dir: str, plot_suf:str='.pdf', show:bool=False, **plot_kwargs):
    ''' 
    plot_alignments(): generate line & distribution plots from fastq alignments dictionary
    
    Parameters:
    fastq_alignments (dict | str): fastq alignments dictionary from count_region() or count_alignments()
    align_col (str): align column name in the annotated library reference file
    id_col (str): id column name in the annotated library reference file
    fastq_dir (str): directory with fastq files
    out_dir (str): directory for output files
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: pandas & plot
    '''
    # Get dictionary from directory path if needed
    if type(fastq_alignments)==str: 
        fastq_alignments = io.get_dir(fastq_alignments, literal_eval=True)
    
    for fastq_name,df_fastq in fastq_alignments.items(): # Iterate through dictionary
        
        # Plot mismatch position per alignment
        print('Plot mismatch position per alignment')
        
        out_dir_fastq_name = os.path.join(out_dir,fastq_name)
        df_fastq_plot = pd.DataFrame()
        for align,id,mismatch_pos_per_alignment in t.zip_cols(df=df_fastq,cols=[align_col,id_col,'mismatch_pos_per_alignment']):
            df_fastq_plot_align = pd.DataFrame({align_col:[align]*len(mismatch_pos_per_alignment), # Obtain individual alignments
                                                id_col:[id]*len(mismatch_pos_per_alignment),
                                                'mismatch_pos':list(mismatch_pos_per_alignment.keys()),
                                                'mismatch_pos_per_alignment':list(mismatch_pos_per_alignment.values())})

            p.scat(typ='line',df=df_fastq_plot_align,x='mismatch_pos',y='mismatch_pos_per_alignment', # Plot mismatches for each alignment
                   title=f'{fastq_name} {id}',x_axis='Alignment Position',y_axis='Mismatches/Alignment',y_axis_dims=(0,1),
                   dir=out_dir_fastq_name,file=f'{id.replace(".","_")}{plot_suf}',
                   show=show,**plot_kwargs)
            
            df_fastq_plot = pd.concat(objs=[df_fastq_plot,df_fastq_plot_align]).reset_index(drop=True) # Group alignment mismatches

        p.scat(typ='line',df=df_fastq_plot,x='mismatch_pos',y='mismatch_pos_per_alignment',cols=id_col, # Plot mismatches for each alignment
               title=f'{fastq_name}',x_axis='Alignment Position',y_axis='Mismatches/Alignment',y_axis_dims=(0,1),
               dir=out_dir_fastq_name,file=f'alignment_mismatches{plot_suf}',legend_ncol=int(math.ceil(len(df_fastq_plot[id_col].value_counts())/20)),
               show=show,**plot_kwargs)

        p.dist(typ='hist',df=df_fastq,x='alignments',x_axis_dims=(0,max(df_fastq['alignments'])),
               title=f'{fastq_name}',dir=out_dir_fastq_name,file=f'alignments{plot_suf}',
               show=show,**plot_kwargs)

def count_region(df_ref: pd.DataFrame | str, align_col: str, id_col: str,
                 fastq_dir: str, df_motif5: pd.DataFrame | str, df_motif3: pd.DataFrame | str,
                 out_dir: str, fastq_col: str=None, match_score: float = 2, mismatch_score: float = -1, 
                 open_gap_score: float = -10, extend_gap_score: float = -0.1, align_dims: tuple=(0,0),
                 align_ckpt: int=10000, plot_suf: str=None, show: bool=False, return_dc: bool=False, exact: bool=False,
                 **plot_kwargs) -> dict[pd.DataFrame]:
    ''' 
    count_region(): align read region from fastq directory to the annotated library with mismatches; plot and return fastq alignments dictionary

    Parameters:
    df_ref (dataframe | str): annotated reference library dataframe (or file path)
    align_col (str): align column name in the annotated reference library
    id_col (str): id column name in the annotated reference library
    fastq_dir (str): directory with fastq files
    df_motif5 (dataframe | str): 5' motif dataframe (or file path)
    df_motif3 (dataframe | str): 3' motif dataframe (or file path)
    out_dir (str): directory for output files
    fastq_col (str, optional): fastq column name in the annotated reference library (Default: None)
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)
    align_dims (tuple, optional): (start_i, end_i) alignments per fastq file to save compute (Default: None)
    align_ckpt (int, optional): save checkpoints for alignments (Default: 10000)
    plot_suf (str, optional): plot type suffix with '.' (Default: None)
    show (bool, optional): show plots (Default: False)
    return_dc (bool, optional): return fastqs dictionary (Default: False)
    exact (bool, optional): perform exact matching only (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, Bio.PairwiseAligner, mismatch_alignments(), perform_alignments(), plot_alignments(), memory_timer(), io, tidy
    '''
    # Initialize timer
    memory_timer(reset=True)

    # Intialize the aligner
    aligner = PairwiseAligner()
    aligner.mode = 'global'  # Use 'local' for local alignment
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = open_gap_score  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = extend_gap_score  # Penalty for extending a gap; applied to both strands

    # Get dataframes from file path if needed
    if type(df_ref)==str: 
        df_ref = io.get(df_ref)
    if type(df_motif5)==str: 
        df_motif5 = io.get(df_motif5)
    if type(df_motif3)==str: 
        df_motif3 = io.get(df_motif3)
    
    # Check dataframe for alignment, id, & fastq columns
    if align_col not in df_ref.columns.tolist():
        raise Exception(f'Missing alignment column: {align_col}') 
    if id_col not in df_ref.columns.tolist():
        raise Exception(f'Missing id column: {id_col}')
    if fastq_col is not None:
        if fastq_col not in df_ref.columns.tolist():
            raise Exception(f'Missing fastq column: {fastq_col}')
    df_ref[align_col] = df_ref[align_col].str.upper() 

    # Check for fastq_file, start_i and end_i columns
    if 'fastq_file' not in df_motif5.columns.tolist():
        raise Exception('Missing column in df_motif5: fastq_file') 
    if 'end_i' not in df_motif5.columns.tolist():
        raise Exception('Missing column in df_motif5: end_i') 
    if 'fastq_file' not in df_motif3.columns.tolist():
        raise Exception('Missing column in df_motif3: fastq_file') 
    if 'start_i' not in df_motif3.columns.tolist():
        raise Exception('Missing column in df_motif3: start_i') 

    # Check if align_dims is a tuple of length 2 with start_i greater than end_i
    if align_dims is None:
        align_dims=(0,0)
    elif not isinstance(align_dims, tuple) or len(align_dims) != 2:
        raise ValueError(f"align_dims={align_dims} was not a tuple of length 2")
    else:
        if align_dims[0]<0 or align_dims[1]<0:
            raise ValueError(f"align_dims={align_dims} needs to be greater than 0")
        if align_dims[1]<align_dims[0]:
            raise ValueError(f"align_dims={align_dims} needs to be in the form (start_i, end_i)")

    # Check if align_ckpt is a positive integer
    if not isinstance(align_ckpt, int) or align_ckpt <= 0:
        raise ValueError(f"align_ckpt={align_ckpt} needs to be a positive integer")
    
    # Memory & stats reporting
    memories = []
    stats = []

    # Make fastqs dictionary
    if return_dc: fastqs = dict()
    for fastq_file in os.listdir(fastq_dir): # Iterate through fastq files
        
        print(f"Processing {fastq_file}...") # Get reads
        regions = 0 # Keep track of # of reads with(out) regions
        missing5 = 0
        missing3 = 0
        overlap53 = 0
        seqs = [] # Store region sequences from reads with motifs
        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            fastq_motif5 = df_motif5[df_motif5['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif5 info
            fastq_motif3 = df_motif3[df_motif3['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif3 info
            if fastq_col is not None: fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: fastq_df_ref = df_ref.copy()
            with gzip.open(os.path.join(fastq_dir,fastq_file), "rt") as handle:
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i+1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue

                    # Obtain motif boundaries that define region
                    start_i = fastq_motif5.iloc[i]["end_i"]
                    end_i = fastq_motif3.iloc[i]["start_i"] 
                    if (start_i!=-1) & (end_i!=-1): # Both motifs are present
                        if start_i<end_i: # Motifs do not overlap
                            regions += 1
                            seqs.append(record.seq[start_i:end_i])
                        else: # Motifs overlap
                            overlap53 += 1
                            seqs.append(None)
                    elif (start_i==-1) & (end_i==-1): # Both motifs are missing
                        missing5 += 1
                        missing3 += 1
                        seqs.append(None)
                    elif start_i==0: # 5' motif is missing
                        missing5 += 1 
                        seqs.append(None)
                    else: # 3' motif is missing
                        missing3 += 1 
                        seqs.append(None)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
        
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            fastq_motif5 = df_motif5[df_motif5['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif5 info
            fastq_motif3 = df_motif3[df_motif3['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif3 info
            if fastq_col is not None: fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: fastq_df_ref = df_ref.copy()
            with open(os.path.join(fastq_dir,fastq_file), "r") as handle:    
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i+1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue

                    # Obtain motif boundaries that define region
                    start_i = fastq_motif5.iloc[i]["end_i"]
                    end_i = fastq_motif3.iloc[i]["start_i"] 
                    if (start_i!=-1) & (end_i!=-1): # Both motifs are present
                        if start_i<end_i: # Motifs do not overlap
                            regions += 1
                            seqs.append(record.seq[start_i:end_i]) 
                        else: # Motifs overlap
                            overlap53 += 1
                            seqs.append(None)
                    elif (start_i==-1) & (end_i==-1): # Both motifs are missing
                        missing5 += 1
                        missing3 += 1
                        seqs.append(None)
                    elif start_i==0: # 5' motif is missing
                        missing5 += 1 
                        seqs.append(None)
                    else: # 3' motif is missing
                        missing3 += 1 
                        seqs.append(None)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
        
        else: # Not a fastq file
            print("Not a fastq file")
            continue
        
        # Update memories & stats: # of reads with(out) the region
        processed_reads = len(seqs)
        print(f'{fastq_name}:\t{len(seqs)} reads\t=>\t{regions} reads;\t{missing5} missing motif5;\t{missing3} missing motif3;\t{overlap53} motif overlaps')
        stats.append((fastq_name, reads, processed_reads,
                    regions, missing5, missing3, overlap53,
                    processed_reads/reads, regions/processed_reads))
        memories.append(memory_timer(task=f"{fastq_file} (region)"))

        # Append # of reads & alignment range to fastq_df_ref
        fastq_df_ref['reads_total']= [reads]*len(fastq_df_ref)
        fastq_df_ref['reads_processed']= [processed_reads]*len(fastq_df_ref)
        if align_dims==(0,0): 
            fastq_df_ref['align_dims']= [(0,reads)]*len(fastq_df_ref)
        else:
            fastq_df_ref['align_dims']= [(align_dims[0],align_dims[1])] * len(fastq_df_ref)

        # Perform alignments, compute mismatches, & append to fastq dataframe to dictionary
        df_fastq = perform_alignments(align_col=align_col, out_dir=out_dir, fastq_name=fastq_name, fastq_df_ref=fastq_df_ref,
                                      aligner=aligner, seqs=seqs, memories=memories, align_ckpt=align_ckpt, exact=exact)
        if return_dc: fastqs[fastq_name]=df_fastq
        memories.append(memory_timer(task=f"{fastq_name} (aligned)"))

        # Plot mismatch position per alignment
        if plot_suf is not None: 
            plot_alignments(fastq_alignments={fastq_name:df_fastq}, align_col=align_col, id_col=id_col,
                            out_dir=out_dir, plot_suf=plot_suf, show=show, **plot_kwargs)
        
    # Save and return
    memories.append(memory_timer(task='count_region()'))
    io.save(dir=os.path.join(out_dir,'.count_region'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_stats.csv',
            obj=pd.DataFrame(stats, columns=['file', 'reads_total', 'reads_processed',
                                            'reads_w_region', 'reads_wo_motif5', 'reads_wo_motif3', 'reads_w_motif_overlap',
                                            'reads_processed_fraction', 'reads_w_region_fraction']))
    io.save(dir=os.path.join(out_dir,'.count_region'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))
    if return_dc: return fastqs

def count_alignments(df_ref: pd.DataFrame | str, align_col: str, id_col: str,
                     fastq_dir: str, out_dir: str, fastq_col: str=None, match_score: float = 2, mismatch_score: float = -1, 
                     open_gap_score: float = -10, extend_gap_score: float = -0.1, align_dims: tuple=(0,0),
                     align_ckpt: int=10000, plot_suf: str=None, show: bool=False, return_dc: bool=False, exact: bool=False,
                     **plot_kwargs) -> dict[pd.DataFrame]:
    ''' 
    count_alignments(): align reads from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary
    
    Parameters:
    df_ref (dataframe | str): annotated reference library dataframe (or file path)
    align_col (str): align column name in the annotated reference library
    id_col (str): id column name in the annotated reference library
    fastq_dir (str): directory with fastq files
    out_dir (str): directory for output files
    fastq_col (str, optional): fastq column name in the annotated reference library (Default: None)
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)
    align_dims (tuple, optional): (start_i, end_i) alignments per fastq file to save compute (Default: None)
    align_ckpt (int, optional): save checkpoints for alignments (Default: 10000)    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    plot_suf (str, optional): plot type suffix with '.' (Default: None)
    show (bool, optional): show plots (Default: False)
    return_dc (bool, optional): return fastqs dictionary (Default: False)
    exact (bool, optional): perform exact matching only (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, Bio.PairwiseAligner, mismatch_alignments(), perform_alignments(), plot_alignments(), memory_timer(), io, tidy
    '''
    # Initialize timer
    memory_timer(reset=True)

    # Intialize the aligner
    aligner = PairwiseAligner()
    aligner.mode = 'global'  # Use 'local' for local alignment
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = open_gap_score  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = extend_gap_score  # Penalty for extending a gap; applied to both strands

    # Get dataframe from file path if needed
    if type(df_ref)==str: 
        df_ref = io.get(df_ref)
    
    # Check dataframe for alignment and id columns
    if align_col not in df_ref.columns.tolist():
        raise Exception(f'Missing alignment column: {align_col}') 
    if id_col not in df_ref.columns.tolist():
        raise Exception(f'Missing id column: {id_col}')
    if fastq_col is not None:
        if fastq_col not in df_ref.columns.tolist():
            raise Exception(f'Missing fastq column: {fastq_col}')
    df_ref[align_col] = df_ref[align_col].str.upper() 
    
    # Memory & stats reporting
    memories = []
    stats = []

    # Make fastqs dictionary
    if return_dc: fastqs = dict()
    for fastq_file in os.listdir(fastq_dir): # Iterate through fastq files
        
        print(f"Processing {fastq_file}...") # Get reads
        seqs = [] # Store alignment sequences
        empty_reads = 0 # Keep track of # of empty reads
        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            if fastq_col is not None: fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: fastq_df_ref = df_ref.copy()
            with gzip.open(os.path.join(fastq_dir,fastq_file), "rt") as handle:
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i + 1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue
                    
                    # Append sequence
                    if record.seq == "":
                        seqs.append(None)
                        empty_reads += 1
                    else:
                        seqs.append(record.seq)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
                
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            if fastq_col is not None: fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: fastq_df_ref = df_ref.copy()
            with open(os.path.join(fastq_dir,fastq_file), "r") as handle:
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i + 1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue
                    
                    # Append sequence
                    if record.seq == "":
                        seqs.append(None)
                        empty_reads += 1
                    else:
                        seqs.append(record.seq)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
        
        else: # Not a fastq file
            print("Not a fastq file")
            continue
        
        # Update memories & print reads stats
        processed_reads = len(seqs)
        nonempty_reads = processed_reads - empty_reads
        print(f'{fastq_name}:\t{reads} reads;\t {processed_reads} processed reads;\t {nonempty_reads} nonempty reads;\t{empty_reads} empty reads')
        memories.append(memory_timer(task=f"{fastq_file} (reads)"))
        
        # Append # of reads & alignment range to fastq_df_ref
        fastq_df_ref['reads_total']= [reads]*len(fastq_df_ref)
        fastq_df_ref['reads_processed']= [processed_reads]*len(fastq_df_ref)
        if align_dims==(0,0): 
            fastq_df_ref['align_dims']= [(0,reads)]*len(fastq_df_ref)
        else:
            fastq_df_ref['align_dims']= [(align_dims[0],align_dims[1])] * len(fastq_df_ref)

        # Perform alignments, compute mismatches, & append to fastq dataframe to dictionary
        df_fastq = perform_alignments(align_col=align_col, out_dir=out_dir, fastq_name=fastq_name, fastq_df_ref=fastq_df_ref,
                                      aligner=aligner, seqs=seqs, memories=memories, align_ckpt=align_ckpt, exact=exact)
        if return_dc: fastqs[fastq_name]=df_fastq
        memories.append(memory_timer(task=f"{fastq_name} (aligned)"))

        # Update memories & stats file with # of reads
        aligned_reads = df_fastq['alignments'].sum()
        print(f'{fastq_name}:\t{reads} reads;\t {processed_reads} processed reads;\t {aligned_reads} aligned reads')
        stats.append((fastq_name, reads, 
                    processed_reads, empty_reads, nonempty_reads, aligned_reads, 
                    processed_reads/reads, empty_reads/processed_reads, nonempty_reads/processed_reads, aligned_reads/processed_reads))

        # Plot mismatch position per alignment
        if plot_suf is not None: 
            plot_alignments(fastq_alignments={fastq_name:df_fastq}, align_col=align_col, id_col=id_col,
                            out_dir=out_dir, plot_suf=plot_suf, show=show, **plot_kwargs)

    # Save and return
    memories.append(memory_timer(task='count_alignments()'))
    io.save(dir=os.path.join(out_dir,'.count_alignments'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_stats.csv',
            obj=pd.DataFrame(stats, columns=['file', 'reads_total', 
                                            'reads_processed', 'reads_empty', 'reads_nonempty', 'reads_aligned',
                                            'reads_processed_fraction', 'reads_empty_fraction', 'reads_nonempty_fraction', 'reads_aligned_fraction']))
    io.save(dir=os.path.join(out_dir,'.count_alignments'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))
    if return_dc: return fastqs

def plot_paired(df: pd.DataFrame | str, title: str, out_dir: str,  
                id_col: str='ID', desired_col: str='desired', y: Literal['count','fraction']='count',
                plot_suf: str='.pdf', show: bool=False, **plot_kwargs):
    ''' 
    plot_paired(): generate stacked bar plots from paired_regions() dataframe
    
    Parameters:
    df (dataframe | str): dataframe from paired_regions() or file path
    title (str): plot title and file name
    out_dir (str): directory for output files
    id_col (str, optional): id column name in the paired region file (Default: 'ID')
    desired_col (str, optional): desired column name in the paired region file (Default: 'desired')
    y (str, optional): y axis for plots (Default: 'count', Options: 'count' & 'fraction')
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: pandas & plot
    '''
    # Get dataframe from file path if needed
    if type(df)==str: 
        df = io.get(df, literal_eval=True)

    # Create, save & plot alignment status
    paired_regions_alignment_status_df = df[[desired_col,'alignment_status']].value_counts().reset_index()
    paired_regions_alignment_status_df_sum = sum(paired_regions_alignment_status_df['count'])
    paired_regions_alignment_status_df['fraction'] = [count/paired_regions_alignment_status_df_sum for count in paired_regions_alignment_status_df['count']]
    
    io.save(dir=os.path.join(out_dir, title),file='alignment_status.csv',obj=paired_regions_alignment_status_df)
    
    p.stack(df=paired_regions_alignment_status_df,x='alignment_status',y=y,
            cols=desired_col,cols_ord=[True,False],vertical=False,figsize=(6,2),
            title=title,dir=os.path.join(out_dir, title),file=f'alignment_status{plot_suf}',show=show,**plot_kwargs)

    # Create, save & plot alignment distribution
    paired_regions_alignment_distribution_df = df[df['alignment_status']=='region 1 & 2']
    desired_ID = []
    for id,desired in t.zip_cols(df=paired_regions_alignment_distribution_df,cols=[id_col,desired_col]):
        if desired==True or id=='chimera': desired_ID.append(id)
        else: desired_ID.append('not chimera')
    paired_regions_alignment_distribution_df[id_col] = desired_ID

    io.save(dir=os.path.join(out_dir, title),file='alignment_distribution_per_read.csv',obj=paired_regions_alignment_distribution_df)

    paired_regions_alignment_distribution_df = paired_regions_alignment_distribution_df[[id_col,desired_col]].value_counts().reset_index()
    paired_regions_alignment_distribution_df_sum = sum(paired_regions_alignment_distribution_df['count'])
    paired_regions_alignment_distribution_df['fraction'] = [count/paired_regions_alignment_distribution_df_sum for count in paired_regions_alignment_distribution_df['count']]
    
    io.save(dir=os.path.join(out_dir, title),file='alignment_distribution.csv',obj=paired_regions_alignment_distribution_df)

    p.stack(df=paired_regions_alignment_distribution_df,
            x=desired_col,y=y,cols=id_col,palette_or_cmap='Spectral',x_ord=[True,False],vertical=False,
            cols_ord=list(paired_regions_alignment_distribution_df[id_col]),
            legend_ncol=4,legend_bbox_to_anchor=(0,-.3),figsize=(10,2),
            title=title,dir=os.path.join(out_dir, title),file=f'alignment_distribution{plot_suf}',show=show,**plot_kwargs)

def paired_regions(meta_dir: str, region1_dir: str, region2_dir: str, out_dir: str, 
                   id_col: str='ID', desired_col: str='desired', 
                   region1_alignment_col: str='r1_alignment', region2_alignment_col: str='r2_alignment', 
                   reads_aligned_col: str='reads_aligned', reads_processed_col: str='reads_processed',
                   y: Literal['count','fraction']='count', plot_suf: str='.pdf', show: bool=False, return_dc: bool=False,
                   **plot_kwargs) -> dict[pd.DataFrame]:
    '''
    paired_regions(): quantify, plot, & return (un)paired regions that aligned to the annotated library

    Parameters:
    meta_dir (str): directory with meta files
    region1_dir (str): directory with region 1 files
    region2_dir (str): directory with region 2 files
    out_dir (str): directory for output files
    id_col (str): id column name in the region & meta files
    desired_col (str): desired column name in the meta files
    region1_alignment_col (str): region 1 alignment column name in the region & meta files
    region2_alignment_col (str): region 2 alignment column name in the region & meta files
    reads_aligned_col (str, optional): reads_aligned column name in the region files (Default: 'reads_aligned')
    reads_processed_col (str, optional): reads_processed column name in the region files (Default: 'reads_processed')
    y (str, optional): y axis for plots (Default: 'count'; Options: 'count' & 'fraction')
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    return_dc (bool, optional): return (un)paired regions dataframe (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: os, pandas, io, plot, & plot_paired()
    '''
    # Initialize timer; memory reporting
    memory_timer(reset=True)
    memories = []

    # Get meta, region 1 and 2 file names
    meta_file_names = io.sorted_file_names(dir=meta_dir)
    region1_file_names = io.sorted_file_names(dir=region1_dir)
    region2_file_names = io.sorted_file_names(dir=region2_dir)

    # Check that files are correctly paired...
    if (len(region1_file_names)!=len(region2_file_names)) & (len(meta_file_names)!=len(region2_file_names)): # Equal # of files
        raise Exception(f'Unequal # of files in meta & region directories:\nmeta_dir: {meta_dir}\nregion1_dir: {region1_dir}\nregion2_dir: {region2_dir}')
    
    # ...and obtain mismatch indices (if applicable)
    file_name_mismatch_ls = [] # Allowed mismatch R"1" & R"2"

    # Compare region file names
    for region1_file_name,region2_file_name in zip(region1_file_names,region2_file_names):
        if len(region1_file_name)!=len(region2_file_name):
            raise ValueError(f"Mispaired files in region directories:\nregion1_dir: {region1_file_name}\nregion2_dir: {region2_file_name}\nFile names are different lengths")
        
        file_name_mismatch = [i for i,(a,b) in enumerate(zip(region1_file_name, region2_file_name)) if a!=b]
        if len(file_name_mismatch)>1:
            raise Exception(f'Mispaired files in region directories:\nregion1_dir: {region1_file_name}\nregion2_dir: {region2_file_name}\nFile names contain more than 1 mismatch')
        elif len(file_name_mismatch)==1:
            file_name_mismatch_ls.extend(file_name_mismatch)
        else:
            file_name_mismatch_ls.append(len(region1_file_name))
    
    # Compare with meta file names
    for i,(meta_file_name,region1_file_name,region2_file_name) in enumerate(zip(meta_file_names,region1_file_names,region2_file_names)):
        if meta_file_name[0:file_name_mismatch_ls[i]]!=region1_file_name[0:file_name_mismatch_ls[i]]:
            raise ValueError(f"Mispaired files in meta & region 1 directories:\nmeta_dir: {meta_file_name}\nregion1_dir: {region1_file_name}\nFile names are different lengths")
        elif meta_file_name[0:file_name_mismatch_ls[i]]!=region2_file_name[0:file_name_mismatch_ls[i]]:
            raise ValueError(f"Mispaired files in meta & region 2 directories:\nrmeta_dir: {meta_file_name}\nregion2_dir: {region2_file_name}\nFile names are different lengths")
    
    # Parse paired regions
    paired_regions_dc = dict()
    for i,(meta_file_name,region1_file_name,region2_file_name) in enumerate(zip(meta_file_names,region1_file_names,region2_file_names)):
        
        # Get regions dataframe
        region1_file_df = io.get(os.path.join(region1_dir,region1_file_name),literal_eval=True)[[region1_alignment_col,reads_processed_col,reads_aligned_col]]
        region2_file_df = io.get(os.path.join(region2_dir,region2_file_name),literal_eval=True)[[region2_alignment_col,reads_processed_col,reads_aligned_col]]
        meta_file_df = io.get(os.path.join(meta_dir,meta_file_name),literal_eval=True)

        # Create empty paired regions dataframe
        paired_regions_file_df = pd.DataFrame({'read': np.arange(1, region1_file_df.iloc[0][reads_processed_col] + 1),
                                                region1_alignment_col: [np.nan] * region1_file_df.iloc[0][reads_processed_col],
                                                region2_alignment_col: [np.nan] * region1_file_df.iloc[0][reads_processed_col]})
        
        # Fill in paired regions dataframe
        for r1_alignment,reads_aligned in t.zip_cols(df=region1_file_df,cols=[region1_alignment_col,reads_aligned_col]):
            for read in reads_aligned:
                paired_regions_file_df.at[read-1,region1_alignment_col] = r1_alignment
        del region1_file_df # Save memory

        for r2_alignment,reads_aligned in t.zip_cols(df=region2_file_df,cols=[region2_alignment_col,reads_aligned_col]):
            for read in reads_aligned:
                paired_regions_file_df.at[read-1,region2_alignment_col] = r2_alignment
        del region2_file_df # Save memory
        
        # Check for paired regions
        alignment_status = []
        for r1_alignment,r2_alignment in t.zip_cols(df=paired_regions_file_df,cols=['r1_alignment','r2_alignment']):
            if pd.isna(r1_alignment)==True and pd.isna(r2_alignment)==True:
                alignment_status.append('neither')
            elif pd.isna(r1_alignment)==True:
                alignment_status.append('region 2')
            elif pd.isna(r2_alignment)==True:
                alignment_status.append('region 1')
            else:
                alignment_status.append('region 1 & 2')
        paired_regions_file_df['alignment_status'] = alignment_status
        
        # Combine paired regions dataframe with meta dataframe
        paired_regions_file_df = pd.merge(left=meta_file_df,right=paired_regions_file_df,how='right',on=[region1_alignment_col,region2_alignment_col])
        del meta_file_df # Save memory

        # Fill NA values
        paired_regions_file_df[desired_col] = paired_regions_file_df[desired_col].fillna(False)
        paired_regions_file_df[id_col] = paired_regions_file_df[id_col].fillna("chimera")

        # Memory reporting, save, & plot
        memories.append(memory_timer(task=meta_file_name[0:file_name_mismatch_ls[i]]))
        io.save(dir=out_dir,file=meta_file_name,obj=paired_regions_file_df)
        plot_paired(df=paired_regions_file_df, title=meta_file_name[0:file_name_mismatch_ls[i]], out_dir=out_dir,
                    id_col=id_col, desired_col=desired_col, y=y, plot_suf=plot_suf, show=show, **plot_kwargs)
        if return_dc: paired_regions_dc[meta_file_name] = paired_regions_file_df
    
    # Save & return
    memories.append(memory_timer(task='paired_regions()'))
    io.save(dir=os.path.join(out_dir,'.paired_regions'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))
    if return_dc: return paired_regions_dc  

### Signature 
def count_signatures(df_ref: pd.DataFrame | str, signature_col: str, id_col: str, edit_col: str, fastq_dir: str, 
                     out_dir: str, out_file: str, config_key: str = None, in_file: pd.DataFrame | str=None, sequence: str=None, n_extra_nt: int=0,
                     df_motif5: pd.DataFrame | str=None, df_motif3: pd.DataFrame | str=None, fastq_col: str=None,  meta: pd.DataFrame | str=None, 
                     match_score: float = 2, mismatch_score: float = -1, open_gap_score: float = -10, extend_gap_score: float = -0.1, 
                     align_dims: tuple=(0,0), align_ckpt: int=10000, save_alignments: bool=False, return_df: bool=False, 
                     literal_eval: bool=True, plot_suf: str='.pdf', show: bool=False, **plot_kwargs) -> pd.DataFrame:
    ''' 
    count_signatures(): generate signatures from fastq read region alignments to WT sequence; count signatures, plot and return fastq signatures dataframe

    Parameters:
    df_ref (dataframe | str): annotated reference library dataframe (or file path)
    signature_col (str): signature column name in the annotated reference library
    id_col (str): id column name in the annotated reference library
    edit_col (str): edit column name in the annotated reference library
    fastq_dir (str): directory with fastq files
    out_dir (str): directory for output files
    out_file (str): output filename

    config_key (str, option 1): Configuration key (FWD primer_REV primer) with 'sequence'
    sequence (str, option 2): Target sequence; retrieved from 'config_key' or 'in_file' if not provided
    in_file (dataframe | str, option 3): Input file (.txt or .csv) with sequences for PrimeDesign. Format: target_name,target_sequence,aa_index (column names required)

    n_extra_nt (int, optional): number of extra nucleotide differences allowed for Signature match (Default: 0)
    df_motif5 (dataframe | str, optional): 5' motif dataframe (or file path)
    df_motif3 (dataframe | str, optional): 3' motif dataframe (or file path)
    fastq_col (str, optional): fastq column name in the annotated reference library (Default: None)
    meta (DataFrame | str, optional): meta dataframe (or file path) must have 'fastq_file' column (Default: None)
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)
    align_dims (tuple, optional): (start_i, end_i) alignments per fastq file to save compute (Default: None)
    align_ckpt (int, optional): save checkpoints for alignments (Default: 10000)
    save_alignments (bool, optional): save alignments (Default: False, save memory)
    return_df (bool, optional): return dataframe (Default: False)
    literal_eval (bool, optional): convert string representations (Default: True)
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot keyword arguments
    '''
    # Initialize timer
    memory_timer(reset=True)

    # Intialize the aligner
    aligner = PairwiseAligner()
    aligner.mode = 'global'  # Use 'local' for local alignment
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = open_gap_score  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = extend_gap_score  # Penalty for extending a gap; applied to both strands

    # Get dataframe from file path if needed
    if type(df_ref)==str: 
        df_ref = io.get(df_ref,literal_eval=literal_eval)
    
    # Check dataframe for alignment, id, & fastq columns
    if signature_col not in df_ref.columns.tolist():
        raise Exception(f'Missing signature column: {signature_col}') 
    if id_col not in df_ref.columns.tolist():
        raise Exception(f'Missing id column: {id_col}')
    if edit_col not in df_ref.columns.tolist():
        raise Exception(f'Missing edit column: {edit_col}')
    if fastq_col is not None:
        if fastq_col not in df_ref.columns.tolist():
            raise Exception(f'Missing fastq column: {fastq_col}')

    # Get ref_seq from config_key, in_file, or sequence
    if sequence is None: 
        if in_file is not None: # Use in_file to get sequence
            if type(in_file)==str: # Get dataframe from file path if needed
                in_file = io.get(in_file)
            sequence = in_file.iloc[0]['target_sequence'].split('(')[1].split(')')[0]
        
        elif config_key is not None: # Use config_key to get sequence
            sequence = config.get_info(id=config_key)['sequence']
        
        else: # No sequence provided
            raise(ValueError("'config_key', 'sequence' or 'in_file' are required. If multiple are provided, 'sequence' (1st) or 'config_key' (2nd) will be used."))
    
    ref_seq = sequence
    
    # Get & check motif dataframes from file path if needed
    if df_motif5 is not None:
        if type(df_motif5)==str: 
            df_motif5 = io.get(df_motif5)
        if 'fastq_file' not in df_motif5.columns.tolist():
            raise Exception('Missing column in df_motif5: fastq_file') 
        if 'end_i' not in df_motif5.columns.tolist():
            raise Exception('Missing column in df_motif5: end_i') 
    
    if df_motif3 is not None:
        if type(df_motif3)==str: 
            df_motif3 = io.get(df_motif3)
        if 'fastq_file' not in df_motif3.columns.tolist():
            raise Exception('Missing column in df_motif3: fastq_file') 
        if 'start_i' not in df_motif3.columns.tolist():
            raise Exception('Missing column in df_motif3: start_i') 
    
    # Check if align_dims is a tuple of length 2 with start_i greater than end_i
    if align_dims is None:
        align_dims=(0,0)
    elif not isinstance(align_dims, tuple) or len(align_dims) != 2:
        raise ValueError(f"align_dims={align_dims} was not a tuple of length 2")
    else:
        if align_dims[0]<0 or align_dims[1]<0:
            raise ValueError(f"align_dims={align_dims} needs to be greater than 0")
        if align_dims[1]<align_dims[0]:
            raise ValueError(f"align_dims={align_dims} needs to be in the form (start_i, end_i)")

    # Check if align_ckpt is a positive integer
    if not isinstance(align_ckpt, int) or align_ckpt <= 0:
        raise ValueError(f"align_ckpt={align_ckpt} needs to be a positive integer")
    
    # Memory & stats reporting
    memories = []
    stats = []
    
    # Convert string representations into Signature objects
    df_ref[signature_col] = [parse_signature_literal(signature) for signature in df_ref[signature_col]]

    # Create output dataframe with counts and fraction from fastq files and reference dataframe
    out_df = pd.DataFrame() # Individual edits
    out_df2 = pd.DataFrame() # Aggregate edits
    if n_extra_nt>0:
        out_df3 = pd.DataFrame() # Individual edits minus extra nt differences
        out_df4 = pd.DataFrame() # Aggregate edits minus extra nt differences
    for fastq_file in os.listdir(fastq_dir): # Iterate through fastq files
        
        print(f"Processing {fastq_file}...") # Get reads
        regions = 0 # Keep track of # of reads with(out) regions
        missing5 = 0
        missing3 = 0
        overlap53 = 0
        seqs = [] # Store region sequences from reads with motifs
        
        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            
            if df_motif5 is not None:
                fastq_motif5 = df_motif5[df_motif5['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif5 info
            if df_motif3 is not None:
                fastq_motif3 = df_motif3[df_motif3['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif3 info
            
            if fastq_col is not None: 
                fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: 
                fastq_df_ref = df_ref.copy()
            
            with gzip.open(os.path.join(fastq_dir,fastq_file), "rt") as handle:
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i+1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue
                    
                    # Extract regions (between motifs) from reads
                    if df_motif5 is None and df_motif3 is None: # No motifs provided
                        # Use entire read as region
                        regions += 1
                        seqs.append(str(record.seq))

                    elif df_motif5 is not None and df_motif3 is None: # df_motif5 is provided
                        start_i = fastq_motif5.iloc[i]["end_i"] # Obtain motif5 boundary that define region

                        if start_i==-1: # motif5 is present
                            regions += 1
                            seqs.append(str(record.seq[start_i:])) 
                            
                        else: # 5' motif is missing
                            missing5 += 1 
                            seqs.append(None)

                    elif df_motif5 is None and df_motif3 is not None: # df_motif3 is provided
                        end_i = fastq_motif3.iloc[i]["start_i"] # Obtain motif3 boundary that define region

                        if end_i==-1: # motif3 is present
                            regions += 1
                            seqs.append(str(record.seq[:end_i])) 
                            
                        else: # 5' motif is missing
                            missing3 += 1 
                            seqs.append(None)

                    else: # Both motifs are provided
                        # Obtain motif boundaries that define region
                        start_i = fastq_motif5.iloc[i]["end_i"]
                        end_i = fastq_motif3.iloc[i]["start_i"]

                        if (start_i!=-1) & (end_i!=-1): # Both motifs are present
                            if start_i<end_i: # Motifs do not overlap
                                regions += 1
                                seqs.append(str(record.seq[start_i:end_i])) 
                            else: # Motifs overlap
                                overlap53 += 1
                                seqs.append(None)
                        elif (start_i==-1) & (end_i==-1): # Both motifs are missing
                            missing5 += 1
                            missing3 += 1
                            seqs.append(None)
                        elif start_i==-1: # 5' motif is missing
                            missing5 += 1 
                            seqs.append(None)
                        else: # 3' motif is missing
                            missing3 += 1 
                            seqs.append(None)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
        
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            
            if df_motif5 is not None:
                fastq_motif5 = df_motif5[df_motif5['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif5 info
            if df_motif3 is not None:
                fastq_motif3 = df_motif3[df_motif3['fastq_file']==fastq_file].reset_index(drop=True) # Isolate fastq motif3 info
            
            if fastq_col is not None: 
                fastq_df_ref = df_ref[df_ref[fastq_col]==fastq_file].reset_index(drop=True) # Isolate fastq reference library info (if needed)
            else: 
                fastq_df_ref = df_ref.copy()
            
            with open(os.path.join(fastq_dir,fastq_file), "r") as handle:    
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    
                    # Get # of reads in fastq file; skip reads that are not in the alignment range
                    reads = i+1
                    if (align_dims[0]!=0)&(align_dims[0]>i):
                        continue
                    elif (align_dims[1]!=0)&(align_dims[1]<=i):
                        continue
                    
                    # Extract regions (between motifs) from reads
                    if df_motif5 is None and df_motif3 is None: # No motifs provided
                        # Use entire read as region
                        regions += 1
                        seqs.append(str(record.seq))

                    elif df_motif5 is not None and df_motif3 is None: # df_motif5 is provided
                        start_i = fastq_motif5.iloc[i]["end_i"] # Obtain motif5 boundary that define region

                        if start_i==-1: # motif5 is present
                            regions += 1
                            seqs.append(str(record.seq[start_i:])) 
                            
                        else: # 5' motif is missing
                            missing5 += 1 
                            seqs.append(None)

                    elif df_motif5 is None and df_motif3 is not None: # df_motif3 is provided
                        end_i = fastq_motif3.iloc[i]["start_i"] # Obtain motif3 boundary that define region

                        if end_i==-1: # motif3 is present
                            regions += 1
                            seqs.append(str(record.seq[:end_i])) 
                            
                        else: # 5' motif is missing
                            missing3 += 1 
                            seqs.append(None)

                    else: # Both motifs are provided
                        # Obtain motif boundaries that define region
                        start_i = fastq_motif5.iloc[i]["end_i"]
                        end_i = fastq_motif3.iloc[i]["start_i"]

                        if (start_i!=-1) & (end_i!=-1): # Both motifs are present
                            if start_i<end_i: # Motifs do not overlap
                                regions += 1
                                seqs.append(str(record.seq[start_i:end_i])) 
                            else: # Motifs overlap
                                overlap53 += 1
                                seqs.append(None)
                        elif (start_i==-1) & (end_i==-1): # Both motifs are missing
                            missing5 += 1
                            missing3 += 1
                            seqs.append(None)
                        elif start_i==-1: # 5' motif is missing
                            missing5 += 1 
                            seqs.append(None)
                        else: # 3' motif is missing
                            missing3 += 1 
                            seqs.append(None)

                    # Processing status
                    if len(seqs)%align_ckpt==0: 
                        print(f"Processed {len(seqs)} reads")
        
        else: # Not a fastq file
            print("Not a fastq file")
            continue
        
        # Update memories & stats: # of reads with(out) the region
        print(f'{fastq_name}:\t{len(seqs)} reads\t=>\t{regions} reads;\t{missing5} missing motif5;\t{missing3} missing motif3;\t{overlap53} motif overlaps')
        stats.append((fastq_name,reads,len(seqs),regions,missing5,missing3,overlap53))
        memories.append(memory_timer(task=f"{fastq_file} (region)"))
        
        # Append # of reads & alignment range to fastq_df_ref
        previous_cols = list(fastq_df_ref.columns) # remove previous columns when duplicating metadata for WT & Not WT
        fastq_df_ref['reads_total']= [reads]*len(fastq_df_ref)
        fastq_df_ref['reads_processed']= [len(seqs)]*len(fastq_df_ref)
        if align_dims==(0,0): 
            fastq_df_ref['align_dims']= [(0,reads)]*len(fastq_df_ref)
        else:
            fastq_df_ref['align_dims']= [(align_dims[0],align_dims[1])] * len(fastq_df_ref)

        # Append metadata...
        fastq_df_ref['fastq_file'] = [fastq_file]*len(fastq_df_ref)
        
        if meta is not None: # ...and merge with meta
            if type(meta)==str: # Get from file path if needed
                meta = io.get(pt=meta)
            
            if id_col in list(meta.columns):
                meta.drop(columns=[id_col],inplace=True)

            if 'fastq_file' not in list(meta.columns): # Check for 'fastq_file' column
                print(f"Warning: Did not merge with meta.\nmeta needs 'fastq_file' column.\nDetected columns: {list(meta.columns)}")
            else: # Merge on 'fastq_file' column
                fastq_df_ref = pd.merge(left=fastq_df_ref,right=meta,on='fastq_file',how='left')
        
        # Add WT & Not WT
        Not_WT_WT_df = pd.concat([fastq_df_ref.iloc[0:1][[col for col in fastq_df_ref.columns if col not in previous_cols]]]*2, ignore_index=True)
        Not_WT_WT_df[id_col] = ['Not WT', 'WT']
        Not_WT_WT_df[edit_col] = ['Not WT', 'WT']
        fastq_df_ref = pd.concat([fastq_df_ref, Not_WT_WT_df], ignore_index=True)

        # Expand Signature Units from Signature Objects in the reference dataframe
        if n_extra_nt > 0:
            fastq_df_ref[f"{signature_col}_units"] = [expand_signature_units(signature) if id != 'Not WT' and id != 'WT' else None for id,signature in t.zip_cols(df=fastq_df_ref, cols=[id_col,signature_col])]

        # Perform alignments, compute Signatures, assign genotype id, & append to fastq dataframe to dictionary
        print("Perform Alignments & Compute Signatures")
        if save_alignments==True: alignment_ls = []
        signature_ls = []
        id_ls = []
        edit_ls = []
        exact_ls = []
        for s,read_seq in enumerate(seqs): # Iterate though sequences
            
            # Alignment Status
            if s==0: # Initial
                print(f'{s+1} out of {len(seqs)}') 

            elif s%align_ckpt==0: # Save checkpoint
                print(f'{s+1} out of {len(seqs)}')
                if save_alignments==True:
                    io.save(dir=os.path.join(out_dir,'Signature'), 
                            file=f'{fastq_name}.csv',
                            obj=pd.DataFrame({'Read_i': np.arange(0+align_dims[0],s+align_dims[0]),
                                              'Alignment': alignment_ls,
                                              'Signature': signature_ls,
                                              id_col: id_ls,
                                              edit_col: edit_ls}))
                else:
                    io.save(dir=os.path.join(out_dir,'Signature'), 
                            file=f'{fastq_name}.csv',
                            obj=pd.DataFrame({'Read_i': np.arange(0+align_dims[0],s+align_dims[0]),
                                              'Signature': signature_ls,
                                              id_col: id_ls,
                                              edit_col: edit_ls}))
                memories.append(memory_timer(task=f"{fastq_name} (alignment/signature {s+1} out of {len(seqs)})"))

            # Alignment -> Signature -> Genotype ID
            if read_seq is None: # Missing region
                if save_alignments==True: alignment_ls.append(None)
                signature_ls.append(None)
                id_ls.append(None)
                edit_ls.append(None)
                exact_ls.append(None)

            else: # Found region
                # Alignment & Signature
                alignment = aligner.align(Seq(ref_seq),Seq(read_seq))[0]
                signature = signature_from_alignment(ref_seq=ref_seq,
                                                     query_seq=read_seq,
                                                     alignment=alignment)
                if save_alignments==True: alignment_ls.append(alignment)
                signature_ls.append(signature)
                
                # WT: no SNVs or indels
                if not signature.snvs and not signature.indels:
                    id_ls.append('WT')
                    edit_ls.append('WT')
                    exact_ls.append('WT')
                    continue
                
                # Edit: search for exact signature match in reference dataframe
                found = False
                for edit, id, id_signature in t.zip_cols(df=fastq_df_ref,cols=[edit_col, id_col, signature_col]):
                    if signature == id_signature: # Found exact match
                        id_ls.append(id)
                        edit_ls.append(edit)
                        exact_ls.append(id)
                        found = True
                        break

                # Edit: search for near-match (n extra nt) in reference dataframe or declare Not WT
                if found == False:
                    found_w_error = False
                    if n_extra_nt>0: # Search for near match
                        signature_units = expand_signature_units(signature) # Expand Signature Units
                        for edit, id, id_signature_units in t.zip_cols(df=fastq_df_ref[(fastq_df_ref[id_col]!='WT') & (fastq_df_ref[id_col]!='Not WT')],
                                                                       cols=[edit_col, id_col, f"{signature_col}_units"]):
                            if is_reference_match_with_n_extra_nt_or_less(query=signature_units, 
                                                                          reference=id_signature_units, 
                                                                          n_extra_nt=n_extra_nt): # Found near match
                                id_ls.append(id)
                                edit_ls.append(edit)
                                exact_ls.append("Not WT")
                                found_w_error = True
                                break

                    # Not WT: no exact or near match found
                    if found_w_error == False:
                        id_ls.append("Not WT")
                        edit_ls.append("Not WT")
                        exact_ls.append("Not WT")

        if save_alignments==True: 
            df_fastq = pd.DataFrame({'Read_i': np.arange(0,len(signature_ls)),
                                    'Alignment': alignment_ls,
                                    'Signature': signature_ls,
                                    id_col: id_ls,
                                    edit_col: edit_ls,
                                    'Exact_match': exact_ls}) 
        else:
            df_fastq = pd.DataFrame({'Read_i': np.arange(0,len(signature_ls)),
                                     'Signature': signature_ls,
                                     id_col: id_ls,
                                     edit_col: edit_ls,
                                     'Exact_match': exact_ls}) 
        
        io.save(dir=os.path.join(out_dir,'Signature'), 
                file=f'{fastq_name}.csv', 
                obj=df_fastq)
        memories.append(memory_timer(task=f"{fastq_name} (alignment/signature)"))

        # Remove None
        df_fastq.dropna(subset=[signature_col], inplace=True, ignore_index=True)

        # Merge ID counts with reference dataframe [ID column], calculate fraction, & append to out dataframe
        fastq_df_ref_by_id = pd.merge(left=fastq_df_ref,right=df_fastq[id_col].value_counts().reset_index(), on=id_col, how='left')
        fastq_df_ref_by_id[edit_col] = [id if isinstance(edit, float) else edit for edit,id in t.zip_cols(df=fastq_df_ref_by_id, cols=[edit_col, id_col])]
        fastq_df_ref_by_id.fillna(value={'count': 0},inplace=True)
        total_count = sum(fastq_df_ref_by_id['count'])
        fastq_df_ref_by_id['fraction'] = [cts/total_count for cts in fastq_df_ref_by_id['count']]
        fastq_df_ref_by_id['fastq_file'] = [fastq_file]*len(fastq_df_ref_by_id)
        out_df = pd.concat([out_df,fastq_df_ref_by_id], ignore_index=True)

        # Total Editing Outcomes
        cts = 0
        fracs = 0
        for edit,count,fraction in t.zip_cols(df=fastq_df_ref_by_id,cols=[edit_col,'count','fraction']):
            if edit!='WT' and edit!='Not WT':
                cts += count
                fracs += fraction

        fastq_df_ref_by_id_agg = fastq_df_ref_by_id[fastq_df_ref_by_id[id_col].isin(['WT','Not WT'])].reset_index(drop=True)
        fastq_df_ref_by_id_agg = pd.concat([fastq_df_ref_by_id_agg,
                                            pd.DataFrame({id_col: ['Edit'],
                                                        edit_col: ['Edit'],
                                                        'count': [cts],
                                                        'fraction': [fracs],
                                                        'fastq_file': [fastq_file]})], ignore_index=True)
        out_df2 = pd.concat([out_df2,fastq_df_ref_by_id_agg], ignore_index=True)

        # Save memory & clear for next use
        del total_count
        del fastq_df_ref_by_id
        del fastq_df_ref_by_id_agg

        # Merge Exact_match counts with reference dataframe [ID column], calculate fraction, & append to out dataframe
        if n_extra_nt>0:
            fastq_df_ref_by_exact = pd.merge(left=fastq_df_ref,right=df_fastq['Exact_match'].value_counts().reset_index(), left_on=id_col, right_on="Exact_match", how='left')
            fastq_df_ref_by_exact[edit_col] = [id if isinstance(edit, float) else edit for edit,id in t.zip_cols(df=fastq_df_ref_by_exact, cols=[edit_col, id_col])]
            fastq_df_ref_by_exact.fillna(value={'count': 0},inplace=True)
            total_count = sum(fastq_df_ref_by_exact['count'])
            fastq_df_ref_by_exact['fraction'] = [cts/total_count for cts in fastq_df_ref_by_exact['count']]
            fastq_df_ref_by_exact['fastq_file'] = [fastq_file]*len(fastq_df_ref_by_exact)
            out_df3 = pd.concat([out_df3,fastq_df_ref_by_exact], ignore_index=True)

            # Total Editing Outcomes minus extra nt differences
            cts = 0
            fracs = 0
            for edit,count,fraction in t.zip_cols(df=fastq_df_ref_by_exact,cols=[edit_col,'count','fraction']):
                if edit!='WT' and edit!='Not WT':
                    cts += count
                    fracs += fraction

            fastq_df_ref_by_exact_agg = fastq_df_ref_by_exact[fastq_df_ref_by_exact[id_col].isin(['WT','Not WT'])].reset_index(drop=True)
            fastq_df_ref_by_exact_agg = pd.concat([fastq_df_ref_by_exact_agg,
                                                    pd.DataFrame({id_col: ['Edit'],
                                                                edit_col: ['Edit'],
                                                                'count': [cts],
                                                                'fraction': [fracs],
                                                                'fastq_file': [fastq_file]})], ignore_index=True)
            out_df4 = pd.concat([out_df4,fastq_df_ref_by_exact_agg], ignore_index=True)

            # Save memory & clear for next use
            del total_count
            del fastq_df_ref_by_exact
            del fastq_df_ref_by_exact_agg

        # Save memory & clear for next use
        del df_fastq

    # Save, plot, and return
    memories.append(memory_timer(task='count_signature()'))
    io.save(dir=os.path.join(out_dir,'.count_signature'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_stats.csv',
            obj=pd.DataFrame(stats, columns=['file','reads_total','reads_processed','reads_w_region','reads_wo_motif5','reads_wo_motif3','reads_w_motif_overlap']))
    io.save(dir=os.path.join(out_dir,'.count_signature'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))
    
    io.save(dir=out_dir,file=out_file,obj=out_df)
    io.save(dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_aggregate.{out_file.split('.')[-1]}",obj=out_df2)
    stack(df=out_df,x='fastq_file',y='fraction',cols=edit_col,vertical=False,
          palette_or_cmap='tab20',repeats=math.ceil(len(out_df[edit_col].unique())/20),cutoff_group='fastq_file',cutoff_value=0,legend_bbox_to_anchor=(0,-.1),legend_ncol=8,
          figsize=(15,10), title='Edit Outcomes', dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}{plot_suf}", show=show, **plot_kwargs)
    stack(df=out_df2,x='fastq_file',y='fraction',cols=edit_col,vertical=False,
          palette_or_cmap='tab20',repeats=1,cutoff_group='fastq_file',cutoff_value=0,legend_bbox_to_anchor=(0,-.1),legend_ncol=3,
          figsize=(15,10), title='Edit Outcomes', dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_aggregate{plot_suf}", show=show, **plot_kwargs)
    
    if n_extra_nt>0:
        io.save(dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_wo_{n_extra_nt}_extra_nt.{out_file.split('.')[-1]}",obj=out_df3)
        io.save(dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_wo_{n_extra_nt}_extra_nt_aggregate.{out_file.split('.')[-1]}",obj=out_df4)
        stack(df=out_df3,x='fastq_file',y='fraction',cols=edit_col,vertical=False,
            palette_or_cmap='tab20',repeats=math.ceil(len(out_df3[edit_col].unique())/20),cutoff_group='fastq_file',cutoff_value=0,legend_bbox_to_anchor=(0,-.1),legend_ncol=8,
            figsize=(15,10), title='Edit Outcomes', dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_wo_{n_extra_nt}_extra_nt{plot_suf}", show=show, **plot_kwargs)
        stack(df=out_df4,x='fastq_file',y='fraction',cols=edit_col,vertical=False,
            palette_or_cmap='tab20',repeats=1,cutoff_group='fastq_file',cutoff_value=0,legend_bbox_to_anchor=(0,-.1),legend_ncol=3,
            figsize=(15,10), title='Edit Outcomes', dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_wo_{n_extra_nt}_extra_nt_aggregate{plot_suf}", show=show, **plot_kwargs)

    if return_df: return out_df

# Quantify edit outcomes
def trim_filter(record, qall:int, qavg:int, qtrim:int, qmask:int, alls:int,
                avgs:int, trims:int, masks:int) -> tuple:
    '''
    trim_filter(): trim and filter fastq sequence based on quality scores
    
    Parameters:
    record: Bio.SeqIO fastq record
    qall (int): phred quality score threshold for all bases for a read to not be discarded
    qtrim (int): phred quality score threshold for trimming reads on both ends
    qavg (int): average phred quality score threshold for a read to not be discarded
    qmask (int): phred quality score threshold for base to not be masked to N
    alls (int): count of records that were dropped due to qall threshold
    avgs (int): count of records that were dropped due to qavg threshold
    trims (int): count of records that were trimmed due to qtrim threshold
    masks (int): count of records that had bases masked due to qmask threshold
    
    Dependencies: Bio.SeqIO, gzip, os, pandas, & Bio.Seq.Seq
    '''
    if all(score >= qall for score in record.letter_annotations['phred_quality']): # All threshold
        if np.mean(record.letter_annotations['phred_quality']) >= qavg: # Avg threshold
            
            quality_scores = record.letter_annotations['phred_quality'] # Set 5' & 3' trim indexes to the start and end
            trim_5 = 0 
            trim_3 = len(quality_scores)
            sequence = record.seq
            
            if qtrim!=0: # Save compute time if trim is not desired
                for i in range(len(quality_scores)): # Find 5' trim
                    if quality_scores[i] >= qtrim: break
                    trim_5 = i
                for i in reversed(range(len(quality_scores))): # Find 3' trim
                    if quality_scores[i] >= qtrim: break
                    trim_3 = i
                if (trim_5!=0)|(trim_3!=len(quality_scores)): trims += 1 # Trimmed read

            sequence = sequence[trim_5:trim_3] # Trim the sequence and quality scores
            quality_scores = quality_scores[trim_5:trim_3]

            
            bases = list(sequence) # Mask bases with 'N' threshold
            if masks !=0: # Save compute time if mask is not desired
                for i, qual in enumerate(quality_scores):
                    if qual < qmask: bases[i] = 'N'
            sequenceN = Seq('').join(bases) # Update the sequence with the modified version
            if Seq('N') in sequenceN: masks += 1

            return record.id,sequence,sequenceN,quality_scores,alls,avgs,trims,masks
    
        else: return None,None,None,None,alls,avgs+1,trims,masks # Avg threshold not met
    else: return None,None,None,None,alls+1,avgs,trims,masks # All threshold not met

def get_fastqs(in_dir: str, qall:int=10, qavg:int=30, qtrim:int=0, qmask:int=0, save:bool=True, 
               return_memories: bool=False, out_dir: str=None) -> tuple[dict[pd.DataFrame], list] | dict[pd.DataFrame]:
    ''' 
    get_fastqs(): get fastq files from directory and store records in dataframes in a dictionary
    
    Parameters:
    dir (str): directory with fastq files
    qall (int, optional): phred quality score threshold for all bases for a read to not be discarded (Q = -log(err))
    qtrim (int, optional): phred quality score threshold for trimming reads on both ends (Q = -log(err))
    qavg (int, optional): average phred quality score threshold for a read to not be discarded (Q = -log(err))
    qmask (int, optional): phred quality score threshold for base to not be masked to N (Q = -log(err))
    save (bool, optional): save reads statistics file to local directory (Default: True)
    return_memories (bool, optional): return memories (Default: False)
    out_dir (str, optional): directory to save reads statistics file (Default: None)

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, & trim_filter()
    '''
    # Memory reporting
    memories = []

    # Make fastqs dictionary
    fastqs = dict()
    if save == True: out = pd.DataFrame()
    for fastq_file in os.listdir(in_dir): # Iterate through fastq files
        reads = 0 
        alls = 0 # Keep track of reads & outcomes
        avgs = 0
        trims = 0
        masks = 0
        ids=[]
        seqs=[]
        seqsN=[]
        phred_scores=[]

        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            with gzip.open(os.path.join(in_dir,fastq_file), "rt") as handle:

                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    reads=r+1
                    record_id,record_seq,record_seqN,record_scores,alls,avgs,trims,masks = trim_filter(record,qall,qavg,qtrim,qmask,alls,avgs,trims,masks) # Trim & filter
                    if record_id is not None: # Save id, sequence, & quality scores
                        ids.append(record_id) 
                        seqs.append(record_seq)
                        seqsN.append(record_seqN)
                        phred_scores.append(record_scores)

        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            with open(os.path.join(in_dir,fastq_file), "r") as handle:

                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    reads=r+1
                    record_id,record_seq,record_seqN,record_scores,alls,avgs,trims,masks = trim_filter(record,qall,qavg,qtrim,qmask,alls,avgs,trims,masks) # Trim & filter
                    if record_id is not None: # Save id, sequence, sequence masked, & quality scores
                        ids.append(record_id) 
                        seqs.append(record_seq)
                        seqsN.append(record_seqN)
                        phred_scores.append(record_scores)
        
        else: # Not a fastq file
            continue

        fastqs[fastq_name]=pd.DataFrame({'id':ids, # Add dataframe to dictionary 
                                         'seq':seqs,
                                         'seqN':seqsN,
                                         'phred_scores':phred_scores})
        print(f'{fastq_name}:\t{reads} reads\t=>\t{len(fastqs[fastq_name])} reads (alls = {alls} & avgs = {avgs});\t{trims} trimmed reads;\t{masks} masked reads')
        if save==True: out = pd.concat([out,
                                        pd.DataFrame({'file': [fastq_name],
                                                      'reads': [reads],
                                                      'reads_filtered': [len(fastqs[fastq_name])],
                                                      'reads_dropped_all': [alls],
                                                      'reads_dropped_avg': [avgs],
                                                      'reads_trimmed': [trims],
                                                      'reads_masked': [masks]})])
        memories.append(memory_timer(task=fastq_name))

    if save==True: 
        if out_dir is None: out_dir = '.'
        io.save(dir=os.path.join(out_dir,'.genotyping'),file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_get_fastqs.csv',obj=out)
    
    if return_memories: return fastqs,memories
    else: return fastqs

def region(fastqs: dict, flank5: str='', flank3: str='', save: bool=True, masks: bool=False, 
           return_memories: bool=False, out_dir: str=None) -> tuple[dict[pd.DataFrame], list] | dict[pd.DataFrame]:
    ''' 
    region(): gets DNA and AA sequence for records within flanks
    
    Parameters:
    fastqs (dict): dictionary from get_fastqs
    flank5 (str, optional): top strand flanking sequence 5' (Default: '')
    flank3 (str, optional): top strand flanking sequence 3' (Default: '')
    save (bool, optional): save reads statistics file to local directory (Default: True)
    masks (bool, optional): include masked sequence and translation (Default: False)
    return_memories (bool, optional): return memories (Default: False)
    out_dir (str, optional): directory to save reads statistics file (Default: None)
    
    Dependencies: pandas & Bio.Seq.Seq
    '''
    # Memory reporting
    memories = []

    # Flanks...
    if flank5!='' and flank3!='': # both are provided

        # Check flank lengths
        if (len(flank5)<9)|(len(flank3)<9): print('Warning: flank5 or flank3 less than 9.')

        # Remove fastq records that do not have flanks
        fastqs_1=dict()
        missing_flank5s = []
        missing_flank3s = []
        for file,fastq in fastqs.items():
            missing_flank5 = set()
            missing_flank3 = set()
            for i,seq in enumerate(fastq['seq']):
                if seq.find(flank5)==-1:
                    missing_flank5.add(i)
                if (seq.find(flank3)==-1) | (seq.find(flank3)<seq.find(flank5)+len(flank5)): # flank3 not found or before flank5
                    missing_flank3.add(i)

            fastqs_1[file] = fastq.drop(sorted(missing_flank5.union(missing_flank3))).reset_index(drop=True)
            missing_flank5s.append(len(missing_flank5))
            missing_flank3s.append(len(missing_flank3))

        # Retain fastqs file length
        fastqs_reads_filtered = {file:len(fastqs[file]) for file in fastqs.keys()}
        del fastqs # Save memory

        # Obtain nucleotide and AA sequences within flanks; remove fastq records with phred scores within flanks
        if save == True: out = pd.DataFrame()
        for j,(file,fastq) in enumerate(fastqs_1.items()):
            nuc=[]
            prot=[]
            if masks==True:
                nucN=[]
                protN=[]
            
            for i,seq in enumerate(fastq['seq']):
                nuc.append(seq[seq.find(flank5)+len(flank5):seq.find(flank3)])
                prot.append(Seq.translate(seq[seq.find(flank5)+len(flank5):seq.find(flank3)]))
                if masks==True:
                    nucN.append(fastq.iloc[i]['seqN'][seq.find(flank5)+len(flank5):seq.find(flank3)])
                    protN.append(Seq.translate(fastq.iloc[i]['seqN'][seq.find(flank5)+len(flank5):seq.find(flank3)]))
            
            fastqs_1[file]['nuc']=nuc
            fastqs_1[file]['prot']=prot
            if masks==True:
                fastqs_1[file]['nucN']=nucN
                fastqs_1[file]['protN']=protN
            
            print(f'{file}:\t{fastqs_reads_filtered[file]} reads\t=>\t{len(fastqs_1[file])} reads;\tmissing {missing_flank5s[j]} flank5;\tmissing {missing_flank3s[j]} flank3')
            if save==True: out = pd.concat([out,
                                            pd.DataFrame({'file': [file],
                                                        'reads_filtered': [fastqs_reads_filtered[file]],
                                                        'reads_w_flanks': [len(fastqs_1[file])],
                                                        'reads_wo_flank5': [missing_flank5s[j]],
                                                        'reads_wo_flank3': [missing_flank3s[j]]})
                                            ])
            memories.append(memory_timer(task=file))
    
    elif flank5!='' or flank3!='': # one is provided
        raise ValueError('Error: Both flank5 and flank3 must be provided, or neither.')

    else: # none are provided

        # Don't remove fastq records based on flanks
        fastqs_1 = fastqs.copy()
        missing_flank5s = [0]*len(fastqs)
        missing_flank3s = [0]*len(fastqs)
        del fastqs # Save memory

        # Retain fastqs file length
        fastqs_reads_filtered = {file:len(fastqs_1[file]) for file in fastqs_1.keys()}

        # Obtain nucleotide and AA sequences within flanks; remove fastq records with phred scores within flanks
        if save == True: out = pd.DataFrame()
        for j,(file,fastq) in enumerate(fastqs_1.items()):
            nuc=[]
            prot=[]
            if masks==True:
                nucN=[]
                protN=[]
            
            for i,seq in enumerate(fastq['seq']):
                nuc.append(seq)
                prot.append(Seq.translate(seq))
                if masks==True:
                    nucN.append(fastq.iloc[i]['seqN'])
                    protN.append(Seq.translate(fastq.iloc[i]['seqN']))

            fastqs_1[file]['nuc']=nuc
            fastqs_1[file]['prot']=prot
            if masks==True:
                fastqs_1[file]['nucN']=nucN
                fastqs_1[file]['protN']=protN
            
            print(f'{file}:\t{fastqs_reads_filtered[file]} reads\t=>\t{len(fastqs_1[file])} reads;\tmissing {missing_flank5s[j]} flank5;\tmissing {missing_flank3s[j]} flank3')
            if save==True: out = pd.concat([out,
                                            pd.DataFrame({'file': [file],
                                                        'reads_filtered': [fastqs_reads_filtered[file]],
                                                        'reads_w_flanks': [len(fastqs_1[file])],
                                                        'reads_wo_flank5': [missing_flank5s[j]],
                                                        'reads_wo_flank3': [missing_flank3s[j]]})
                                            ])
            memories.append(memory_timer(task=file))
    
    if save==True: 
        if out_dir is None: out_dir = '.'
        io.save(dir=os.path.join(out_dir,'.genotyping'),file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_region.csv',obj=out)
    
    if return_memories: return fastqs_1,memories
    else: return fastqs_1

### Supporting methods for genotype()
def find_AA_edits(wt: str | Seq, res: int, seq: str) -> str:
    '''
    find_AA_edits(): find amino acid edits compared to wildtype sequence
    
    Parameters:
    wt (str): expected wildtype AA sequence
    res (int): first AA number
    seq (str): amino acid sequence to compare against wildtype
    '''
    # Find amino acid edits
    e=[]
    for j, (a, b) in enumerate(zip(wt, seq)):
        if a != b: e.append(a+str(j+res)+b)

    # Combine & return edit(s)    
    if len(e)>1: return ", ".join(e)
    elif len(e)==1: return e[0]
    else: return 'Unknown'

def trim(seq: str|Seq) -> str|Seq:
    """
    trim(): trim the sequence to a multiple of 3.

    Parameters:
    seq (str|Seq): The sequence to be trimmed.
    """
    while len(seq) % 3 != 0:
        seq = seq[:-1]
    return seq

def format_alignment(a: str|Seq, b: str|Seq, show: bool=False, return_alignment: bool=False) -> str:
    '''
    format_alignment(): formats two sequences for alignment display & return the middle.
    
    Parameters:
    a (str|Seq): The first sequence.
    b (str|Seq): The second sequence.
    show (bool, optional): Print the formatted alignment (Default: False).
    return_alignment (bool, optional): Returns the full formatted alignment (True) or just the middle alignment (False; Default: False).
    '''
    # Determine the middle of the alignment
    mid = []
    for x, y in zip(a, b):
        if x == y:
            mid.append('|')
        elif x == '-' or y == '-':
            mid.append('-')
        else:
            mid.append('.')
    mid = ''.join(mid)

    # Format the sequences for display & return
    if show: 
        print(f"{a}\n{mid}\n{b}")
    if return_alignment is True:
        return f"{a}\n{mid}\n{b}"
    else:
        return mid

def find_indel(wt:str|Seq, mut:str|Seq, res: int, show:bool=False,
               match_score: float = 2, mismatch_score: float = -1, 
               open_gap_score: float = -10, extend_gap_score: float = -0.1) -> tuple[str, str]:
    '''
    find_indel(): aligns two sequences and returns the indel edit.

    Parameters:
    wt (str|Seq): The wild type sequence.
    mut (str|Seq): The mutant sequence.
    res (int): The first amino acid number in the sequence.
    show (bool): If True, prints the formatted alignment.
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)

    Dependencies: Biopython
    '''
    # High sequence homology; punish gaps
    aligner = PairwiseAligner()
    aligner.mode = "global"
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = open_gap_score  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = extend_gap_score  # Penalty for extending a gap; applied to both strands

    # Get the best protein alignment
    alignment = aligner.align(Seq(trim(wt)).translate(),Seq(trim(mut)).translate())[0]
    mid = format_alignment(alignment[0], alignment[1], show=show, return_alignment=False) # Just get the middle alignment

    if len(mut)%3!=0: # Frameshift Indel
        
        # Find the first occurrence of '-' or '.' in the middle alignment
        starts = [mid.find('-'), mid.find('.')]
        start = min(starts) if min(starts) != -1 else max(starts) # Handles case where '-' or '.' is not found
        
        # Check start for AA
        reverse = True
        for i in range(start, len(alignment[0])): # Find AA from start to end of the alignemnt
            if alignment[0][i] != '-':
                start = i
                reverse = False
                break
        
        if reverse: # Find AA from start to beginning of the alignment
            for i in range(0,start,-1):
                if alignment[0][i] != '-':
                    start = i
                    break

        # Format & return the edit
        edit = f"{alignment[0][start]}{start+res-alignment[0].count('-',0,start)}fs"
        category = 'Frameshift Indel'
        if show: print(f"{edit} ({category})")
        return edit,category
            
    elif alignment[0].count('-') > 0 and alignment[1].count('-') > 0: # In-frame Indel
        
        # Find the first occurrence of '-' or '.' in the middle alignment
        starts = [mid.find('-'), mid.find('.')]
        start = min(starts) if min(starts) != -1 else max(starts) # Handles case where '-' or '.' is not found
        
        num = start + res - 1 - alignment[0].count('-',0,start)
        
        # Find the last occurrence of '-' or '.' in the middle alignment
        end = max(mid.rfind('-')+1,mid.rfind('.')+1)

        # WT residues
        befores = [aa_wt for aa_wt in alignment[0][start-1:end+1]]
        before = ''.join(befores).replace('-', '') # remove replace to include "-"
        
        # Mut residues
        afters = [aa_mut for aa_mut in alignment[1][start-1:end+1]]
        after = ''.join(afters).replace('-', '') # remove replace to include "-"
        
        # Format & return the edit
        edit = f"{before}{num}{after}" 
        if "*" in alignment[1]:
            category = 'Stop Codon Indel'
        else:
            category = 'In-frame Indel'
        if show: print(f"{edit} ({category})")
        return edit,category

    elif alignment[0].count('-') > 0: # In-frame Insertion
        
        # Find the first occurrence of '-' or '.' in the middle alignment
        starts = [mid.find('-'), mid.find('.')]
        start = min(starts) if min(starts) != -1 else max(starts) # Handles case where '-' or '.' is not found
        
        num = start + res - 1 - alignment[0].count('-',0,start)
        
        # Find the last occurrence of '-' or '.' in the middle alignment
        end = max(mid.rfind('-')+1,mid.rfind('.')+1)
        
        # WT residues
        befores = [aa_wt for aa_wt in alignment[0][start-1:end-1]]
        before = ''.join(befores).replace('-', '') # remove replace to include "-"
        
        # Mut residues
        afters = [aa_mut for aa_mut in alignment[1][start-1:end]]
        after = ''.join(afters).replace('-', '') # remove replace to include "-"
        
        # Format & return the edit
        edit = f"{before}{num}{after}"  
        if "*" in alignment[1]:
            category = 'Stop Codon Indel'
        else:
            category = 'In-frame Insertion'
        if show: print(f"{edit} ({category})")
        return edit,category
                
    elif alignment[1].count('-') > 0: # In-frame Deletion
        
        # Find the first occurrence of '-' or '.' in the middle alignment
        starts = [mid.find('-'), mid.find('.')]
        start = min(starts) if min(starts) != -1 else max(starts)
        
        num = start + res - alignment[0].count('-',0,start)
        
        # Find the last occurrence of '-' or '.' in the middle alignment
        end = max(mid.rfind('-')+1,mid.rfind('.')+1)
        
        # WT residues
        befores = [aa_wt for aa_wt in alignment[0][start:end+1]]
        before = ''.join(befores).replace('-', '') # remove replace to include "-"
        
        # Mut residues
        afters = [aa_mut for aa_mut in alignment[1][start:end+1]]
        after = ''.join(afters).replace('-', '') # remove replace to include "-"

        # Format & return the edit
        edit = f"{before}{num}{after}" 
        if "*" in alignment[1]:
            category = 'Stop Codon Indel'
        else:
            category = 'In-frame Deletion'
        if show: print(f"{edit} ({category})")
        return edit,category
    
    else: # Unknown Indel

        # Format & return the edit
        edit = 'Unknown'
        category = 'Indel'
        if show: print(f"{edit} ({category})")
        return edit,category

def genotype(fastqs: dict, res: int, wt: str, save: bool=False, masks: bool=False, keepX: bool=False,
             match_score: float = 2, mismatch_score: float = -1, open_gap_score: float = -10, 
             extend_gap_score: float = -0.1, return_memories: bool=False, out_dir: str=None) -> tuple[dict[pd.DataFrame], list] | dict[pd.DataFrame]:
    ''' 
    genotype(): assign genotypes to sequence records
    
    Parameters:
    fastqs (dict): dictionary from filter_fastqs
    res (int): first AA number
    wt (str, optional 2): expected wildtype nucleotide sequence (in frame AA; required unless pt is provided)
    save (bool, optional): save genotyped reads to local directory (Default: False)
    masks (bool, optional): include masked sequence and translation (Default: False)
    keepX (bool, optional): keep unknown translation (i.e., X) due to sequencing error (Default: False) 
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)
    return_memories (bool, optional): return memories (Default: False)
    out_dir (str, optional): directory to save genotyped reads (Default: None)
    
    Dependencies: pandas & Bio.Seq.Seq

    Note: Need to add single indels eventually
    '''
    # Memory reporting
    memories = []

    # Get wildtype protein sequence
    if len(wt)%3!=0: # Check if wildtype sequence is provided in-frame
        raise(ValueError(f'WT sequence is not in-frame:\n{wt}'))
    wt_prot = Seq.translate(Seq(wt))

    # Iterate through fastq files
    for file,fastq in fastqs.items():
        
        # Save edits & corresponding categories
        edits=[]
        categories=[]
        editsN=[]
        categoriesN=[]

        for i in range(len(fastq['prot'])): # Iterate through translated sequences
            if len(fastq.iloc[i]['prot'])==0: # Empty sequence?
                edits.append('Unknown')
                categories.append('Flanks')
                if masks==True: 
                    editsN.append('Unknown')
                    categoriesN.append('Flanks')
                
            elif len(wt)!=len(fastq.iloc[i]['nuc']): # Indel
                edit,category = find_indel(wt=wt, mut=fastq.iloc[i]['nuc'], res=res, show=False, 
                                           match_score=match_score, mismatch_score=mismatch_score,
                                           open_gap_score=open_gap_score, extend_gap_score=extend_gap_score)
                edits.append(edit)
                categories.append(category)
                if masks == True: 
                    edit,category = find_indel(wt=wt, mut=fastq.iloc[i]['nucN'], res=res, show=False, 
                                               match_score=match_score, mismatch_score=mismatch_score,
                                               open_gap_score=open_gap_score, extend_gap_score=extend_gap_score)
                    editsN.append(edit)
                    categoriesN.append(category)
            
            elif wt_prot==fastq.iloc[i]['prot']: # WT sequence
                edits.append('WT')
                categories.append('WT')
                if masks==True: 
                    editsN.append('WT')
                    categoriesN.append('WT')

            else: # Substitution(s) without indels
                edits.append(find_AA_edits(wt=wt_prot, res=res, seq=fastq.iloc[i]['prot']))
                categories.append('Substitution') 
                if masks==True: 
                    editsN.append(find_AA_edits(wt=wt_prot, res=res, seq=fastq.iloc[i]['protN']))
                    categoriesN.append('Substitution')
        
        fastqs[file]['Edit']=edits
        fastqs[file]['Category']=categories
        if masks==True: 
            fastqs[file]['EditN']=editsN
            fastqs[file]['CategoryN']=categoriesN
        print(f'{file}:\t{len(fastqs[file])} reads')
        memories.append(memory_timer(task=file))
    
    if save==True: 
        if out_dir is None: out_dir = '.'
        io.save(dir=os.path.join(out_dir,'.genotyping'),file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_genotype.csv',obj=t.reorder_cols(df=t.join(dc=fastqs,col='fastq_file'),cols=['fastq_file']))
    
    if return_memories: return fastqs,memories
    else: return fastqs

def outcomes(fastqs: dict, col: str='Edit', return_memories: bool=False) -> tuple[pd.DataFrame, list] | pd.DataFrame:
    ''' 
    outcomes(): returns edit count & fraction per sample (tidy format)

    Parameters:
    fastqs (dict): dictionary from genotype
    col (str, optional): column name (Default: Edit)
    return_memories (bool, optional): return memories (Default: False)
    
    Dependencies: pandas
    '''
    # Memory reporting
    memories = []

    df = pd.DataFrame()
    for file,fastq in fastqs.items():
        temp=pd.DataFrame({'fastq_file':[file]*len(fastq[col].value_counts()),
                           col:list(fastq[col].value_counts().keys()),
                           'count':fastq[col].value_counts(),
                           'fraction':fastq[col].value_counts()/len(fastq[col])})
        df=pd.concat([df,temp]).reset_index(drop=True)
        memories.append(memory_timer(task=file))

    if return_memories: return df,memories
    else: return df

def genotyping(in_dir: str, config_key: str=None, sequence: str=None, res: int=None,
               out_dir: str=None, out_file_prefix: str=None, return_dc:bool=False, **kwargs) -> dict[pd.DataFrame]:
    ''' 
    genotying(): quantify edit outcomes workflow
    
    Parameters:
    in_dir (str): directory with fastq files
    config_key (str, optional 1): config file key (FWD primer-REV primer) with 'sequence' & 'res'
    sequence (str, optional 2): sequence formatted flank5(genotype region)flank3 or genotype region only
    res (int, optional 2): first AA number in genotype region
    out_dir (str, optional): output directory (Default: None)
    out_file (str, optional): output file (Default: None)
    return_dc (bool, optional): return dictionary containing edit & category outcomes dataframes (Default: False)

    **kwargs:
    qall (int, optional): phred quality score threshold for all bases for a read to not be discarded (Q = -log(err))
    qtrim (int, optional): phred quality score threshold for trimming reads on both ends (Q = -log(err))
    qavg (int, optional): average phred quality score threshold for a read to not be discarded (Q = -log(err))
    qmask (int, optional): phred quality score threshold for base to not be masked to N (Q = -log(err))
    save (bool, optional): save reads statistics and genotype file to local directory (Default: True)
    masks (bool, optional): include masked sequence and translation (Default: False)
    keepX (bool, optional): keep unknown translation (i.e., X) due to sequencing error (Default: False) 
    match_score (float, optional): match score for pairwise alignment (Default: 2)
    mismatch_score (float, optional): mismatch score for pairwise alignment (Default: -1)
    open_gap_score (float, optional): open gap score for pairwise alignment (Default: -10)
    extend_gap_score (float, optional): extend gap score for pairwise alignment (Default: -0.1)
    
    Dependencies: get_fastq(), region(), genotype(), outcomes(), outcomes_desired()
    '''
    # Initialize timer and memory reporting
    memory_timer(reset=True)
    memories = []
    kwargs['return_memories'] = True # Return memories for all methods
    
    # Check config file
    if config_key is not None:
        config_key = config.get_info(id=config_key)
        sequence = config_key['sequence']
        res = config_key['res']
    
    # Check sequence 
    if len(re.findall(r'[()]+', sequence)) == 0: # and genotype region
        flank5 = ''
        wt = sequence
        flank3 = ''
        print(f'Warning - Skipping region() because missing "(" or ")" in sequence:\n{sequence}')

    elif len(re.findall(r'[(]+', sequence)) == 1 and len(re.findall(r'[)]+', sequence)) == 1: # and obtain flank5(genotype region)flank3
        flank5 = sequence.split('(')[0]
        wt = sequence.split('(')[1].split(')')[0]
        flank3 = sequence.split(')')[1]

    else: # incorrect sequence format
        raise(ValueError(f'Incorrect sequence format:\n{sequence}\nExpected format: flank5(genotype region)flank3 or genotype region only'))
    
    if len(wt)%3!=0:
        raise(ValueError(f'WT sequence is not in-frame:\n{wt}'))

    # Split **kwargs
    get_fastqs_kw = ['qall','qtrim','qavg','qmask','save','return_memories'] # get_fastq()
    region_kw = ['save','masks','return_memories'] # region()
    genotype_kw = ['save','masks','keepX','match_score','mismatch_score','open_gap_score','extend_gap_score','return_memories'] # genotype()
    outcomes_kw = ['return_memories'] # outcomes()

    get_fastqs_kwargs = {k:kwargs[k] for k in get_fastqs_kw if k in kwargs}
    region_kwargs = {k:kwargs[k] for k in region_kw if k in kwargs}
    genotype_kwargs = {k:kwargs[k] for k in genotype_kw if k in kwargs}
    outcomes_kwargs = {k:kwargs[k] for k in outcomes_kw if k in kwargs}

    # Quantify edit outcomes workflow
    memories.append(memory_timer(task='get_fastqs()'))
    dc,memories1 = get_fastqs(in_dir=in_dir,out_dir=out_dir,**get_fastqs_kwargs)
    memories.extend(memories1)

    memories.append(memory_timer(task='region()'))
    dc,memories1 = region(fastqs=dc,flank5=flank5,flank3=flank3,out_dir=out_dir,**region_kwargs)
    memories.extend(memories1)

    memories.append(memory_timer(task='genotype()'))
    dc,memories1 = genotype(fastqs=dc,res=res,wt=wt,out_dir=out_dir,**genotype_kwargs)
    memories.extend(memories1)

    memories.append(memory_timer(task='outcomes()'))
    df_edits,memories1 = outcomes(fastqs=dc, **outcomes_kwargs)
    memories.extend(memories1)
    df_categories,memories1  = outcomes(fastqs=dc, col='Category', **outcomes_kwargs)
    memories.extend(memories1)
    del dc # Remove dc to save memory

    # Save and return edit outcomes dataframe
    memories.append(memory_timer(task='genotyping()'))
    if out_dir is not None and out_file_prefix is not None: # Save dataframes (optional)
        io.save(dir=os.path.join(out_dir,'.genotyping'), # memory reporting
                file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
                obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

        io.save(dir=out_dir,file=f'{out_file_prefix}_edit_outcomes.csv',obj=df_edits) # Edit outcomes
        io.save(dir=out_dir,file=f'{out_file_prefix}_category_outcomes.csv',obj=df_categories) # Edit categoy outcomes
    
    if return_dc: 
        return {'Edit': df_edits,
                'Category': df_categories}

def abundances(df: pd.DataFrame | str, desired_edits: list, edit_col: str='Edit', combinations: int=1) -> pd.DataFrame:
    ''' 
    abundances(): isolate desired edits count & fraction per sample

    Parameters:
    df (DataFrame | str): dataframe with edit count & fraction per sample (tidy format)
    desired_edits (list): list of desired edits to isolate
    edit_col (str, optional): edit column name (Default: Edit)
    combinations (int, optional): Maximum # of desired edit combinations to search for (Default: 1 => single edits, 2 => paired edits, etc.)

    Dependencies: pandas
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Search for multple edits
    if combinations > 1:

        desired_edits_combos = []

        for r in range(1, combinations + 1):
            for combo in itertools.combinations(desired_edits, r):

                sorted_combo = sorted(combo, key=lambda x: float(''.join(filter(str.isdigit, x)))) # Sort each combination numerically
                combo_str = ", ".join(sorted_combo) # Join as a string with ", "
                desired_edits_combos.append(combo_str)

        desired_edits.extend(desired_edits_combos)

    return df[df[edit_col].isin(desired_edits)].reset_index(drop=True)

def editing_per_library(edit_dc: dict | str, paired_regions_dc: dict | str, fastq_ids: pd.DataFrame | str, 
                        out_dir: str=None, count: str='count', psuedocount: int=1, return_df: bool=True) -> pd.DataFrame:
    '''
    editing_per_library(): Determine editing relative library abundance

    Parameters:
    edit_dc (dict | str): Dictionary of edit outcomes dataframes or path to directory containing them.
    paired_regions_dc (dict | str): Dictionary of paired regions dataframes or path to directory containing them.
    fastq_ids (pd.DataFrame | str): DataFrame containing fastq IDs for 'genotyping' and 'paired_regions'.
    out_dir (str, optional): Directory to save the output dataframes (Default = None; no save).
    count (str, optional): Column name in paired regions dataframe to use for counting epeg-ngRNAs (Default = 'count').
    psuedocount (int, optional): Pseudocount to add to counts for calculations (Default = 1).
    return_df (bool, optional): Whether to return the final dataframe (Default = True).
    '''
    # Get edit and paired regions dictionaries; fastq_ids dataframe if needed
    if isinstance(edit_dc, str):
        edit_dc = io.get_dir(edit_dc)
    if isinstance(paired_regions_dc, str):
        paired_regions_dc = io.get_dir(paired_regions_dc)
    if isinstance(fastq_ids, str):
        fastq_ids = io.get(fastq_ids)

    # Check fastq_ids dataframe has 'genotyping' and 'paired_regions' columns
    if not all(col in fastq_ids.columns for col in ['genotyping', 'paired_regions']):
        raise ValueError("fastq_ids DataFrame must contain 'genotyping' and 'paired_regions' columns.")

    # Get editing per library abundance
    out_dc = dict()
    for edit_fq,edit_df in edit_dc.items(): # Iterate through edit outcomes dataframes
        paired_regions_fq = fastq_ids[fastq_ids['genotyping']==edit_fq]['paired_regions'].values[0]

        # Check if paired regions file column has 'Edit' column
        if 'Edit' not in paired_regions_dc[paired_regions_fq].columns:
            if 'Edit' in paired_regions_dc[paired_regions_fq].columns: # Rename 'Edit' column to 'Edit' for consistency
                paired_regions_dc[paired_regions_fq].rename(columns={'Edit':'Edit'}, inplace=True) 
            else:
                raise(TypeError(f"Error: 'Edit' column not found in {paired_regions_fq}. Please check the file."))
            
        # Get counts of epeg-ngRNAs
        paired_regions_df = paired_regions_dc[paired_regions_fq][['Edit','ID','desired']].value_counts().reset_index() 
        paired_regions_df = paired_regions_df[paired_regions_df['desired']==True] # Discard chimeras

        # Add missing edits
        for edit in edit_df['Edit']:
            if edit not in paired_regions_df['Edit'].values:
                paired_regions_df = pd.concat([paired_regions_df,
                                            pd.DataFrame({'Edit':[edit], 'ID':[None], 'desired':[True], 'count':[0]})
                                            ]).reset_index(drop=True)
        
        # Calculate fraction of epeg-ngRNAs with psuedocount...
        paired_regions_df[f'{count}+{psuedocount}'] = paired_regions_df['count']+psuedocount
        total_count = paired_regions_df[count].sum()
        total_count_psuedocount = paired_regions_df[f'{count}+{psuedocount}'].sum()
        paired_regions_df['fraction'] = [count/total_count for count in paired_regions_df[count]]
        paired_regions_df[f'fraction+{psuedocount}'] = [count_psuedocount/total_count_psuedocount for count_psuedocount in paired_regions_df[f'{count}+{psuedocount}']]
        
        # ...to determine fraction of total epeg-ngRNAs for each edit and the editing per library
        library_count_psuedocount = []
        library_fraction_psuedocount = []
        for edit in edit_df['Edit']:
            library_count_psuedocount.append(paired_regions_df[paired_regions_df['Edit']==edit][f'count+{psuedocount}'].values[0])
            library_fraction_psuedocount.append(paired_regions_df[paired_regions_df['Edit']==edit][f'fraction+{psuedocount}'].values[0])
        edit_df[f'library_count+{psuedocount}'] = library_count_psuedocount
        edit_df[f'library_fraction+{psuedocount}'] = library_fraction_psuedocount
        edit_df['editing_per_library'] = edit_df['fraction'] / edit_df[f'library_fraction+{psuedocount}']
        
        # Save the edited dataframe to the output dictionary
        out_dc[edit_fq] = edit_df
        if out_dir is not None:
            io.save(dir=os.path.join(out_dir,'split'), file=f"{edit_fq}.csv", obj=edit_df)

    # Save the output dictionary as a single dataframe
    out_df = t.join(dc=out_dc, col='fastq_file')
    if out_dir is not None:
        io.save(dir=out_dir, file='editing_per_library.csv', obj=out_df)
    if return_df:
        return out_df

# UMI methods
def extract_umis(fastq_dir: str, out_dir: str='./extract_umis', 
                 bc_pattern: str='NNNNNNNNNNNNNNNN', env: str='umi_tools'):
    ''' 
    extract_umis(): extract UMIs using umi_tools

    Parameters:
    fastq_dir (str): directory with FASTQ files
    out_dir (str): output directory (Default: ./extract_umis)
    bc_pattern (str, optional): UMI barcode pattern (Default: NNNNNNNNNNNNNNNN)
    env (str, optional): conda environment with umi_tools installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .extract_umi subdirectory for logs
    mkdir(os.path.join(out_dir,'.extract_umi'))

    # Iterate through fastq files in the directory
    for file in os.listdir(path=fastq_dir):
        if file.endswith('.fastq') or file.endswith('.fastq.gz'):
            # Extract UMIs using umi_tools
            command = f'conda run -n {env} umi_tools extract --bc-pattern={bc_pattern} --stdin={os.path.join(fastq_dir,file)} --stdout={os.path.join(out_dir,file.replace(".gz",""))} --log={os.path.join(out_dir,".extract_umi",file)}.log'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'umi_tools extract: {file}'))

    # Memory reporting
    memories.append(memory_timer(task='extract_umis()'))
    io.save(dir=os.path.join(out_dir,'.extract_umis'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))     
    
def trim_motifs(fastq_dir: str, out_dir: str='./trim_motifs', 
                config_key: str = None, in_file: pd.DataFrame | str = None, motif5: str=None, motif3: str=None, 
                motif_length: int=21, error_rate: float=0.1,
                env: str='umi_tools'):
    ''' 
    trim_motifs(): trimming motifs with cutadapt

    Parameters:
    fastq_dir (str): directory with FASTQ files (with UMIs extracted)
    out_dir (str): output directory (Default: ./trim_motifs)

    config_key (str, option 1): config file key (FWD primer_REV primer) with 'motif5' & 'motif3'
    in_file (dataframe | str, option 2): Input file (.txt or .csv) with sequences for PrimeDesign. Format: target_name,target_sequence (column names required)
    motif5 (str, option 3): 5' motif pattern (Default: None)
    motif3 (str, option 3): 3' motif pattern (Default: None)

    motif_length (int, optional): trim 'in_file' motifs to this length (Default: 21)
    error_rate (float, optional): maximum error rate allowed in each motif (Default: 0.1)
    cutadapt (str, optional): Conda environment with cutadapt installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .trim_motifs subdirectory for logs
    mkdir(os.path.join(out_dir,'.trim_motifs'))
    mkdir(os.path.join(out_dir,'.trim5'))
    
    # Get motifs from in_file if needed
    if motif5 is None and motif3 is None:
        if in_file is not None: # Use in_file to get motifs
            if type(in_file)==str: # Get PrimeDesign input DataFrame from file path if needed
                in_file = io.get(pt=in_file)
            if 'target_sequence' not in in_file.columns:
                raise(ValueError(f"Error: 'target_sequence' column not found in {in_file}. Please check the file."))
            target_sequence = in_file.iloc[0]['target_sequence']
            motif5 = target_sequence.split('(')[0][-motif_length:]
            motif3 = target_sequence.split(')')[1][:motif_length]
        
        elif config_key is not None: # Use config file to get motifs
            config_key = config.get_info(id=config_key)
            if 'motif5' in config_key:
                motif5 = config_key['motif5']
            if 'motif3' in config_key:
                motif3 = config_key['motif3']
        
        else: # No motifs provided
            raise(ValueError("Error: Either 'in_file', 'config_key', or 'motif5' and/or 'motif3' must be provided."))
        
    # Iterate through fastq files in the directory
    for file in os.listdir(path=fastq_dir):
        if file.endswith('.fastq') or file.endswith('.fastq.gz'):
            # Trim motifs using cutadapt (only keep reads with both motifs (up to 10% error rate))
            command = f'conda run -n {env} cutadapt -g {motif5} -e {error_rate} --trimmed-only -o {os.path.join(out_dir,".trim5",file.replace(".gz",""))} {os.path.join(fastq_dir,file)} > {os.path.join(out_dir,".trim_motifs",file)}_trim5.log'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            command = f'conda run -n {env} cutadapt -a {motif3} -e {error_rate} --trimmed-only -o {os.path.join(out_dir,file.replace(".gz",""))} {os.path.join(out_dir,".trim5",file.replace(".gz",""))} > {os.path.join(out_dir,".trim_motifs",file)}_trim53.log'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)

            # Memory reporting
            memories.append(memory_timer(task=f'cutadapt: {file}'))

    # Memory reporting
    memories.append(memory_timer(task='trim_motifs()'))
    io.save(dir=os.path.join(out_dir,'.trim_motifs'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))  

def make_sams(fastq_dir: str, out_dir: str='./make_sams', 
              in_file: pd.DataFrame | str = None, fasta: str=None,
              sensitivity: Literal['very-fast','fast','sensitive','very-sensitive','very-fast-local','fast-local','sensitive-local','very-sensitive-local']='very-sensitive', 
              env: str='umi_tools'):
    ''' 
    make_sams(): generates alignments saved as a SAM files using bowtie2

    Parameters:
    fastq_dir (str): directory with FASTQ files (with UMIs extracted and motifs trimmed)
    out_dir (str): output directory (Default: ./make_sams)

    in_file (dataframe | str, option 1): Input file (.txt or .csv) with sequences for PrimeDesign. Format: target_name,target_sequence (column names required)
    fasta (str, option 2): reference fasta file (Default: None)
    
    sensitivity (str, optional): bowtie2 sensitivity preset (Default: very-sensitive). Options: very-fast, fast, sensitive, very-sensitive, very-fast-local, fast-local, sensitive-local, very-sensitive-local.
        For --end-to-end:
        --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50
        --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50
        --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)
        --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50

        For --local:
        --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00
        --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75
        --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)
        --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50
    env (str, optional): Conda environment with bowtie2 installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .make_sams subdirectory for logs
    mkdir(os.path.join(out_dir,'.make_sams'))

    # Get fasta from in_file if needed
    if fasta is None:
        if in_file is None:
            raise(ValueError("Error: Either 'in_file' or 'fasta' must be provided."))
        
        if type(in_file)==str: # Get PrimeDesign input DataFrame from file path if needed
            in_file = io.get(pt=in_file)
        if 'target_sequence' not in in_file.columns:
            raise(ValueError(f"Error: 'target_sequence' column not found in {in_file}. Please check the file."))
        elif 'target_name' not in in_file.columns:
            raise(ValueError(f"Error: 'target_name' column not found in {in_file}. Please check the file."))
        
        # Extract target name and sequence
        target_name = in_file.iloc[0]['target_name']
        target_sequence = in_file.iloc[0]['target_sequence']
        seq = Seq(target_sequence.split('(')[1].split(')')[0])
        
        # Save fasta file
        fasta = os.path.join(out_dir,'.make_sams',f'{target_name}.fasta')
        SeqIO.write(SeqRecord(seq, id=target_name, description=''), fasta, 'fasta')
        
    # Create bowtie2 index
    command = f'conda run -n {env} bowtie2-build {fasta} {".".join(fasta.split(".")[:-1])}'
    print(f"{command}")
    result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
    
    # Print output/errors
    if result.stdout: print(f"output:\n{result.stdout}")
    if result.stderr: print(f"errors:\n{result.stderr}")

    # Memory reporting
    memories.append(memory_timer(task=f'bowtie2-build: {fasta}'))

    # Iterate through fastq files in the directory
    for file in os.listdir(path=fastq_dir):
        if file.endswith('.fastq') or file.endswith('.fastq.gz'):
            # Generate SAM files using bowtie2
            command = f'conda run -n {env} bowtie2 -x {".".join(fasta.split(".")[:-1])} -U {os.path.join(fastq_dir,file)} -S {os.path.join(out_dir,file)}.sam --{sensitivity} > {os.path.join(out_dir,".make_sams",file)}.log'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'bowtie2: {file}'))

    # Memory reporting
    memories.append(memory_timer(task='make_sams()'))

    io.save(dir=os.path.join(out_dir,'.make_sams'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

def make_bams(sam_dir: str, out_dir: str='./make_bams', env: str='umi_tools'):
    '''
    make_bams(): converts SAM files to BAM files using samtools
    
    Parameters:
    sam_dir (str): directory with SAM files
    out_dir (str): output directory (Default: ./make_bams)
    env (str, optional): Conda environment with samtools installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .make_bams subdirectory for logs
    mkdir(os.path.join(out_dir,'.make_bams'))

    # Iterate through SAM files in the directory
    for file in os.listdir(path=sam_dir):
        if file.endswith('.sam'):
            # Convert SAM to sorted BAM using samtools
            command = f"conda run -n {env} bash -lc \
                        'set -euo pipefail; \
                        samtools view -b {os.path.join(sam_dir,file)} | samtools sort -o {os.path.join(out_dir,file.replace('.sam','.sorted.bam'))}' \
                        > {os.path.join(out_dir,'.make_bams',file)}.log 2>&1"
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'samtools view | samtools sort: {file}'))

            # Make BAM index using samtools
            command = f'conda run -n {env} samtools index {os.path.join(out_dir,file.replace(".sam",".sorted.bam"))} > {os.path.join(out_dir,".make_bams",file)}.index.log'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)

            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'samtools index: {file}'))

    # Memory reporting
    memories.append(memory_timer(task='make_bams()'))
    io.save(dir=os.path.join(out_dir,'.make_bams'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

def bam_umi_tags(bam_dir: str, out_dir: str='./bam_umi_tags',
                 env: str='umi_tools'):
    '''
    bam_umi_tags(): copy UMI in read ID to RX tag in BAM files using fgbio
    
    Parameters:
    bam_dir (str): directory with BAM files
    out_dir (str): output directory (Default: ./bam_umi_tags)
    env (str, optional): Conda environment with umi_tools installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .bam_umi_tags subdirectory for logs
    mkdir(os.path.join(out_dir,'.bam_umi_tags'))

    # Iterate through BAM files in the directory
    for file in os.listdir(path=bam_dir):
        if file.endswith('.bam'):
            # Group BAM files by UMI using fgbio
            command = f'conda run -n {env} fgbio CopyUmiFromReadName -i {os.path.join(bam_dir,file)} -o {os.path.join(out_dir,file.replace(".bam",".withRX.bam"))} --field-delimiter _'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'fgbio GroupReadsByUmi: {file}'))
    
    # Memory reporting
    memories.append(memory_timer(task='bam_umi_tags()'))
    io.save(dir=os.path.join(out_dir,'.bam_umi_tags'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

def group_umis(bam_dir: str, out_dir: str='./group_umis', 
               strategy: Literal['Identical','Edit','Adjacency','Paired']='Adjacency',
               edits: int=1,
               env: str='umi_tools'):
    '''
    group_umis(): group BAM files by UMI using fgbio
    
    Parameters:
    bam_dir (str): directory with BAM files
    out_dir (str): output directory (Default: ./group_umis)
    strategy (str, optional): UMI grouping strategy (Default: adjacency). Options: Identical, Edit, Adjacency, Paired
        1. Identity: only reads with identical UMI sequences are grouped together. This strategy may be useful for evaluating
           data, but should generally be avoided as it will generate multiple UMI groups per original molecule in the presence
           of errors.
        2. Edit: reads are clustered into groups such that each read within a group has at least one other read in the group
           with <= edits differences and there are inter-group pairings with <= edits differences. Effective when there are
           small numbers of reads per UMI, but breaks down at very high coverage of UMIs.
        3. Adjacency: a version of the directed adjacency method described in umi_tools (http://dx.doi.org/10.1101/051755)
           that allows for errors between UMIs but only when there is a count gradient.
        4. Paired: similar to adjacency but for methods that produce template such that a read with A-B is related to but not
           identical to a read with B-A. Expects the UMI sequences to be stored in a single SAM tag separated by a hyphen (e.g.
           'ACGT-CCGG') and allows for one of the two UMIs to be absent (e.g. 'ACGT-' or '-ACGT'). The molecular IDs produced
           have more structure than for single UMI strategies and are of the form '{base}/{A|B}'. E.g. two UMI pairs would be
           mapped as follows AAAA-GGGG -> 1/A, GGGG-AAAA -> 1/B.
    env (str, optional): Conda environment with fgbio installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .group_umis, family_hist, & grouping_metrics subdirectories for logs
    mkdir(os.path.join(out_dir,'.group_umis'))
    mkdir(os.path.join(out_dir,'family_hist'))
    mkdir(os.path.join(out_dir,'grouping_metrics'))

    # Iterate through BAM files in the directory
    for file in os.listdir(path=bam_dir):
        if file.endswith('.bam'):
            # Group BAM files by UMI using fgbio
            command = f'conda run -n {env} fgbio GroupReadsByUmi -i {os.path.join(bam_dir,file)} -o {os.path.join(out_dir,file.replace(".bam",".grouped.bam"))} -s {strategy} -e {edits} -f {os.path.join(out_dir,"family_hist",file.replace(".bam",".family_hist.txt"))} -g {os.path.join(out_dir,"grouping_metrics",file.replace(".bam",".grouping_metrics.txt"))}'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'fgbio GroupReadsByUmi: {file}'))
    
    # Memory reporting
    memories.append(memory_timer(task='group_umis()'))
    io.save(dir=os.path.join(out_dir,'.group_umis'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

def consensus_umis(bam_dir: str, out_dir: str='./consensus_umis', 
                   min_reads: int=1,
                   env: str='umi_tools'):
    '''
    consensus_umis(): generate consensus reads from grouped BAM files using fgbio
    
    Parameters:
    bam_dir (str): directory with grouped BAM files
    out_dir (str): output directory (Default: ./consensus_umis)
    min_reads (int, optional): minimum reads per UMI group to generate consensus (Default: 1)
    env (str, optional): Conda environment with fgbio installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .consensus_umis subdirectory for logs
    mkdir(os.path.join(out_dir,'.consensus_umis'))

    # Iterate through grouped BAM files in the directory
    for file in os.listdir(path=bam_dir):
        if file.endswith('.grouped.bam'):
            # Generate consensus reads from grouped BAM files using fgbio
            command = f'conda run -n {env} fgbio CallMolecularConsensusReads -i {os.path.join(bam_dir,file)} -o {os.path.join(out_dir,file.replace(".grouped.bam",".consensus.bam"))} -M {min_reads}'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'fgbio CallMolecularConsensusReads: {file}'))

    # Memory reporting
    memories.append(memory_timer(task='consensus_umis()'))
    io.save(dir=os.path.join(out_dir,'.consensus_umis'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

def bam_to_fastq(bam_dir: str, out_dir: str='./bam_to_fastq', env: str='umi_tools'):
    '''
    bam_to_fastq(): convert BAM files to FASTQ files using samtools

    Parameters:
    bam_dir (str): directory with BAM files
    out_dir (str): output directory (Default: ./bam_to_fastq)
    env (str, optional): Conda environment with samtools installed (Default: umi_tools)
    '''
    # Memory reporting
    memory_timer(reset=True)
    memories = []

    # Create output directory and .bam_to_fastq subdirectory for logs
    mkdir(os.path.join(out_dir,'.bam_to_fastq'))

    # Iterate through BAM files in the directory
    for file in os.listdir(path=bam_dir):
        if file.endswith('.bam'):
            # Convert BAM to FASTQ using samtools
            command = f'conda run -n {env} samtools fastq -n {os.path.join(bam_dir,file)} > {os.path.join(out_dir,file.replace(".bam",".fastq"))}'
            print(f"{command}")
            result = subprocess.run(f"{command}", shell=True, cwd='.', capture_output=True, text=True)
            
            # Print output/errors
            if result.stdout: print(f"output:\n{result.stdout}")
            if result.stderr: print(f"errors:\n{result.stderr}")

            # Memory reporting
            memories.append(memory_timer(task=f'samtools fastq: {file}'))
    
    # Memory reporting
    memories.append(memory_timer(task='bam_to_fastq()'))
    io.save(dir=os.path.join(out_dir,'.bam_to_fastq'),
            file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_memories.csv',
            obj=pd.DataFrame(memories, columns=['Task','Memory, MB','Time, s']))

# Supporting methods for plots
''' aa_props: dictionary of AA properties with citations (Generated by ChatGPT)
    -------------------------------------------------------------------------
    SOURCES FOR AMINO ACID BIOPHYSICAL PARAMETERS

    Hydrophobicity (KyteDoolittle):
    Kyte J, Doolittle RF. "A simple method for displaying the hydropathic
    character of a protein." J Mol Biol. 1982.

    Polarity (Zimmerman Polarity Index):
    Zimmerman JM, Eliezer N, Simha R. "The characterization of amino acids
    for protein chain folding and ion-binding." J Theor Biol. 1968.

    Van der Waals / Side-chain Volumes (Grantham volumes):
    Grantham R. "Amino acid difference formula to help explain protein 
    evolution." Science. 1974.

    pKa Values:
    From standard peptide pKa values:
        - T. E. Creighton, "Proteins: Structures and Molecular Properties"
        - EMBOSS pK matrices (amino acid side-chain pKa reference)
        - CRC Handbook of Chemistry and Physics

    pKa_C_term   = -carboxyl group
    pKa_N_term   = -amino group
    pKa_side_chain = only for ionizable residue side chains
    -------------------------------------------------------------------------
'''
aa_props = {
    # Acidic (large -> small)
    'E': {'name': 'Glutamic acid', 'hydrophobicity': -3.5, 'weight': 147.1,
          'polarity': 12.3, 'charge': 'negative', 'vdw_volume': 83,
          'pKa_C_term': 2.10, 'pKa_N_term': 9.47, 'pKa_side_chain': 4.07},

    'D': {'name': 'Aspartic acid', 'hydrophobicity': -3.5, 'weight': 133.1,
          'polarity': 13.0, 'charge': 'negative', 'vdw_volume': 54,
          'pKa_C_term': 1.99, 'pKa_N_term': 9.90, 'pKa_side_chain': 3.90},

    # Basic (large -> small)
    'K': {'name': 'Lysine', 'hydrophobicity': -3.9, 'weight': 146.2,
          'polarity': 11.3, 'charge': 'positive', 'vdw_volume': 119,
          'pKa_C_term': 2.16, 'pKa_N_term': 9.06, 'pKa_side_chain': 10.54},

    'R': {'name': 'Arginine', 'hydrophobicity': -4.5, 'weight': 174.2,
          'polarity': 10.5, 'charge': 'positive', 'vdw_volume': 124,
          'pKa_C_term': 1.82, 'pKa_N_term': 8.99, 'pKa_side_chain': 12.48},

    'H': {'name': 'Histidine', 'hydrophobicity': -3.2, 'weight': 155.2,
          'polarity': 10.4, 'charge': 'positive', 'vdw_volume': 96,
          'pKa_C_term': 1.80, 'pKa_N_term': 9.33, 'pKa_side_chain': 6.04},

    # Aromatic
    'W': {'name': 'Tryptophan', 'hydrophobicity': -0.9, 'weight': 204.2,
          'polarity': 5.4, 'charge': 'neutral', 'vdw_volume': 170,
          'pKa_C_term': 2.46, 'pKa_N_term': 9.41, 'pKa_side_chain': None},

    'F': {'name': 'Phenylalanine', 'hydrophobicity': 2.8, 'weight': 165.2,
          'polarity': 5.2, 'charge': 'neutral', 'vdw_volume': 132,
          'pKa_C_term': 2.20, 'pKa_N_term': 9.31, 'pKa_side_chain': None},

    # Aromatic & hydroxyl
    'Y': {'name': 'Tyrosine', 'hydrophobicity': -1.3, 'weight': 181.2,
          'polarity': 6.2, 'charge': 'neutral', 'vdw_volume': 136,
          'pKa_C_term': 2.20, 'pKa_N_term': 9.21, 'pKa_side_chain': 10.46},

    # Hydroxyl
    'S': {'name': 'Serine', 'hydrophobicity': -0.8, 'weight': 105.1,
          'polarity': 9.2, 'charge': 'neutral', 'vdw_volume': 32,
          'pKa_C_term': 2.19, 'pKa_N_term': 9.21, 'pKa_side_chain': None},

    'T': {'name': 'Threonine', 'hydrophobicity': -0.7, 'weight': 119.1,
          'polarity': 8.6, 'charge': 'neutral', 'vdw_volume': 61,
          'pKa_C_term': 2.09, 'pKa_N_term': 9.10, 'pKa_side_chain': None},

    # Amide
    'Q': {'name': 'Glutamine', 'hydrophobicity': -3.5, 'weight': 146.2,
          'polarity': 10.5, 'charge': 'neutral', 'vdw_volume': 85,
          'pKa_C_term': 2.17, 'pKa_N_term': 9.13, 'pKa_side_chain': None},

    'N': {'name': 'Asparagine', 'hydrophobicity': -3.5, 'weight': 132.1,
          'polarity': 11.6, 'charge': 'neutral', 'vdw_volume': 56,
          'pKa_C_term': 2.14, 'pKa_N_term': 8.72, 'pKa_side_chain': None},

    # Sulfur
    'C': {'name': 'Cysteine', 'hydrophobicity': 2.5, 'weight': 121.2,
          'polarity': 5.5, 'charge': 'neutral', 'vdw_volume': 55,
          'pKa_C_term': 1.92, 'pKa_N_term': 10.70, 'pKa_side_chain': 8.37},

    'M': {'name': 'Methionine', 'hydrophobicity': 1.9, 'weight': 149.2,
          'polarity': 5.7, 'charge': 'neutral', 'vdw_volume': 105,
          'pKa_C_term': 2.13, 'pKa_N_term': 9.28, 'pKa_side_chain': None},

    # Inflexible
    'P': {'name': 'Proline', 'hydrophobicity': -1.6, 'weight': 115.1,
          'polarity': 8.0, 'charge': 'neutral', 'vdw_volume': 32.5,
          'pKa_C_term': 1.95, 'pKa_N_term': 10.64, 'pKa_side_chain': None},

    # Branched
    'L': {'name': 'Leucine', 'hydrophobicity': 3.8, 'weight': 131.2,
          'polarity': 4.9, 'charge': 'neutral', 'vdw_volume': 111,
          'pKa_C_term': 2.33, 'pKa_N_term': 9.74, 'pKa_side_chain': None},

    'I': {'name': 'Isoleucine', 'hydrophobicity': 4.5, 'weight': 131.2,
          'polarity': 5.2, 'charge': 'neutral', 'vdw_volume': 111,
          'pKa_C_term': 2.32, 'pKa_N_term': 9.76, 'pKa_side_chain': None},

    'V': {'name': 'Valine', 'hydrophobicity': 4.2, 'weight': 117.1,
          'polarity': 5.9, 'charge': 'neutral', 'vdw_volume': 84,
          'pKa_C_term': 2.29, 'pKa_N_term': 9.74, 'pKa_side_chain': None},

    # Flexible
    'A': {'name': 'Alanine', 'hydrophobicity': 1.8, 'weight': 89.1,
          'polarity': 8.1, 'charge': 'neutral', 'vdw_volume': 31,
          'pKa_C_term': 2.35, 'pKa_N_term': 9.87, 'pKa_side_chain': None},

    'G': {'name': 'Glycine', 'hydrophobicity': -0.4, 'weight': 75.1,
          'polarity': 9.0, 'charge': 'neutral', 'vdw_volume': 3,
          'pKa_C_term': 2.35, 'pKa_N_term': 9.78, 'pKa_side_chain': None},

    # Stop codon
    '*': {'name': 'Stop', 'hydrophobicity': None, 'weight': None,
          'polarity': None, 'charge': None, 'vdw_volume': None,
          'pKa_C_term': None, 'pKa_N_term': None, 'pKa_side_chain': None},
}

def edit_change(df: pd.DataFrame, col: str='Edit', aa_properties: list=[]) -> pd.DataFrame:
    ''' 
    edit_change(): split edit column to before, after, and amino acid number; determine aa property changes
    
    Parameters:
    df (dataframe): fastq outcomes dataframe
    col (str, optional): edit column name
    aa_properties (list, optional): list of properties to include in labels (Default: None). Options: 'hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain'

    Dependencies: pandas
    '''
    df = df[(df[col].str.contains(',')==False) & # Isolate single AA changes
              (df[col]!='WT')&(df[col]!='Not WT')].reset_index(drop=True) # Remove WT & Not WT
    
    num_ls = [] # Extract edit information
    before_ls = []
    after_ls = []
    for edit in df[col].to_list():
        num = int(re.findall(r'\d+', edit)[0])
        before = edit[:edit.find(str(num))]
        after = edit[edit.find(str(num))+len(str(num)):]
        num_ls.append(num)
        before_ls.append(before)
        after_ls.append(after)
    
    df['Before']=before_ls # Save edit information
    df['After']=after_ls
    df['AA Number']=num_ls

    basic = ['R', 'K', 'H'] # Organize data by conservation
    acidic = ['D', 'E']
    polar = ['S', 'T', 'Y', 'N', 'Q', 'C']
    nonpolar = ['A', 'V', 'L', 'I', 'M', 'F', 'W', 'P', 'G']
    
    change = []
    if aa_properties is not None: 
        before_aa_prop_ls = []
        after_aa_prop_ls = []
    for (before,after) in t.zip_cols(df=df, cols=['Before','After']):

        if len(before)==1 & len(after)==1: # Substitution (changed to)
            if (before not in basic) & (after in basic): change.append('Basic')
            elif (before not in acidic) & (after in acidic): change.append('Acidic')
            elif (before not in polar) & (after in polar): change.append('Polar')
            elif (before not in nonpolar) & (after in nonpolar): change.append('Nonpolar')
            else: change.append('Conserved')
            if aa_properties is not None: 
                before_aa_prop_dc = dict()
                after_aa_prop_dc = dict()
                for prop in aa_properties:
                    if prop in aa_properties:
                        before_aa_prop_dc[prop] = aa_props[before][prop] if before in aa_props else before
                        after_aa_prop_dc[prop] = aa_props[after][prop] if after in aa_props else after
                    else:
                        print(f"Warning: '{prop}' not recognized. Skipping...")
                before_aa_prop_ls.append(before_aa_prop_dc)
                after_aa_prop_ls.append(after_aa_prop_dc)

        elif len(before)==1 & len(after)>1: # Insertion (changed to)
            if after[-1] in basic: change.append('Basic')
            elif after[-1] in acidic: change.append('Acidic')
            elif after[-1] in polar: change.append('Polar')
            elif after[-1] in nonpolar: change.append('Nonpolar')
            else: change.append('Conserved')
            if aa_properties is not None:
                before_aa_prop_dc = dict()
                after_aa_prop_dc = dict()
                for prop in aa_properties:
                    if prop in aa_properties:
                        before_aa_prop_dc[prop] = ""
                        after_aa_prop_dc[prop] = aa_props[after[-1]][prop] if after[-1] in aa_props else after[-1]
                    else:
                        print(f"Warning: '{prop}' not recognized. Skipping...")
                before_aa_prop_ls.append(before_aa_prop_dc)
                after_aa_prop_ls.append(after_aa_prop_dc)

        elif len(before)>1 & len(after)==1: # Deletion (removed... inverse)
            if before[0] in basic: change.append('Acidic')
            elif before[0] in acidic: change.append('Basic')
            elif before[0] in polar: change.append('Nonpolar')
            elif before[0] in nonpolar: change.append('Polar')
            else: change.append('Conserved')
            if aa_properties is not None: 
                before_aa_prop_dc = dict()
                after_aa_prop_dc = dict()
                for prop in aa_properties:
                    if prop in aa_properties:
                        before_aa_prop_dc[prop] = aa_props[before[0]][prop] if before[0] in aa_props else before[0]
                        after_aa_prop_dc[prop] = ""
                    else:
                        print(f"Warning: '{prop}' not recognized. Skipping...")
                before_aa_prop_ls.append(before_aa_prop_dc)
                after_aa_prop_ls.append(after_aa_prop_dc)
        
        else: # Complex (multiple changes)
            change.append('Complex')
            if aa_properties is not None: 
                before_aa_prop_ls.append({})
                after_aa_prop_ls.append({})

    df['Change'] = change
    if aa_properties is not None: 
        df['Before_AA_Properties'] = before_aa_prop_ls
        df['After_AA_Properties'] = after_aa_prop_ls
    return df

def make_label_info(label: str, before_aa_dict: dict, after_aa_dict: dict, 
                    patients: str=None, cancer_types: str=None, 
                    secondary_structure: str=None, uniprot_ptm: str=None, phosphositeplus: str=None,
                    protein_contacts: str=None, nucleic_contacts: str=None,
                    protein_neighbors: str=None, nucleic_neighbors: str=None,
                    font_size: int=16) -> str:
    '''
    make_label_info(): include additional info for labels including aa property changes and patient information
    
    Parameters:
    label (str): Label for the table
    before_aa_dict (dict): Dictionary of aa properties before the change
    after_aa_dict (dict): Dictionary of aa properties after the change
    patients (str, optional): Patient information to include in the label (Default: None)
    cancer_types (str, optional): Cancer type information to include in the label (Default: None)
    secondary_structure (str, optional): Secondary structure information to include in the label (Default: None)
    uniprot_ptm (str, optional): UniProt post-translational modification information to include in the label (Default: None)
    phosphositeplus (str, optional): PhosphoSitePlus post-translational modification information to include in the label (Default: None)
    protein_contacts (str, optional): PDB protein contact information to include in the label (Default: None)
    nucleic_contacts (str, optional): PDB nucleic acid contact information to include in the label (Default: None)
    protein_neighbors (str, optional): PDB protein neighbor information to include in the label (Default: None)
    nucleic_neighbors (str, optional): PDB nucleic acid neighbor information to include in the label (Default: None)
    font_size (int, optional): Font size for the label text (Default: 16)

    Dependencies: aa_props
    '''
    # Get union of AA property keys to be safe if dicts differ slightly
    keys = sorted(set(before_aa_dict.keys()) | set(after_aa_dict.keys()))
    
    # Create table rows for each AA property
    rows = []
    for k in keys:
        # Get before and after values, defaulting to "" if not present
        bv = before_aa_dict.get(k, "")
        av = after_aa_dict.get(k, "")

        # Format key names
        if k == 'hydrophobicity': k = 'Hydrophobicity (KyteDoolittle)'
        elif k == 'polarity': k = 'Polarity (Zimmerman Index)'
        elif k == 'charge': k = 'Charge'
        elif k == 'pKa_C_term': k = 'pKa (C-terminal)'
        elif k == 'pKa_N_term': k = 'pKa (N-terminal)'
        elif k == 'pKa_side_chain': k = 'pKa (Side Chain)'
        elif k == 'vdw_volume': k = 'Van der Waals Volume ()'

        # Add row to HTML table
        rows.append(
            f"<tr>"
            f"<th style='padding:0px 0px; text-align:left;'>{k}</th>"
            f"<td style='padding:0px 4px;'>{bv}</td>"
            f"<td style='padding:0px 4px;'>{av}</td>"
            f"</tr>"
        )
    rows_html = "\n".join(rows)
    
    # Create full HTML text with AA property changes (table) and optional patient/cancer info
    text = f"""
<div style="background-color: rgba(255,255,255,1); padding:4px; border-radius:4px;">
  <table style='font-size:{font_size}px; border-collapse:collapse;'>
    <thead>
      <tr>
        <th style='padding:0px 0px; text-align:left;'>Property</th>
        <th style='padding:0px 4px;'>Before</th>
        <th style='padding:0px 4px;'>After</th>
      </tr>
    </thead>
    <tbody>
      <br><span style='font-size:{font_size}px'><b><u>{label}</u></b></span>
"""
    if patients is not None and cancer_types is not None:
        text += f"""<br><span style='font-size:{font_size}px'><b>Patients:</b> {int(patients)}</span>"""
        num = cancer_types.count(',') + 1
        if num <= 1:
            text += f"""<br><span style='font-size:{font_size}px'><b>Cancer Types:</b> {cancer_types}</span>"""
        else:
            text += f"""<br><span style='font-size:{font_size}px'><b>Cancer Types:</b> {','.join(cancer_types.split(',')[0:2])},</span>"""
            text += f"""<br><span style='font-size:{font_size}px'>{',<br>'.join(t.split_nth(','.join(cancer_types.split(',')[2:]), ',',2))}</span>"""
    if secondary_structure is not None:
        text += f"""<br><span style='font-size:{font_size}px'><b>2 Structure:</b> {secondary_structure}</span>"""
    if uniprot_ptm is not None:
        text += f"""<br><span style='font-size:{font_size}px'><b>PTM (UniProt):</b> {uniprot_ptm}</span>"""
    if phosphositeplus is not None:
        text += f"""<br><span style='font-size:{font_size}px'><b>PTM (PhosphoSitePlus [Refs]):</b> {phosphositeplus}</span>"""
    if protein_contacts is not None:
        num = protein_contacts.count('),') + 1
        if num <= 0:
            text += f"""<br><span style='font-size:{font_size}px'><b>AA Contacts:</b> {protein_contacts}</span>"""
        else:
            text += f"""<br><span style='font-size:{font_size}px'><b>AA Contacts:</b> {protein_contacts.split('),')[0]}),</span>"""
            text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(protein_contacts.split('),')[1:]), '),',2))}</span>"""
    if nucleic_contacts is not None:
        num = nucleic_contacts.count('),') + 1
        if ['dA','dC','dG','dT'] in nucleic_contacts:
            if num <= 0:
                text += f"""<br><span style='font-size:{font_size}px'><b>DNA Contacts:</b> {nucleic_contacts}</span>"""
            else:
                text += f"""<br><span style='font-size:{font_size}px'><b>DNA Contact:</b> {nucleic_contacts.split('),')[0]}),</span>"""
                text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(nucleic_contacts.split('),')[1:]), '),',2))}</span>"""
        else:
            if num <= 0:
                text += f"""<br><span style='font-size:{font_size}px'><b>RNA Contacts:</b> {nucleic_contacts}</span>"""
            else:
                text += f"""<br><span style='font-size:{font_size}px'><b>RNA Contacts:</b> {nucleic_contacts.split('),')[0]}),</span>"""
                text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(nucleic_contacts.split('),')[1:]), '),',2))}</span>"""
    if protein_neighbors is not None:
        num = protein_neighbors.count('),') + 1
        if num <= 0:
            text += f"""<br><span style='font-size:{font_size}px'><b>AA Neighbors (CoM):</b> {protein_neighbors}</span>"""
        else:
            text += f"""<br><span style='font-size:{font_size}px'><b>AA Neighbors (CoM):</b> {protein_neighbors.split('),')[0]}),</span>"""
            text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(protein_neighbors.split('),')[1:]), '),',3))}</span>"""
    if nucleic_neighbors is not None:
        num = nucleic_neighbors.count('),') + 1
        if ['dA','dC','dG','dT'] in nucleic_neighbors:
            if num <= 0:
                text += f"""<br><span style='font-size:{font_size}px'><b>DNA Neighbors (CoM):</b> {nucleic_neighbors}</span>"""
            else:
                text += f"""<br><span style='font-size:{font_size}px'><b>DNA Neighbors (CoM):</b> {nucleic_neighbors.split('),')[0]}),</span>"""
                text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(nucleic_neighbors.split('),')[1:]), '),',3))}</span>"""
        else:
            if num <= 0:
                text += f"""<br><span style='font-size:{font_size}px'><b>RNA Neighbors (CoM):</b> {nucleic_neighbors}</span>"""
            else:
                text += f"""<br><span style='font-size:{font_size}px'><b>RNA Neighbors (CoM):</b> {nucleic_neighbors.split('),')[0]}),</span>"""
                text += f"""<br><span style='font-size:{font_size}px'>{'),<br>'.join(t.split_nth('),'.join(nucleic_neighbors.split('),')[1:]), '),',3))}</span>"""

    text += f"""
      {rows_html}
    </tbody>
  </table>
</div>
"""
    return text

def add_label_info(df: pd.DataFrame, label: str='Edit', label_size: int=16, label_info: bool=True, 
                   aa_properties: bool | list=True, cBioPortal: str=None, UniProt: str=None, 
                   PhosphoSitePlus: str=None, PDB_contacts: str=None, PDB_neighbors: str=None) -> pd.DataFrame:
    ''' 
    add_label_info(): AA properties for conservation (change to); plus additional info from cBioPortal, UniProt, PhosphoSitePlus, PDB if specified
    
    Parameters:
    df (dataframe): plot dataframe
    label (str, optional): label column name (Default: 'Edit'). Can't be None.
    label_size (int, optional): label font size (Default: 16)
    label_info (bool, optional): include additional info for labels if .html plot (Default: True)
    aa_properties (bool |list, optional): use aa_properties to format labels (Default: True). Options: True | False; ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain']
    cBioPortal (str, optional): gene name (if saved to ~/.config/edms/cBioPortal_mutations) or file path for cBioPortal mutation data processed through edms.dat.cBioPortal.mutations()
    UniProt (str, optional): UniProt accession (if saved to ~/.config/edms/UniProt) or file path for UniProt flat file. See edms.dat.uniprot.retrieve() or edms uniprot retrieve -h for more information.
    PhosphoSitePlus (str, optional): UniProt accession
    PDB_contacts (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    PDB_neighbors (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    
    Dependencies: make_label_info(), pandas
    '''
    
    # Determine AA properties to include
    if aa_properties == True:
        aa_properties = ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_side_chain']  # All except terminal pKa
    elif aa_properties == False:  # No AA properties
        aa_properties = None
    else:
        if any([prop not in ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain'] for prop in aa_properties]):  # Validate list entries
            raise ValueError("aa_properties list can only contain: 'hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain'")
    df = edit_change(df=df, col=label, aa_properties=aa_properties)

    # Assign cBioPortal mutation & indications
    if cBioPortal is not None:
        # Load cBioPortal mutation data
        try: # from file path
            cBioPortal_df = io.get(pt=cBioPortal)
        except:
            try: # from config
                for cBioPortal_file in os.listdir(os.path.expanduser('~/.config/edms/cBioPortal_mutations/')):
                    if cBioPortal.lower() in cBioPortal_file.lower():
                        cBioPortal_df = io.get(pt=os.path.expanduser(f"~/.config/edms/cBioPortal_mutations/{cBioPortal_file}"))
                        break
            except:
                raise FileNotFoundError(f"cBioPortal mutation data file not found: {cBioPortal}.\nPlease provide a valid gene name (if saved to {os.path.expanduser('~/.config/edms/cBioPortal_mutations/')} or file path for cBioPortal mutation data processed through edms.dat.cBioPortal.mutations()")

        # Merge cBioPortal mutation data with volcano plot data
        cBioPortal_df.rename(columns={'counts':'Patients'}, inplace=True)
        df = pd.merge(df, cBioPortal_df[['Edit Change','Patients','Cancer Types']], how='left', left_on=label, right_on='Edit Change')
        df.fillna({'Patients':0, 'Cancer Types':'None'}, inplace=True)

    # Assign UniProt secondary structure & PTMs
    if UniProt is not None:
        # Load UniProt flat file data
        try: # from filepath
            UniProt_ss = uniprot.secondary_structure_from_flat_file(obj=UniProt)
            UniProt_ptms = uniprot.ptms_from_flat_file(obj=UniProt)
        except:
            try: # from config
                for UniProt_file in os.listdir(os.path.expanduser('~/.config/edms/UniProt/')):
                    if UniProt.lower() in UniProt_file.lower():
                        UniProt_ss = uniprot.secondary_structure_from_flat_file(obj=f'{os.path.expanduser("~/.config/edms/UniProt")}/{UniProt_file}')
                        UniProt_ptms = uniprot.ptms_from_flat_file(obj=f'{os.path.expanduser("~/.config/edms/UniProt")}/{UniProt_file}')
                        break
            except:
                raise FileNotFoundError(f"UniProt flat file not found: {UniProt}.\nPlease provide a valid filename or UniProt accession (if saved to {os.path.expanduser('~/.config/edms/UniProt/')}) or file path for UniProt flat file. See edms.dat.uniprot.retrieve_flat_file() or edms uniprot retrieve -h for more information.")

        # Merge UniProt secondary structure & ptm data with volcano plot data
        secondary_structure_ls = []
        ptms_ls = []
        for aa_num in df['AA Number']:

            # Find secondary structure
            found_ss = False
            for start,end,name in t.zip_cols(df=UniProt_ss, cols=['start','end','name']):
                if aa_num>=start and aa_num<=end:
                    secondary_structure_ls.append(name)
                    found_ss = True
                    break
            if not found_ss:
                secondary_structure_ls.append(None)
            
            # Find PTMs
            found_ptm = 0
            ptm = f""
            for start,end,desc in t.zip_cols(df=UniProt_ptms, cols=['start','end','normalized_description']):
                if aa_num>=start and aa_num<=end:
                    ptm += f"{desc}; "
                    found_ptm += 1
                    break
            ptms_ls.append(ptm[:-2] if found_ptm>0 else None)
        
        df['2 structure'] = secondary_structure_ls
        df['UniProt PTM'] = ptms_ls
    
    # Assign PhosphoSitePlus PTMs
    if PhosphoSitePlus is not None:
        # Load PhosphoSitePlus file & filter for specified UniProt accession
        PhosphoSitePlus_ptms = load_resource_csv(filename='PhosphoSitePlus.csv')
        PhosphoSitePlus_ptms = PhosphoSitePlus_ptms[PhosphoSitePlus_ptms['ACC_ID']==PhosphoSitePlus].reset_index(drop=True)
        if PhosphoSitePlus_ptms.empty:
            print(f"Warning: No PhosphoSitePlus data found. Check UniProt accession: {PhosphoSitePlus}")
        
        # Merge PhosphoSitePlus ptm data with volcano plot data
        ptms_ls = []
        for aa_num in df['AA Number']:
            
            # Find PTMs
            found_ptm = 0
            ptm = f""
            for start,end,desc,ref in t.zip_cols(df=PhosphoSitePlus_ptms, cols=['start','end','normalized_description', 'references']):
                if aa_num==start or aa_num==end:
                    ptm += f"{desc} [{ref}]; "
                    found_ptm += 1
                    break
            ptms_ls.append(ptm[:-2] if found_ptm>0 else None)
        
        df['PhosphoSitePlus PTM'] = ptms_ls
    
    # Assign PDB structure contacts
    if PDB_contacts is not None:
        # Load PDB structure data
        try: # from filepath
            protein_contacts = pdb.compute_residue_contacts(PDB_contacts, "protein", "protein")
            nucleic_contacts = pdb.compute_residue_contacts(PDB_contacts, "nucleic", "nucleic", exclude_backbone_backbone=False)
        except:
            try: # from config
                for PDB_contacts_file in os.listdir(os.path.expanduser('~/.config/edms/PDB/')):
                    if PDB_contacts.lower() in PDB_contacts_file.lower():
                        protein_contacts = pdb.compute_residue_contacts(f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_contacts_file}', "protein", "protein")
                        nucleic_contacts = pdb.compute_residue_contacts(f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_contacts_file}', "nucleic", "nucleic", exclude_backbone_backbone=False)
                        break
            except:
                raise FileNotFoundError(f"PDB file not found: {PDB_contacts_file}.\nPlease provide a valid filename or PDB id (if saved to {os.path.expanduser('~/.config/edms/PDB/')}) or file path for PDB structure file")
        
        # Merge PDB structure data with volcano plot data
        protein_contacts_ls = []
        nucleic_contacts_ls = []
        for aa_num in df['AA Number']:

            # Find protein neighbors
            protein_contacts_aa_num = protein_contacts[protein_contacts['query_resid']==aa_num]
            protein_contacts_aa_num_text = ', '.join([f'({protein_contacts_aa_num['query_atom'].values[i]}{protein_contacts_aa_num['partner_atom'].values[i]}, {SeqUtils.seq1(protein_contacts_aa_num['partner_resname'].values[i])}{protein_contacts_aa_num['partner_resid'].values[i]}, {protein_contacts_aa_num['min_atom_distance'].values[i]:.1f} )' for i in range(len(protein_contacts_aa_num))])
            if protein_contacts_aa_num_text=='':
                protein_contacts_ls.append(None)
            else:
                protein_contacts_ls.append(protein_contacts_aa_num_text)
            
            # Find nucleic neighbors
            nucleic_contacts_aa_num = nucleic_contacts[nucleic_contacts['query_resid']==aa_num]
            nucleic_contacts_aa_num_text = ', '.join([f'({nucleic_contacts_aa_num['query_atom'].values[i]}{nucleic_contacts_aa_num['partner_atom'].values[i]}, {nucleic_contacts_aa_num['partner_resname'].values[i].replace('D','d')}{nucleic_contacts_aa_num['partner_resid'].values[i]}, {nucleic_contacts_aa_num['min_atom_distance'].values[i]:.1f} )' for i in range(len(nucleic_contacts_aa_num))])
            if nucleic_contacts_aa_num_text=='':
                nucleic_contacts_ls.append(None)
            else:
                nucleic_contacts_ls.append(nucleic_contacts_aa_num_text)

        df['AA Contacts'] = protein_contacts_ls
        df['DNA Contacts'] = nucleic_contacts_ls

    # Assign PDB structure neighbors
    if PDB_neighbors is not None:
        # Load PDB structure data
        try: # from filepath
            protein_neighbors = pdb.compute_residue_neighbors(PDB_neighbors, "protein", "protein")
            nucleic_neighbors = pdb.compute_residue_neighbors(PDB_neighbors, "nucleic", "nucleic")
        except:
            try: # from config
                for PDB_neighbors_file in os.listdir(os.path.expanduser('~/.config/edms/PDB/')):
                    if PDB_neighbors.lower() in PDB_neighbors_file.lower():
                        protein_neighbors = pdb.compute_residue_neighbors(f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_neighbors_file}', "protein", "protein")
                        nucleic_neighbors = pdb.compute_residue_neighbors(f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_neighbors_file}', "nucleic", "nucleic")
                        break
            except:
                raise FileNotFoundError(f"PDB file not found: {PDB_neighbors_file}.\nPlease provide a valid filename or PDB id (if saved to {os.path.expanduser('~/.config/edms/PDB/')}) or file path for PDB structure file")

        # Merge PDB structure data with volcano plot data
        protein_neighbors_ls = []
        nucleic_neighbors_ls = []
        for aa_num in df['AA Number']:

            # Find protein neighbors
            protein_neighbors_aa_num = protein_neighbors[protein_neighbors['query_resid']==aa_num]
            protein_neighbors_aa_num_text = ', '.join([f'{SeqUtils.seq1(protein_neighbors_aa_num['partner_resname'].values[i])}{protein_neighbors_aa_num['partner_resid'].values[i]} ({protein_neighbors_aa_num['distance'].values[i]:.1f} )' for i in range(len(protein_neighbors_aa_num))])
            if protein_neighbors_aa_num_text=='':
                protein_neighbors_ls.append(None)
            else:
                protein_neighbors_ls.append(protein_neighbors_aa_num_text)
            
            # Find nucleic neighbors
            nucleic_neighbors_aa_num = nucleic_neighbors[nucleic_neighbors['query_resid']==aa_num]
            nucleic_neighbors_aa_num_text = ', '.join([f'{nucleic_neighbors_aa_num['partner_resname'].values[i].replace('D','d')}{nucleic_neighbors_aa_num['partner_resid'].values[i]} ({nucleic_neighbors_aa_num['distance'].values[i]:.1f} )' for i in range(len(nucleic_neighbors_aa_num))])
            if nucleic_neighbors_aa_num_text=='':
                nucleic_neighbors_ls.append(None)
            else:
                nucleic_neighbors_ls.append(nucleic_neighbors_aa_num_text)

        df['AA Neighbors'] = protein_neighbors_ls
        df['DNA Neighbors'] = nucleic_neighbors_ls

    # Make label_info
    if label_info == True:

        # Temporarily make None columns that don't exist
        if cBioPortal is None:
            df['Patients'] = [None]*len(df)
            df['Cancer Types'] = [None]*len(df)
        if UniProt is None:
            df['2 structure'] = [None]*len(df)
            df['UniProt PTM'] = [None]*len(df)
        if PhosphoSitePlus is None:
            df['PhosphoSitePlus PTM'] = [None]*len(df)
        if PDB_contacts is None:
            df['AA Contacts'] = [None]*len(df)
            df['DNA Contacts'] = [None]*len(df)
        if PDB_neighbors is None:
            df['AA Neighbors'] = [None]*len(df)
            df['DNA Neighbors'] = [None]*len(df)
        
        # Make label info based on available data
        df[f'{label}_info'] = [make_label_info(label, before, after, patients, cancers, structure, uniprot_ptm, phosphositeplus_ptm, protein_contacts, nucleic_contacts,protein_neighbors, nucleic_neighbors, font_size=label_size) 
                                for label, before, after, patients, cancers, structure, uniprot_ptm, phosphositeplus_ptm, protein_contacts, nucleic_contacts, protein_neighbors, nucleic_neighbors in 
                                t.zip_cols(df=df, cols=[label,'Before_AA_Properties','After_AA_Properties','Patients','Cancer Types','2 structure','UniProt PTM','PhosphoSitePlus PTM','AA Contacts','DNA Contacts','AA Neighbors','DNA Neighbors'])]
        
        # Remove temporary None columns
        if cBioPortal is None:
            df.drop(columns=['Patients','Cancer Types'], inplace=True)
        if UniProt is None:
            df.drop(columns=['2 structure','UniProt PTM'], inplace=True)
        if PhosphoSitePlus is None:
            df.drop(columns=['PhosphoSitePlus PTM'], inplace=True)
        if PDB_contacts is None:
            df.drop(columns=['AA Contacts','DNA Contacts'], inplace=True)
        if PDB_neighbors is None:
            df.drop(columns=['AA Neighbors','DNA Neighbors'], inplace=True)

    return df

# Plot methods
def cat(typ: str, df: pd.DataFrame | str, x: str='', y: str='', cats_ord: list = None, cats_exclude: list|str = None, cols: str=None, cols_ord: list=None, cols_exclude: list|str=None, PDB_pt: str=None,
        file: str=None, dir: str=None, palette_or_cmap: str='colorblind', edgecol: str='black', lw: int=1, errorbar: str = 'sd', errwid: int = 1, errcap: float = 0.1,
        figsize: tuple = (5, 5), title: str='', title_size: int=18, title_weight: str='bold', title_font: str='Arial',
        x_axis: str='', x_axis_size=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_scale: str='linear', x_axis_dims: tuple=(0,0), x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=0, x_ticks_font: str='Arial', x_ticks: list=[],
        y_axis: str='', y_axis_size=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_scale: str='linear', y_axis_dims: tuple=(0,0), y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=0, y_ticks_font: str='Arial', y_ticks: list=[],
        legend_title: str='', legend_title_size: int=12, legend_size: int=9, legend_bbox_to_anchor=(1,1), legend_loc: str='upper left', legend_items: tuple=(0,0), legend_ncol: int=1,
        legend_columnspacing: int=0, legend_handletextpad: float=0.5, legend_labelspacing: float=0.5, legend_borderpad: float=0.5, legend_handlelength: float=1, legend_size_html_multiplier: float=1.0,
        show: bool=True, space_capitalize: bool=True,
        **kwargs):
    ''' 
    cat: creates categorical graphs
    
    Parameters:
    typ (str): plot type (bar, box, violin, swarm, strip, point, count, bar_strip, box_strip, violin_strip, bar_swarm, box_swarm, violin_swarm)
    df (dataframe | str): pandas dataframe (or file path)
    x (str, optional): x-axis column name
    y (str, optional): y-axis column name
    cats_ord (list, optional): category column values order (x- or y-axis)
    cats_exclude (list | str, optional): category column values exclude (x- or y-axis)
    cols (str, optional): color column name
    cols_ord (list, optional): color column values order
    cols_exclude (list | str, optional): color column values exclude
    PDB_pt (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    palette_or_cmap (str, optional): seaborn color palette or matplotlib color map
    edgecol (str, optional): point edge color
    lw (int, optional): line width
    errorbar (str, optional): error bar type (sd)
    errwid (int, optional): error bar line width
    errcap (int, optional): error bar cap line width
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_scale (str, optional): x-axis scale linear, log, etc.
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_scale (str, optional): y-axis scale linear, log, etc.
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y_ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_items (tuple, optional): legend items to show (start, end)
    legend_ncol (tuple, optional): # of columns
    legends_items (tuple, optional): legend items to show (start, end)
    legend_columnspacing (int, optional): space between columns (Default: 0; only for html plots)
    legend_handletextpad (float, optional): space between marker and text (Default: 0.5; only for html plots)
    legend_labelspacing (float, optional): vertical space between entries (Default: 0.5; only for html plots)
    legend_borderpad (float, optional): padding inside legend box (Default: 0.5; only for html plots)
    legend_handlelength (float, optional): marker length (Default: 1; only for html plots)
    legend_size_html_multiplier (float, optional): legend size multiplier for html plots (Default: 1.0)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: os, matplotlib, seaborn, & plot
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)

    # Sort data by genotype position
    if cats_ord is None:
        cats = ''
        if x!='': # Check that x column is numeric
            if df[x].apply(lambda row: isinstance(row, (int, float))).all()==True: 
                cats = y
        if y!='' and cats=='': # Check that y column is numeric
            if df[y].apply(lambda row: isinstance(row, (int, float))).all()==True: 
                cats = x
        if cats=='': pass # No numeric column found
        genotypes = list(df[cats].value_counts().keys())
        positions = list()
        for geno in genotypes:
            numbers = re.findall(r'\d+\.?\d*', geno)
            if 'Indel' == geno: positions.append(1000001) # Places Indel near the end
            elif 'Not WT' == geno: positions.append(1000002) # Places Not WT near the end
            elif 'WT' == geno: positions.append(1000003) # Places WT at the end
            else: positions.append(sum([int(n) for n in numbers])/len(numbers))
        assign = pd.DataFrame({'positions':positions,
                               'genotypes':genotypes})
        cols_ord = list(assign.sort_values(by='positions')['genotypes'])

    # PDB label information
    if PDB_pt is not None:
        if len(PDB_pt)==4: # If PDB ID given
            for PDB_file in os.listdir(os.path.expanduser('~/.config/edms/PDB/')):
                if PDB_pt.lower() in PDB_file.lower():
                    PDB_pt = f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_file}'
                    break

    p.cat(typ=typ,df=df,x=x,y=y,cats_ord=cats_ord,cats_exclude=cats_exclude,cols=cols,cols_ord=cols_ord,cols_exclude=cols_exclude,PDB_pt=PDB_pt,
          file=file,dir=dir,palette_or_cmap=palette_or_cmap,edgecol=edgecol,lw=lw,errorbar=errorbar,errwid=errwid,errcap=errcap,
          figsize=figsize,title=title,title_size=title_size,title_weight=title_weight,title_font=title_font,
          x_axis=x_axis,x_axis_size=x_axis_size,x_axis_weight=x_axis_weight,x_axis_font=x_axis_font,x_axis_scale=x_axis_scale,x_axis_dims=x_axis_dims,x_axis_pad=x_axis_pad,x_ticks_size=x_ticks_size,x_ticks_rot=x_ticks_rot,x_ticks_font=x_ticks_font,x_ticks=x_ticks,
          y_axis=y_axis,y_axis_size=y_axis_size,y_axis_weight=y_axis_weight,y_axis_font=y_axis_font,y_axis_scale=y_axis_scale,y_axis_dims=y_axis_dims,y_axis_pad=y_axis_pad,y_ticks_size=y_ticks_size,y_ticks_rot=y_ticks_rot,y_ticks_font=y_ticks_font,y_ticks=y_ticks,
          legend_title=legend_title,legend_title_size=legend_title_size,legend_size=legend_size,legend_bbox_to_anchor=legend_bbox_to_anchor,legend_loc=legend_loc,legend_items=legend_items,legend_ncol=legend_ncol,
          legend_columnspacing=legend_columnspacing,legend_handletextpad=legend_handletextpad,legend_labelspacing=legend_labelspacing,legend_borderpad=legend_borderpad,legend_handlelength=legend_handlelength,legend_size_html_multiplier=legend_size_html_multiplier,
          show=show,space_capitalize=space_capitalize,**kwargs)

def stack(df: pd.DataFrame | str, x: str='fastq_file', y: str='fraction', cols: str='Edit', cutoff_group: str='fastq_file', cutoff_value: float=0, cutoff_keep: bool=True, 
          cols_ord: list=[], x_ord: list=[], PDB_pt: str=None,
          file: str=None, dir: str=None, palette_or_cmap: str='tab20', repeats: int=1, errcap: int=4, vertical: bool=True,
          figsize: tuple = (5, 5), title: str='Editing Outcomes', title_size: int=18, title_weight: str='bold', title_font: str='Arial',
          x_axis: str='', x_axis_size: int=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=0, x_ticks_font: str='Arial',
          y_axis: str='', y_axis_size: int=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_dims: tuple=(0,0), y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=0, y_ticks_font: str='Arial',
          legend_title: str='', legend_title_size: int=12, legend_size: int=12,legend_bbox_to_anchor: tuple=(1,1), legend_loc: str='upper left', legend_ncol: int=1, 
          legend_columnspacing: int=0, legend_handletextpad: float=0.5, legend_labelspacing: float=0.5, legend_borderpad: float=0.5, legend_handlelength: float=1, legend_size_html_multiplier: float=1.0,
          show: bool=True, space_capitalize: bool=True, **kwargs):
    ''' 
    stack(): creates stacked bar plot

    Parameters:
    df (dataframe | str): pandas dataframe (or file path)
    x (str, optional): x-axis column name (Default: 'fastq_file')
    y (str, optional): y-axis column name (Default: 'fraction')
    cols (str, optional): color column name (Default: 'Edit')
    cutoff_group (str, optional): column name to group by when applying cutoff (Default: 'fastq_file')
    cutoff_value (float, optional): y-axis values needs be greater than (Default: 0)
    cutoff_keep (bool, optional): keep cutoff group even if below cutoff (Default: True)
    cols_ord (list, optional): color column values order
    x_ord (list, optional): x-axis column values order
    PDB_pt (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    palette_or_cmap (str, optional): seaborn palette or matplotlib color map
    repeats (int, optional): number of color palette or map repeats (Default: 1)
    errcap (int, optional): error bar cap line width
    vertical (bool, optional): vertical orientation; otherwise horizontal (Default: True)
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-ticks font
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    legend_columnspacing (int, optional): space between columns (Default: 0; only for html plots)
    legend_handletextpad (float, optional): space between marker and text (Default: 0.5; only for html plots)
    legend_labelspacing (float, optional): vertical space between entries (Default: 0.5; only for html plots)
    legend_borderpad (float, optional): padding inside legend box (Default: 0.5; only for html plots)
    legend_handlelength (float, optional): marker length (Default: 1; only for html plots)
    legend_size_html_multiplier (float, optional): legend size multiplier for html plots (Default: 1.0)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: re, os, pandas, numpy, matplotlib.pyplot & plot
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)

    # Omit smaller than cutoff and convert it to <cutoff
    if cutoff_group in df.columns and cutoff_value>0: # If cutoff group and value specified
        df_cut=df[df[y]>=cutoff_value]
        if cutoff_keep==True: # Keep cutoff group even if below cutoff
            df_other=df[df[y]<cutoff_value]
            for group in list(df_other[cutoff_group].value_counts().keys()):
                df_temp = df_other[df_other[cutoff_group]==group]
                df_temp[y]=sum(df_temp[y])
                df_temp[cols]=f'<{cutoff_value}'
                df_cut = pd.concat([df_cut,df_temp.iloc[:1]])
    else: # Otherwise use full dataframe
        df_cut=df

    # Sort pivot table columns by genotype position
    if cols_ord==[]:
        genotypes = list(df_cut[cols].value_counts().keys())
        positions = list()
        for geno in genotypes:
            numbers = re.findall(r'\d+\.?\d*', geno)
            if geno==f'<{cutoff_value}':positions.append(1000000) # Places <cutoff near the end
            elif 'Indel' == geno: positions.append(1000001) # Places Indel near the end
            elif 'Not WT' == geno: positions.append(1000002) # Places Not WT near the end
            elif 'WT' == geno: positions.append(1000003) # Places WT at the end
            else: positions.append(sum([int(n) for n in numbers])/len(numbers))
        assign = pd.DataFrame({'positions':positions,
                               'genotypes':genotypes})
        cols_ord = list(assign.sort_values(by='positions')['genotypes'])
    
    # PDB label information
    if PDB_pt is not None:
        if len(PDB_pt)==4: # If PDB ID given
            for PDB_file in os.listdir(os.path.expanduser('~/.config/edms/PDB/')):
                if PDB_pt.lower() in PDB_file.lower():
                    PDB_pt = f'{os.path.expanduser("~/.config/edms/PDB")}/{PDB_file}'
                    break

    # Make stacked barplot
    p.stack(df=df_cut,x=x,y=y,cols=cols,cutoff_group=cutoff_group,cutoff_value=0,cutoff_keep=cutoff_keep,cols_ord=cols_ord,x_ord=x_ord,
            file=file,dir=dir,palette_or_cmap=palette_or_cmap,repeats=repeats,errcap=errcap,vertical=vertical,
            figsize=figsize,title=title,title_size=title_size,title_weight=title_weight,title_font=title_font,
            x_axis=x_axis,x_axis_size=x_axis_size,x_axis_weight=x_axis_weight,x_axis_font=x_axis_font,x_axis_pad=x_axis_pad,x_ticks_size=x_ticks_size,x_ticks_rot=x_ticks_rot,x_ticks_font=x_ticks_font,
            y_axis=y_axis,y_axis_size=y_axis_size,y_axis_weight=y_axis_weight,y_axis_font=y_axis_font,y_axis_dims=y_axis_dims,y_axis_pad=y_axis_pad,y_ticks_size=y_ticks_size,y_ticks_rot=y_ticks_rot,y_ticks_font=y_ticks_font,
            legend_title=legend_title,legend_title_size=legend_title_size,legend_size=legend_size,legend_bbox_to_anchor=legend_bbox_to_anchor,legend_loc=legend_loc,legend_ncol=legend_ncol,
            legend_columnspacing=legend_columnspacing, legend_handletextpad=legend_handletextpad, legend_labelspacing=legend_labelspacing, legend_borderpad=legend_borderpad, legend_handlelength=legend_handlelength,legend_size_html_multiplier=legend_size_html_multiplier,
            show=show,space_capitalize=space_capitalize,PDB_pt=PDB_pt,**kwargs)

def vol(df: pd.DataFrame | str, FC: str, pval: str, size: str=None, size_dims: tuple=None, label: str='Edit', label_size: int=16,
        label_info: bool=True, aa_properties: bool | list=True, cBioPortal: str=None, UniProt: str=None, PhosphoSitePlus: str=None, PDB_contacts: str=None, PDB_neighbors: str=None,
        FC_threshold: float=1, pval_threshold: float=1, file: str=None, dir: str=None, color: str='lightgray', alpha: float=0.5, edgecol: str='black', vertical: bool=True,
        figsize: tuple = (5, 5), title: str='', title_size: int=18, title_weight: str='bold', title_font: str='Arial',
        x_axis: str='', x_axis_size: int=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_dims: tuple=(0,0), x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=0, x_ticks_font: str='Arial', x_ticks: list=[],
        y_axis: str='', y_axis_size: int=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_dims: tuple=(0,0), y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=0, y_ticks_font: str='Arial', y_ticks: list=[],
        legend_title: str='',legend_title_size: int=12, legend_size: int=9, legend_bbox_to_anchor: tuple=(1,1), legend_loc: str='upper left', legend_ncol: int=1,
        legend_columnspacing: int=-4, legend_handletextpad: float=0.5, legend_labelspacing: float=0.5, legend_borderpad: float=0.5, legend_handlelength: float=0.5, legend_size_html_multiplier: float=0.75, 
        display_legend: bool=True, display_labels: bool=True, display_lines: bool=False, display_axis: bool=True, return_df: bool=True, dpi: int = 0, show: bool=True, space_capitalize: bool=True,
        **kwargs) -> pd.DataFrame:
    ''' 
    vol(): creates volcano plot
    
    Parameters:
    df (dataframe | str): pandas dataframe (or file path) from st.compare()
    FC (str): fold change column name (x-axis)
    pval (str): p-value column name (y-axis)
    cols (str, optional): color column name
    size (str | bool, optional): size column name (Default: pval; specify False for no size)
    size_dims (tuple, optional): (minimum,maximum) values in size column (Default: None)
    label (str, optional): label column name (Default: 'Edit'). Can't be None.
    label_size (int, optional): label font size (Default: 16)
    label_info (bool, optional): include additional info for labels if .html plot (Default: True)
    aa_properties (bool |list, optional): use aa_properties to format labels (Default: True). Options: True | False; ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain']
    cBioPortal (str, optional): gene name (if saved to ~/.config/edms/cBioPortal_mutations) or file path for cBioPortal mutation data processed through edms.dat.cBioPortal.mutations()
    UniProt (str, optional): UniProt accession (if saved to ~/.config/edms/UniProt) or file path for UniProt flat file. See edms.dat.uniprot.retrieve() or edms uniprot retrieve -h for more information.
    PhosphoSitePlus (str, optional): UniProt accession
    PDB_contacts (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    PDB_neighbors (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    FC_threshold (float, optional): fold change threshold (Default: 1, meaning no threshold)
    pval_threshold (float, optional): p-value threshold (Default: 1, meaning no threshold)
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    color (str, optional): matplotlib color for nonsignificant values
    alpha (float, optional): transparency for nonsignificant values (Default: 0.5)
    edgecol (str, optional): point edge color
    vertical (bool, optional): vertical orientation; otherwise horizontal (Default: True)
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-axis ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-axis ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    legend_columnspacing (int, optional): space between columns (Default: -4; only for html plots)
    legend_handletextpad (float, optional): space between marker and text (Default: 0.5; only for html plots)
    legend_labelspacing (float, optional): vertical space between entries (Default: 0.5; only for html plots)
    legend_borderpad (float, optional): padding inside legend box (Default: 0.5; only for html plots)
    legend_handlelength (float, optional): marker length (Default: 0.5; only for html plots)
    legend_size_html_multiplier (float, optional): legend size multiplier for html plots (Default: 0.75)
    display_legend (bool, optional): display legend on plot (Default: True)
    display_labels (bool, optional): display labels for significant values (Default: True)
    display_lines (bool, optional): display threshold lines (Default: False)
    display_axis (bool, optional): display x- and y-axis lines (Default: True)
    dpi (int, optional): figure dpi (Default: 600 for non-HTML, 150 for HTML)
    return_df (bool, optional): return dataframe (Default: True)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: os, matplotlib, seaborn, pandas, & edit_change()
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Organize data by conservation (changed to)
    stys_order = ['Conserved','Basic','Acidic','Polar','Nonpolar','Complex']
    mark_order = ['D','^','v','<','>','o']

    # Add label info: AA properties for conservation (change to); plus additional info from cBioPortal, UniProt, PhosphoSitePlus, PDB if specified
    df = add_label_info(df=df, label=label, label_size=label_size, label_info=label_info,
                        aa_properties=aa_properties, cBioPortal=cBioPortal, UniProt=UniProt, 
                        PhosphoSitePlus=PhosphoSitePlus, PDB_contacts=PDB_contacts, PDB_neighbors=PDB_neighbors)

    PDB_pt = None if PDB_contacts is None else PDB_contacts
    PDB_pt = PDB_pt if PDB_pt is not None else PDB_neighbors

    if file is not None:
        if file.endswith('.html') == True:
            if label_info == True: label = f'{label}_info'
            if legend_ncol == 1: legend_ncol = 3

    # Volcano plot
    p.vol(df=df, FC=FC, pval=pval, size=size, stys='Change', size_dims=size_dims, label=label, stys_order=stys_order, mark_order=mark_order,
          FC_threshold=FC_threshold, pval_threshold=pval_threshold, file=file, dir=dir, color=color, alpha=alpha, edgecol=edgecol, vertical=vertical,
          figsize=figsize, title=title, title_size=title_size, title_weight=title_weight, title_font=title_font, 
          x_axis=x_axis, x_axis_size=x_axis_size, x_axis_weight=x_axis_weight, x_axis_font=x_axis_font, x_axis_dims=x_axis_dims, x_axis_pad=x_axis_pad, x_ticks_size = x_ticks_size, x_ticks_rot=x_ticks_rot, x_ticks_font=x_ticks_font, x_ticks=x_ticks,
          y_axis=y_axis, y_axis_size=y_axis_size, y_axis_weight=y_axis_weight, y_axis_font=y_axis_font, y_axis_dims=y_axis_dims, y_axis_pad=y_axis_pad, y_ticks_size=y_ticks_size, y_ticks_rot=y_ticks_rot, y_ticks_font=y_ticks_font, y_ticks=y_ticks,
          legend_title=legend_title, legend_title_size=legend_title_size, legend_size=legend_size, legend_bbox_to_anchor=legend_bbox_to_anchor, legend_loc=legend_loc, legend_ncol=legend_ncol, 
          legend_columnspacing=legend_columnspacing, legend_handletextpad=legend_handletextpad, legend_labelspacing=legend_labelspacing, legend_borderpad=legend_borderpad, legend_handlelength=legend_handlelength, legend_size_html_multiplier=legend_size_html_multiplier,
          display_legend=display_legend, display_labels=display_labels, display_lines=display_lines, display_axis=display_axis, return_df=return_df, dpi=dpi, show=show, space_capitalize=space_capitalize,
          PDB_pt=PDB_pt,
          **kwargs)

def torn(df: pd.DataFrame | str, FC: str, pval: str, size: str | bool=None, size_dims: tuple=None, label: str='Edit', label_size: int=16,
        label_info: bool=True, aa_properties: bool | list=True, cBioPortal: str=None, UniProt: str=None, PhosphoSitePlus: str=None, PDB_contacts: str=None, PDB_neighbors: str=None, ss_h: int=None, ss_y: int=None,
        file: str=None, dir: str=None, edgecol: str='black', figsize=(5,5), title: str='', title_size: int=18, title_weight: str='bold', title_font: str='Arial',
        x_axis: str='', x_axis_size: int=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_dims: tuple=(0,0), x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=0, x_ticks_font: str='Arial', x_ticks: list=[],
        y_axis: str='', y_axis_size: int=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_dims: tuple=(0,0), y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=0, y_ticks_font: str='Arial', y_ticks: list=[],
        legend_title: str='',legend_title_size: int=12, legend_size: int=9, legend_bbox_to_anchor: tuple=(1,1), legend_loc: str='upper left', legend_ncol: int=1, 
        legend_columnspacing: int=-3, legend_handletextpad: float=0.5, legend_labelspacing: float=0.5, legend_borderpad: float=0.5, legend_handlelength: float=0.5, legend_size_html_multiplier: float=0.75,
        display_legend: bool=True, display_labels: bool=True, display_axis: bool=True, return_df: bool=True, dpi: int = 0, show: bool=True, space_capitalize: bool=True,
        **kwargs) -> pd.DataFrame:
    ''' 
    torn(): creates tornado plot
    
    Parameters:
    df (dataframe | str): pandas dataframe (or file path) from st.compare()
    FC (str): fold change column name (y-axis)
    pval (str): p-value column name (size column if not specified)
    size (str | bool, optional): size column name (Default: pval; specify False for no size)
    size_dims (tuple, optional): (minimum,maximum) values in size column (Default: None)
    label (str, optional): label column name (Default: 'Edit'). Can't be None.
    label_size (int, optional): label font size (Default: 16)
    label_info (bool, optional): include additional info for labels if .html plot (Default: True)
    aa_properties (bool |list, optional): use aa_properties to format labels (Default: True). Options: True | False; ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain']
    cBioPortal (str, optional): gene name (if saved to ~/.config/edms/cBioPortal_mutations) or file path for cBioPortal mutation data processed through edms.dat.cBioPortal.mutations()
    UniProt (str, optional): UniProt accession (if saved to ~/.config/edms/UniProt) or file path for UniProt flat file. See edms.dat.uniprot.retrieve() or edms uniprot retrieve -h for more information.
    PhosphoSitePlus (str, optional): UniProt accession
    PDB_contacts (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    PDB_neighbors (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    ss_h (int, optional): height for secondary structure in the plot (Default: autogenerate)
    ss_y (int, optional): y position for secondary structure in the plot (Default: autogenerate)
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    edgecol (str, optional): point edge color
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-axis ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-axis ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    legend_columnspacing (int, optional): space between columns (Default: -3; only for html plots)
    legend_handletextpad (float, optional): space between marker and text (Default: 0.5; only for html plots)
    legend_labelspacing (float, optional): vertical space between entries (Default: 0.5; only for html plots)
    legend_borderpad (float, optional): padding inside legend box (Default: 0.5; only for html plots)
    legend_handlelength (float, optional): marker length (Default: 0.5; only for html plots)
    legend_size_html_multiplier (float, optional): legend size multiplier for html plots (Default: 0.75)
    display_legend (bool, optional): display legend on plot (Default: True)
    display_labels (bool, optional): display labels for significant values (Default: True)
    display_axis (bool, optional): display x-axis line (Default: True)
    dpi (int, optional): figure dpi (Default: 600 for non-HTML, 150 for HTML)
    return_df (bool, optional): return dataframe (Default: True)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: os, matplotlib, seaborn, pandas, & edit_change()
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Organize data by conservation (changed to)
    stys_order = ['Conserved','Basic','Acidic','Polar','Nonpolar','Complex']
    mark_order = ['D','^','v','<','>','o']

    # Add label info: AA properties for conservation (change to); plus additional info from cBioPortal, UniProt, PhosphoSitePlus, PDB if specified
    df = add_label_info(df=df, label=label, label_size=label_size, label_info=label_info,
                        aa_properties=aa_properties, cBioPortal=cBioPortal, UniProt=UniProt, 
                        PhosphoSitePlus=PhosphoSitePlus, PDB_contacts=PDB_contacts, PDB_neighbors=PDB_neighbors)
    
    PDB_pt = None if PDB_contacts is None else PDB_contacts
    PDB_pt = PDB_pt if PDB_pt is not None else PDB_neighbors

    # Determine if we are saving to HTML (for interactive behavior)
    if file is not None:
        is_html = file.endswith('.html')

        if is_html == True:
            # Match title fontsize for html plots
            x_axis_size=title_size
            y_axis_size=title_size
            x_ticks_size=title_size
            y_ticks_size=title_size
            legend_title_size=title_size
            legend_size=title_size*legend_size_html_multiplier

            # Detailed labels for html plots
            if label_info == True:
                label = f'{label}_info'

    else:
        is_html = False

    # Log transform data
    df[f'log2({FC})'] = [np.log10(FC_val)/np.log10(2) for FC_val in df[FC]]
    df[f'-log10({pval})'] = [-np.log10(pval_val) for pval_val in df[pval]]
    
    # Organize data by 'pval' or specified 'size' column, typically input abundance
    sizes=(1,100)
    if size in [False,'False','false']: # No size
        size = None 

    else:
        if size is None: size = f'-log10({pval})' # default to pval

        if size is not None and size in df.columns:
            # Filter by size dimensions
            if size_dims is not None: 
                df = df[(df[size]>=size_dims[0])&(df[size]<=size_dims[1])]

            # Shared size normalization across all scatter calls so marker areas are consistent
            size_norm = None
            _vmin, _vmax = None, None
            if display_legend:
                _vmin = df[size].min()
                _vmax = df[size].max()
                # Guard against degenerate case where all values are equal
                if _vmin == _vmax:
                    _vmax = _vmin + 1e-12
                size_norm = mcolors.Normalize(vmin=_vmin, vmax=_vmax)
    
    # Set dimensions
    if x_axis_dims==(0,0): x_axis_dims=(min(df[f'AA Number']),max(df[f'AA Number']))
    if y_axis_dims==(0,0): y_axis_dims=(min(df[f'log2({FC})']),max(df[f'log2({FC})']))

    # Generate figure
    fig, ax = plt.subplots(figsize=figsize)

    # with data
    if display_legend==False: size=None
    stys='Change'
    sns.scatterplot(
        data=df[df[f'log2({FC})']<0],
        x='AA Number', y=f'log2({FC})',
        hue=f'log2({FC})', edgecolor=edgecol, palette='Blues_r', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm,
        legend=False,
        ax=ax, **kwargs)
    sns.scatterplot(
        data=df[df[f'log2({FC})']>=0],
        x='AA Number', y=f'log2({FC})',
        hue=f'log2({FC})', edgecolor=edgecol, palette='Reds', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm, 
        legend=False,
        ax=ax, **kwargs)
    
    # with x-axis line
    if display_axis == True:
        ax.plot([x_axis_dims[0], x_axis_dims[1]], [0,0], color='black', linestyle='-', linewidth=1)

    # with secondary structure
    if UniProt is not None:
        # Load UniProt flat file data
        try: # from filepath
            UniProt_ss = uniprot.secondary_structure_from_flat_file(obj=UniProt)
        except:
            try: # from config
                for UniProt_file in os.listdir(os.path.expanduser('~/.config/edms/UniProt/')):
                    if UniProt.lower() in UniProt_file.lower():
                        UniProt_ss = uniprot.secondary_structure_from_flat_file(obj=f'{os.path.expanduser("~/.config/edms/UniProt")}/{UniProt_file}')
                        break
            except:
                raise FileNotFoundError(f"UniProt flat file not found: {UniProt}.\nPlease provide a valid filename or UniProt accession (if saved to {os.path.expanduser('~/.config/edms/UniProt/')}) or file path for UniProt flat file. See edms.dat.uniprot.retrieve_flat_file() or edms uniprot retrieve -h for more information.")
        
        # parameters for the secondary-structure "track"
        if ss_h is None:
            ss_h  = 0.5 # height of secondary structure track
        if ss_y is None:
            ss_y  = min(df[f'log2({FC})'])-2*0.5 # position below min y value

        # helices
        helix_df = UniProt_ss[UniProt_ss["type"] == "-helix"]
        helix_spans = [(row.start, row.end - row.start + 1) for _, row in helix_df.iterrows()]
        ax.broken_barh(helix_spans, (ss_y, ss_h), label="-helix", facecolors='coral')

        # -strands
        strand_df = UniProt_ss[UniProt_ss["type"] == "-strand"]
        strand_spans = [(row.start, row.end - row.start + 1) for _, row in strand_df.iterrows()]
        ax.broken_barh(strand_spans, (ss_y, ss_h), label="-strand", facecolors='gold')

    # with legend
    if display_legend == True:
        if size_norm is not None and size is not None: # Add consistent size legend with 5 representative values
            if stys is not None and mark_order is not None and stys_order is not None: # Add stys legend too
                legend_vals = np.linspace(_vmin, _vmax, len(mark_order))
                for lv,mark,sty in zip(legend_vals,mark_order,stys_order):
                    plt.scatter([], [], s=np.interp(lv, [_vmin, _vmax], sizes), color='lightgray', label=f'{lv:.2g}; {sty}', marker=mark)
                if is_html:
                    if legend_bbox_to_anchor == (1,1): legend_bbox_to_anchor = (-0.1,-0.2)
                    if legend_ncol == 1: legend_ncol = 3
                    plt.legend(title=legend_title if legend_title!='' else f'{size}; {stys}', 
                                title_fontsize=legend_title_size, fontsize=legend_size,
                                bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol,
                                columnspacing=legend_columnspacing,    # space between columns
                                handletextpad=legend_handletextpad,    # space between marker and text
                                labelspacing=legend_labelspacing,      # vertical space between entries
                                borderpad=legend_borderpad,            # padding inside legend box
                                handlelength=legend_handlelength)      # marker length
                else:
                    plt.legend(title=legend_title if legend_title!='' else f'{size}; {stys}', 
                            title_fontsize=legend_title_size, fontsize=legend_size,
                            bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol)
            else:
                legend_vals = np.linspace(_vmin, _vmax, 5)
                for lv in legend_vals:
                    plt.scatter([], [], s=np.interp(lv, [_vmin, _vmax], sizes), color='lightgray', label=f'{lv:.2g}')
                plt.legend(title=legend_title if legend_title!='' else size, 
                            title_fontsize=legend_title_size, fontsize=legend_size,
                            bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol)
    
    # with labels
    if display_labels == True:
        if is_html:
            # For HTML, show labels interactively as tooltips instead of static text
            pts = ax.scatter(
                x=df['AA Number'],
                y=df[f'log2({FC})'],
                s=20,
                alpha=0
            )
            labels_list = df[label].fillna("").astype(str).tolist()
            tooltip = p.SafeHTMLTooltip(pts, labels_list)
            clicker = p.ClickTooltip(pts, labels_list)
            mpld3.plugins.connect(fig, tooltip, clicker)
        else:
            # For static images, keep labels as always-visible text
            for i, l in enumerate(df[label]):
                plt.text(
                    x=df.iloc[i]['AA Number'],
                    y=df.iloc[i][f'log2({FC})'],
                    s=l
                )
    
    # Set x axis
    if x_axis=='': x_axis='AA Number'
    plt.xlabel(x_axis, fontsize=x_axis_size, fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
    if x_ticks==[]: 
        if x_ticks_rot==0: plt.xticks(rotation=x_ticks_rot,ha='center',va='top',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        elif x_ticks_rot == 90: plt.xticks(rotation=x_ticks_rot,ha='right',va='center',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        else: plt.xticks(rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font,fontsize=x_ticks_size)
    else: 
        if x_ticks_rot==0: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot, ha='center',va='top',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        elif x_ticks_rot == 90: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,ha='right',va='center',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        else: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font,fontsize=x_ticks_size)
    
    # Set y axis
    if y_axis=='': y_axis=f'log2({FC})'
    plt.ylabel(y_axis, fontsize=y_axis_size, fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad)

    if y_ticks==[]: plt.yticks(rotation=y_ticks_rot,fontfamily=y_ticks_font,fontsize=y_ticks_size)
    else: plt.yticks(ticks=y_ticks,labels=y_ticks,rotation=y_ticks_rot,fontfamily=y_ticks_font,fontsize=y_ticks_size)

    # Set title
    if title=='' and file is not None: 
        if space_capitalize: title=p.re_un_cap(".".join(file.split(".")[:-1]))
        else: ".".join(file.split(".")[:-1])
    plt.title(title, fontsize=title_size, fontweight=title_weight, family=title_font)

    # Save & show fig; return dataframe
    p.save_fig(file=file, dir=dir, fig=fig, dpi=dpi, PDB_pt=PDB_pt, icon='tornado')
    if show:
        ext = file.split('.')[-1].lower() if file is not None else ''
        if ext not in ('html', 'json'):
            plt.show()
        else:
            mpld3.show(fig)
    if return_df:
        return df

def corr(df: pd.DataFrame | str, cond_col: str, cond_vals: list, FC: str, pval: str, size: str | bool=None, size_dims: tuple=None, 
        conservative: bool=True, method: str='pearson', weighted: bool=True, label: str='Edit', label_size: int=16,
        label_info: bool=True, aa_properties: bool | list=True, cBioPortal: str=None, UniProt: str=None, PhosphoSitePlus: str=None, PDB_contacts: str=None, PDB_neighbors: str=None,
        file: str=None, dir: str=None, edgecol: str='black', figsize=(5,5), title: str='', title_size: int=18, title_weight: str='bold', title_font: str='Arial',
        x_axis: str='', x_axis_size: int=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_dims: tuple=(0,0), x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=0, x_ticks_font: str='Arial', x_ticks: list=[],
        y_axis: str='', y_axis_size: int=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_dims: tuple=(0,0), y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=0, y_ticks_font: str='Arial', y_ticks: list=[],
        legend_title: str='',legend_title_size: int=12, legend_size: int=9, legend_bbox_to_anchor: tuple=(1,1), legend_loc: str='upper left', legend_ncol: int=1,
        legend_columnspacing: int=-4, legend_handletextpad: float=0.5, legend_labelspacing: float=0.5, legend_borderpad: float=0.5, legend_handlelength: float=0.5, legend_size_html_multiplier: float=0.75,
        display_legend: bool=True, display_labels: bool=True, display_axis: bool=True, return_df: bool=True, dpi: int = 0, show: bool=True, space_capitalize: bool=True,
        **kwargs) -> pd.DataFrame:
    ''' 
    corr(): creates correlation plot
    
    Parameters:
    df (dataframe | str): pandas dataframe (or file path) from st.compare()
    cond_col (str): condition column name for comparison
    cond_vals (list): two condition values for comparison (x and y-axis)
    FC (str): fold change column name (x and y-axis)
    pval (str): p-value column name (size column if not specified)
    size (str | bool, optional): size column name (Default: pval; specify False for no size)
    size_dims (tuple, optional): (minimum,maximum) values in size column (Default: None)
    conservative (bool, optional): use conservative approach for p-value and size column (minimum value, default: True); alternatively, use combined evidence approach (sum values)
    method (str, optional): correlation method (Default: 'pearson'). Options: 'pearson', 'spearman', 'kendall'
    weighted (bool, optional): weighted correlation by size column (Default: True)
    label (str, optional): label column name (Default: 'Edit'). Can't be None.
    label_size (int, optional): label font size (Default: 16)
    label_info (bool, optional): include additional info for labels if .html plot (Default: True)
    aa_properties (bool |list, optional): use aa_properties to format labels (Default: True). Options: True | False; ['hydrophobicity', 'polarity', 'charge', 'vdw_volume', 'pKa_C_term', 'pKa_N_term', 'pKa_side_chain']
    cBioPortal (str, optional): gene name (if saved to ~/.config/edms/cBioPortal_mutations) or file path for cBioPortal mutation data processed through edms.dat.cBioPortal.mutations()
    UniProt (str, optional): UniProt accession (if saved to ~/.config/edms/UniProt) or file path for UniProt flat file. See edms.dat.uniprot.retrieve() or edms uniprot retrieve -h for more information.
    PhosphoSitePlus (str, optional): UniProt accession
    PDB_contacts (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    PDB_neighbors (str, optional): PDB ID (if saved to ~/.config/edms/PDB) or file path for PDB structure file. See edms.dat.pdb.retrieve() or edms uniprot retrieve -h for more information.
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    edgecol (str, optional): point edge color
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-axis ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-axis ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    legend_columnspacing (int, optional): space between columns (Default: -4; only for html plots)
    legend_handletextpad (float, optional): space between marker and text (Default: 0.5; only for html plots)
    legend_labelspacing (float, optional): vertical space between entries (Default: 0.5; only for html plots)
    legend_borderpad (float, optional): padding inside legend box (Default: 0.5; only for html plots)
    legend_handlelength (float, optional): marker length (Default: 0.5; only for html plots)
    legend_size_html_multiplier (float, optional): legend size multiplier for html plots (Default: 0.75)
    display_legend (bool, optional): display legend on plot (Default: True)
    display_labels (bool, optional): display labels for significant values (Default: True)
    display_axis (bool, optional): display x- and y-axis lines (Default: True)
    dpi (int, optional): figure dpi (Default: 600 for non-HTML, 150 for HTML)
    return_df (bool, optional): return dataframe (Default: True)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: os, matplotlib, seaborn, pandas, & edit_change()
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Organize data by conservation (changed to)
    stys_order = ['Conserved','Basic','Acidic','Polar','Nonpolar','Complex']
    mark_order = ['D','^','v','<','>','o']

    # Log transform data
    df[f'log2({FC})'] = [np.log10(FC_val)/np.log10(2) for FC_val in df[FC]]
    df[f'-log10({pval})'] = [-np.log10(pval_val) for pval_val in df[pval]]

    # Split dataframe into two conditions for correlation plot
    df1 = df[df[cond_col]==cond_vals[0]].copy()
    df2 = df[df[cond_col]==cond_vals[1]].copy()

    if size in df.columns: # include size column
        df = pd.merge(df1[[f'log2({FC})',f'-log10({pval})',size,label,cond_col]], 
                    df2[[f'log2({FC})',f'-log10({pval})',size,label,cond_col]], 
                    on=label, how='inner',
                    suffixes=(f'_{cond_vals[0]}', f'_{cond_vals[1]}'))
        del df1, df2
        df[f'sum({size})'] = df[f'{size}_{cond_vals[0]}'] + df[f'{size}_{cond_vals[1]}']
        df[f'min({size})'] = df[[f'{size}_{cond_vals[0]}', f'{size}_{cond_vals[1]}']].min(axis=1)

    else: # no size column
        df = pd.merge(df1[[f'log2({FC})',f'-log10({pval})',label,cond_col]], 
                    df2[[f'log2({FC})',f'-log10({pval})',label,cond_col]], 
                    on=label, how='inner',
                    suffixes=(f'_{cond_vals[0]}', f'_{cond_vals[1]}'))
        del df1, df2
    df[f'sum(-log10({pval}))'] = df[f'-log10({pval})_{cond_vals[0]}'] + df[f'-log10({pval})_{cond_vals[1]}']
    df[f'min(-log10({pval}))'] = df[[f'-log10({pval})_{cond_vals[0]}', f'-log10({pval})_{cond_vals[1]}']].min(axis=1)
    df[f'magnitude(log2({FC}))'] = [np.sqrt(log2fc_0**2 + log2fc_1**2) for log2fc_0, log2fc_1 in t.zip_cols(df=df, cols=[f'log2({FC})_{cond_vals[0]}',f'log2({FC})_{cond_vals[1]}'])]

    # Use min or sum based on conservative
    if conservative==True:
        min_or_sum = 'min'
    else:
        min_or_sum = 'sum'

    # Add label info: AA properties for conservation (change to); plus additional info from cBioPortal, UniProt, PhosphoSitePlus, PDB if specified
    df = add_label_info(df=df, label=label, label_size=label_size, label_info=label_info,
                        aa_properties=aa_properties, cBioPortal=cBioPortal, UniProt=UniProt, 
                        PhosphoSitePlus=PhosphoSitePlus, PDB_contacts=PDB_contacts, PDB_neighbors=PDB_neighbors)
    
    PDB_pt = None if PDB_contacts is None else PDB_contacts
    PDB_pt = PDB_pt if PDB_pt is not None else PDB_neighbors

    # Determine if we are saving to HTML (for interactive behavior)
    if file is not None:
        is_html = file.endswith('.html')

        if is_html == True:
            # Match title fontsize for html plots
            x_axis_size=title_size
            y_axis_size=title_size
            x_ticks_size=title_size
            y_ticks_size=title_size
            legend_title_size=title_size
            legend_size=title_size*legend_size_html_multiplier

            # Detailed labels for html plots
            if label_info == True:
                label = f'{label}_info'

    else:
        is_html = False

    # Organize data by 'pval' or specified 'size' column, typically input abundance
    sizes=(1,100)
    if size in [False,'False','false']: # No size
        size = None

    else:
        if size is None: # default to pval
            size = f'{min_or_sum}(-log10({pval}))'

        if f'{size}_{cond_vals[0]}' in df.columns and f'{size}_{cond_vals[1]}' in df.columns:
            size = f'{min_or_sum}({size})'

            # Filter by size dimensions
            if size_dims is not None: 
                df = df[(df[size]>=size_dims[0])&(df[size]<=size_dims[1])]

            # Shared size normalization across all scatter calls so marker areas are consistent
            size_norm = None
            _vmin, _vmax = None, None
            if display_legend:
                _vmin = df[size].min()
                _vmax = df[size].max()
                # Guard against degenerate case where all values are equal
                if _vmin == _vmax:
                    _vmax = _vmin + 1e-12
                size_norm = mcolors.Normalize(vmin=_vmin, vmax=_vmax)

    # Set dimensions
    if x_axis_dims==(0,0): x_axis_dims=(min(df[f'log2({FC})_{cond_vals[0]}']),max(df[f'log2({FC})_{cond_vals[0]}']))
    if y_axis_dims==(0,0): y_axis_dims=(min(df[f'log2({FC})_{cond_vals[1]}']),max(df[f'log2({FC})_{cond_vals[1]}']))

    # Generate figure
    fig, ax = plt.subplots(figsize=figsize)

    # with data
    if display_legend==False: size=None
    stys='Change'
    sns.scatterplot( # bottom left
        data=df[(df[f'log2({FC})_{cond_vals[0]}']<=0) & (df[f'log2({FC})_{cond_vals[1]}']<=0)],
        x=f'log2({FC})_{cond_vals[0]}', y=f'log2({FC})_{cond_vals[1]}',
        hue=f'magnitude(log2({FC}))', edgecolor=edgecol, palette='Blues', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm,
        legend=False,
        ax=ax, **kwargs)
    sns.scatterplot( # top left
        data=df[(df[f'log2({FC})_{cond_vals[0]}']<=0) & (df[f'log2({FC})_{cond_vals[1]}']>0)],
        x=f'log2({FC})_{cond_vals[0]}', y=f'log2({FC})_{cond_vals[1]}',
        hue=f'magnitude(log2({FC}))', edgecolor=edgecol, palette='Greens', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm,
        legend=False,
        ax=ax, **kwargs)
    sns.scatterplot( # bottom right
        data=df[(df[f'log2({FC})_{cond_vals[0]}']>0) & (df[f'log2({FC})_{cond_vals[1]}']<=0)],
        x=f'log2({FC})_{cond_vals[0]}', y=f'log2({FC})_{cond_vals[1]}',
        hue=f'magnitude(log2({FC}))', edgecolor=edgecol, palette='Oranges', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm, 
        legend=False,
        ax=ax, **kwargs)
    sns.scatterplot( # top right
        data=df[(df[f'log2({FC})_{cond_vals[0]}']>0) & (df[f'log2({FC})_{cond_vals[1]}']>0)],
        x=f'log2({FC})_{cond_vals[0]}', y=f'log2({FC})_{cond_vals[1]}',
        hue=f'magnitude(log2({FC}))', edgecolor=edgecol, palette='Reds', 
        style=stys, style_order=stys_order if stys_order else None, markers=mark_order if mark_order else None,
        size=size if display_legend else None, sizes=sizes, size_norm=size_norm, 
        legend=False,
        ax=ax, **kwargs)

    # with x- & y-axis lines
    if display_axis == True:
        ax.plot([x_axis_dims[0], x_axis_dims[1]], [0,0], color='black', linestyle='-', linewidth=1)
        ax.plot([0,0], [y_axis_dims[0], y_axis_dims[1]], color='black', linestyle='-', linewidth=1)

    # with correlation line and coefficient
    if weighted==True and size not in [False,'False','false']: # weighted
        coeff = st.weighted_correlation(
            df=df,
            x=f'log2({FC})_{cond_vals[0]}',
            y=f'log2({FC})_{cond_vals[1]}',
            weight=size,
            method=method)
        
        a,b = st.weighted_corr_line(df=df, ax=ax,
                                    x=f'log2({FC})_{cond_vals[0]}', 
                                    y=f'log2({FC})_{cond_vals[1]}', 
                                    weight=size)

    else: # unweighted
        coeff = st.weighted_correlation(
            df=df,
            x=f'log2({FC})_{cond_vals[0]}',
            y=f'log2({FC})_{cond_vals[1]}',
            method=method)
        
        a,b = st.corr_line(df=df, ax=ax,
                            x=f'log2({FC})_{cond_vals[0]}', 
                            y=f'log2({FC})_{cond_vals[1]}')

    # correlation equation string
    m = "?"
    if method == 'pearson': m = "R"
    if method == 'spearman': m = ""
    if method == 'kendall': m = ""

    if is_html:
        if np.isfinite(a) and np.isfinite(b):
            if a >= 0:
                corr_eq = f" (y = {b:.2f}x + {a:.2f}; {m} = {coeff**2:.1g})"
            else:
                corr_eq = f" (y = {b:.2f}x - {abs(a):.2f}; {m} = {coeff**2:.1g})"
        else:
            corr_eq = f" (undefined; {m} = {coeff**2:.1g})"
    else:
        if np.isfinite(a) and np.isfinite(b):
            if a >= 0:
                corr_eq = f"\ny = {b:.2f}x + {a:.2f}; {m} = {coeff**2:.1g})"
            else:
                corr_eq = f"\ny = {b:.2f}x - {abs(a):.2f}; {m} = {coeff**2:.1g})"
        else:
            corr_eq = f"\nundefined; {m} = {coeff**2:.1g})"

    # with legend
    if display_legend == True:
        if size_norm is not None and size is not None: # Add consistent size legend with 5 representative values
            if stys is not None and mark_order is not None and stys_order is not None: # Add stys legend too
                legend_vals = np.linspace(_vmin, _vmax, len(mark_order))
                for lv,mark,sty in zip(legend_vals,mark_order,stys_order):
                    plt.scatter([], [], s=np.interp(lv, [_vmin, _vmax], sizes), color='lightgray', label=f'{lv:.2g}; {sty}', marker=mark)
                if is_html:
                    if legend_bbox_to_anchor == (1,1): legend_bbox_to_anchor = (-0.1,-0.15)
                    if legend_ncol == 1: legend_ncol = 3
                    plt.legend(title=legend_title if legend_title!='' else f'{size}; {stys}', 
                                title_fontsize=legend_title_size, fontsize=legend_size,
                                bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol,
                                columnspacing=legend_columnspacing,    # space between columns
                                handletextpad=legend_handletextpad,    # space between marker and text
                                labelspacing=legend_labelspacing,      # vertical space between entries
                                borderpad=legend_borderpad,            # padding inside legend box
                                handlelength=legend_handlelength)      # marker length
                else:
                    plt.legend(title=legend_title if legend_title!='' else f'{size}; {stys}', 
                                title_fontsize=legend_title_size, fontsize=legend_size,
                                bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol)
                
            else:
                legend_vals = np.linspace(_vmin, _vmax, 5)
                for lv in legend_vals:
                    plt.scatter([], [], s=np.interp(lv, [_vmin, _vmax], sizes), color='lightgray', label=f'{lv:.2g}')
                plt.legend(title=legend_title if legend_title!='' else size, 
                            title_fontsize=legend_title_size, fontsize=legend_size,
                            bbox_to_anchor=legend_bbox_to_anchor, loc=legend_loc, ncol=legend_ncol)
    
    # with labels
    if display_labels == True:
        if is_html:
            # For HTML, show labels interactively as tooltips instead of static text
            pts = ax.scatter(
                x=df[f'log2({FC})_{cond_vals[0]}'],
                y=df[f'log2({FC})_{cond_vals[1]}'],
                s=20,
                alpha=0
            )
            labels_list = df[label].fillna("").astype(str).tolist()
            tooltip = p.SafeHTMLTooltip(pts, labels_list)
            clicker = p.ClickTooltip(pts, labels_list)
            mpld3.plugins.connect(fig, tooltip, clicker)
        else:
            # For static images, keep labels as always-visible text
            for i, l in enumerate(df[label]):
                plt.text(
                    x=df.iloc[i][f'log2({FC})_{cond_vals[0]}'],
                    y=df.iloc[i][f'log2({FC})_{cond_vals[1]}'],
                    s=l
                )
    
    # Set x axis
    if x_axis=='': x_axis=f'log2({FC})_{cond_vals[0]}'
    plt.xlabel(x_axis, fontsize=x_axis_size, fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
    if x_ticks==[]: 
        if x_ticks_rot==0: plt.xticks(rotation=x_ticks_rot,ha='center',va='top',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        elif x_ticks_rot == 90: plt.xticks(rotation=x_ticks_rot,ha='right',va='center',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        else: plt.xticks(rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font,fontsize=x_ticks_size)
    else: 
        if x_ticks_rot==0: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot, ha='center',va='top',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        elif x_ticks_rot == 90: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,ha='right',va='center',fontfamily=x_ticks_font,fontsize=x_ticks_size)
        else: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font,fontsize=x_ticks_size)

    # Set y axis
    if y_axis=='': y_axis=f'log2({FC})_{cond_vals[1]}'
    plt.ylabel(y_axis, fontsize=y_axis_size, fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad)

    if y_ticks==[]: plt.yticks(rotation=y_ticks_rot,fontfamily=y_ticks_font,fontsize=y_ticks_size)
    else: plt.yticks(ticks=y_ticks,labels=y_ticks,rotation=y_ticks_rot,fontfamily=y_ticks_font,fontsize=y_ticks_size)

    # Set title
    if title=='' and file is not None: 
        if space_capitalize: title=p.re_un_cap(".".join(file.split(".")[:-1]))
        else: ".".join(file.split(".")[:-1])
    title += corr_eq
    plt.title(title, fontsize=title_size, fontweight=title_weight, family=title_font)

    # Save & show fig; return dataframe
    p.save_fig(file=file, dir=dir, fig=fig, dpi=dpi, PDB_pt=PDB_pt, icon='scatter')
    if show:
        ext = file.split('.')[-1].lower() if file is not None else ''
        if ext not in ('html', 'json'):
            plt.show()
        else:
            mpld3.show(fig)
    if return_df:
        return df

def heat(df: pd.DataFrame | str, cond_col: str, cond: str, FC: str, wt_prot: str, wt_res: int, cutoff: float=0, aa: str='aa', label: str='Edit',
        file: str=None, dir: str=None, edgecol: str='black', lw: int=1, center: float=0, cmap: str="seismic", cmap_WT: str='forestgreen', cmap_not_WT: str='lightgray', sq: bool=False, 
        cbar: bool=True, cbar_label: str=None, cbar_label_size: int=None, cbar_label_weight: str='bold', cbar_tick_size: int=None, cbar_shrink: float=None, cbar_aspect: int=None, cbar_pad: float=None, cbar_orientation: str=None,
        title: str='', title_size: int=12, title_weight: str='bold', title_font: str='Arial', figsize: tuple = (5, 5), vertical: bool=True,
        x_axis: str='', x_axis_size: int=12, x_axis_weight: str='bold', x_axis_font: str='Arial', x_axis_pad: int=None, x_ticks_size: int=9, x_ticks_rot: int=None, x_ticks_font: str='Arial',
        y_axis: str='', y_axis_size: int=12, y_axis_weight: str='bold', y_axis_font: str='Arial', y_axis_pad: int=None, y_ticks_size: int=9, y_ticks_rot: int=None, y_ticks_font: str='Arial',
        dpi: int=0, show: bool=True, **kwargs):
    ''' 
    heat(): creates heatmap plot
    
    Parameters:
    df (dataframe | str): pandas dataframe (or file path) from st.compare()
    cond_col (str): Condition column name
    cond (str): Condition column value for filtering
    FC (str): fold change column name (values within heatmap after log2 transformation)
    wt_prot (str): WT protein sequence
    wt_res (int): WT protein sequence residue start number
    cutoff (float, optional): comparison count mean cutoff for masking low-abundance values
    aa (str, optional): AA saturation mutagenesis (Options: 'aa' [default], 'aa_subs', 'aa_ins', 'aa_dels'). The 'aa' option makes all amino acid substitutions ('aa_subs'), +1 amino acid insertions ('aa_ins'), and -1 amino acid deletions ('aa_dels').
    label (str, optional): label column name (Default: 'Edit'). Can't be None.
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    edgecol (str, optional): point edge color
    lw (int, optional): line width
    center (float, optional): center value for colormap
    cmap (str, optional): matplotlib color map
    sq (bool, optional): square dimensions (Default: False)
    cmap_WT (str, optional): color for WT values in colorbar
    cmap_not_WT (str, optional): color for non-WT values in colorbar
    cbar (bool, optional): show colorbar (Default: True)
    cbar_label (str, optional): colorbar label
    cbar_label_size (int, optional): colorbar label font size
    cbar_label_weight (str, optional): colorbar label bold, normal, & heavy
    cbar_tick_size (int, optional): colorbar tick font size
    cbar_shrink (float, optional): colorbar shrink factor
    cbar_aspect (int, optional): colorbar aspect ratio
    cbar_pad (float, optional): colorbar padding
    cbar_orientation (str, optional): colorbar orientation ('vertical' | 'horizontal')
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    figsize (tuple, optional): figure size per subplot
    vertical (bool, optional): vertical orientation; otherwise horizontal (Default: True)
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_pad (int, optional): x-axis label padding
    x_ticks_size (int, optional): x-axis ticks font size
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_pad (int, optional): y-axis label padding
    y_ticks_size (int, optional): y-axis ticks font size
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-ticks font
    dpi (int, optional): figure dpi (Default: 600 for non-HTML, 150 for HTML)
    show (bool, optional): show plot (Default: True)
    
    Dependencies: matplotlib, seaborn, pandas, & aa_props
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # cbar kwargs
    cbar_kws = dict()
    if cbar_label is not None: cbar_kws['label'] = cbar_label
    else: cbar_kws['label'] = f'log2({FC})'
    if cbar_shrink is not None: cbar_kws['shrink'] = cbar_shrink
    if cbar_aspect is not None: cbar_kws['aspect'] = cbar_aspect
    if cbar_pad is not None: cbar_kws['pad'] = cbar_pad
    if cbar_orientation is not None: cbar_kws['orientation'] = cbar_orientation
    
    # Isolate condition dataframe
    df = df[df[cond_col]==cond]
    
    # Add label info: Before, Number, After, AA properties
    df = add_label_info(df=df, label=label)
    df.drop(columns=[f'{label}_info'], inplace=True)

    # Determine if we are saving to HTML (for interactive behavior)
    if file is not None:
        is_html = file.endswith('.html')
        
        if is_html == True:
            # Match title fontsize for html plots
            x_axis_size=title_size
            y_axis_size=title_size
            x_ticks_size=title_size
            y_ticks_size=title_size

    else:
        is_html = False

    # Log transform data
    df[f'log2({FC})'] = [np.log10(FC_val)/np.log10(2) for FC_val in df[FC]]

    # Set DMS Before & After
    before_ls = []
    after_ls = []

    for (before,after) in t.zip_cols(df=df, cols=['Before','After']):
        if len(before)==1 & len(after)==1: # Substitution
            before_ls.append(before)
            after_ls.append(after)

        elif len(before)==1 & len(after)>1: # Insertion
            before_ls.append(before)
            after_ls.append(f"ins{after[1:]}")

        elif len(before)>1 & len(after)==1: # Deletion
            before_ls.append(before[:-1])
            if before_ls[-1]==before[0]:
                after_ls.append("del")
            else:
                after_ls.append(f"{before[:-1]}del")

        else: # Complex (multiple changes)
            before_ls.append(before)
            after_ls.append(after)
    
    df['DMS_Before'] = before_ls
    df['DMS_After'] = after_ls

    def dms_region(df_temp: pd.DataFrame, aa_: str) -> pd.DataFrame:
        '''
        dms_region(): Extracts DMS grid data for specified amino acid changes
        '''
        wt_nums = np.arange(wt_res,wt_res+len(wt_prot))

        df_ls = []
        for num in wt_nums: # Iterate through WT protein sequence
            vals=dict() # Create dictionary with all amino acid changes for a given residue
            
            # Create all amino acid changes
            if aa_=='aa_subs': # Substitutions
                vals['DMS_Before']=[wt_prot[num-wt_res]]*len(list(aa_props.keys()))
                vals['AA Number']=[num]*len(list(aa_props.keys()))
                vals['DMS_After']=list(aa_props.keys())
                vals['DMS_Before_Number']=[f"{wt_prot[num-wt_res]}{num}"]*len(list(aa_props.keys()))
            
            elif aa_=='aa_ins': # Insertions
                vals['DMS_Before']=[wt_prot[num-wt_res]]*(len(list(aa_props.keys()))-1)
                vals['AA Number']=[num]*(len(list(aa_props.keys()))-1)
                vals['DMS_After']=[f"ins{aa}" for aa in list(aa_props.keys()) if aa!='*']
                vals['DMS_Before_Number']=[f"{wt_prot[num-wt_res]}{num}"]*(len(list(aa_props.keys()))-1)
            
            elif aa_=='aa_dels': # Deletions
                vals['DMS_Before']=[wt_prot[num-wt_res]]
                vals['AA Number']=[num]
                vals['DMS_After']=["del"]
                vals['DMS_Before_Number']=[f"{wt_prot[num-wt_res]}{num}"]
            
            # Extract log2FC, label info, and comparison count mean for each amino acid change
            log2FC_ls=[]
            #label_ls=[]
            comparison_count_mean_ls=[]
            
            # Filter to specific residue and WT amino acid
            df_temp2 = df_temp[(df_temp['AA Number']==num) & (df_temp['DMS_Before']==wt_prot[num-wt_res])]

            for DMS_after in vals['DMS_After']:

                # Filter to specific amino acid change
                df_temp3 = df_temp2[df_temp2['DMS_After'] == DMS_after]
                if df_temp3.empty == False: # Present
                    log2FC_ls.append(df_temp3[f'log2({FC})'].to_list()[0])
                    #label_ls.append(df_temp3[label].to_list()[0])
                    comparison_count_mean_ls.append(df_temp3['count_mean_compare'].to_list()[0])

                    if df_temp3.shape[0]>1:
                        print(f"Warning: Multiple entries found for {wt_prot[num-wt_res]}{num}{DMS_after} in condition {cond}. Using first entry.")

                else: # Absent
                    log2FC_ls.append(None)
                    #label_ls.append('')
                    comparison_count_mean_ls.append(-1)

            vals[f'log2({FC})']=log2FC_ls
            #vals[label]=label_ls
            vals['count_mean_compare']=comparison_count_mean_ls
            df_ls.append(pd.DataFrame(vals))

        return pd.concat(df_ls, ignore_index=True)

    # Restrict to specified aa mutations
    df2 = pd.DataFrame()

    if aa == 'aa_subs' or aa == 'aa': # Substitutions
        df2 = pd.concat([df2, 
                        dms_region(df_temp = df[(df['DMS_Before'].str.len()==1) & (df['DMS_After'].str.len()==1)],
                                    aa_='aa_subs')], ignore_index=True)
    
    if aa == 'aa_ins' or aa == 'aa': # +1 Insertions
        df2 = pd.concat([df2, 
                        dms_region(df_temp = df[df['DMS_After'].str.startswith('ins')],
                                    aa_='aa_ins')], ignore_index=True)
    
    if aa == 'aa_dels' or aa == 'aa': # -1 Deletions
        df2 = pd.concat([df2, 
                        dms_region(df_temp = df[df['DMS_After']=='del'],
                                    aa_='aa_dels')], ignore_index=True)

    # Make DMS grid
    if aa == 'aa': index = list(aa_props.keys()) + ['del'] + [f"ins{key}" for key in aa_props.keys() if key != '*']
    elif aa == 'aa_subs': index = list(aa_props.keys())
    elif aa == 'aa_ins': index = [f"ins{key}" for key in aa_props.keys() if key != '*']
    elif aa == 'aa_dels': index = ['del']

    df2_log2FC = pd.pivot(df2, columns='AA Number', index='DMS_After', values=f'log2({FC})').astype(float).reindex(index)
    #df2_label = pd.pivot(df2, columns='AA Number', index='DMS_After', values=label).astype(str).reindex(index)
    df2_compare_count_mean = pd.pivot(df2, columns='AA Number', index='DMS_After', values='count_mean_compare').astype(float).reindex(index)

    # Mask values below cutoff & WT
    mask_not_WT = np.zeros(df2_log2FC.shape, dtype=bool)
    mask_WT = np.zeros(df2_log2FC.shape, dtype=bool)

    # Iterate through dataframe to set masks
    nrows, ncols = df2_log2FC.shape
    for i in range(nrows):
        for j in range(ncols):
            if df2_log2FC.index[i] == df2[(df2['AA Number'] == df2_log2FC.columns[j])].reset_index(drop=True).iloc[0,0]:
                mask_WT[i, j] = True # WT
            elif np.isnan(df2_log2FC.iat[i, j]) or df2_compare_count_mean.iat[i, j] < cutoff:
                mask_not_WT[i, j] = True # NaN value or comparison count mean below cutoff

    if vertical==True: # Vertical orientation

        # Create figure
        fig, ax = plt.subplots(figsize=(figsize[0],figsize[1]))
        
        sns.heatmap(df2_log2FC, mask=mask_not_WT | mask_WT, # Data heatmap
                    cbar_kws=cbar_kws,
                    cmap=cmap, center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=cbar, square=sq, 
                    **kwargs)
        
        sns.heatmap(mask_not_WT, mask=~mask_not_WT, # Masked low-abundance values
                    cmap=[cmap_not_WT], center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=False, square=sq, 
                    **kwargs)

        sns.heatmap(mask_WT, mask=~mask_WT, # Masked WT values
                    cmap=[cmap_WT], center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=False, square=sq, 
                    **kwargs)
        
        # Format x axis
        if x_axis=='': ax.set_xlabel("AA",fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
        else: ax.set_xlabel(x_axis,fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
        
        # Format y axis
        if y_axis=='': ax.set_ylabel("Change",fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad) # Add y axis label
        else: ax.set_ylabel(y_axis,fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad)
        
        # Format x ticks
        ax.set_xticks(np.arange(df2_log2FC.shape[1]) + 0.5)
        ax.set_xticklabels(df2['DMS_Before_Number'].unique())
        if x_ticks_rot is None: x_ticks_rot=45
        if x_ticks_rot==0: plt.setp(ax.get_xticklabels(), rotation=x_ticks_rot, ha="center", va="top", rotation_mode="anchor",fontname=x_ticks_font,fontsize=x_ticks_size) 
        elif x_ticks_rot == 90: plt.setp(ax.get_xticklabels(), rotation=x_ticks_rot, ha="right", va="center", rotation_mode="anchor",fontname=x_ticks_font,fontsize=x_ticks_size)
        else: plt.setp(ax.get_xticklabels(), rotation=x_ticks_rot, ha="right",rotation_mode="anchor",fontname=x_ticks_font,fontsize=x_ticks_size) 
        
        # Format y ticks
        ax.set_yticks(np.arange(df2_log2FC.shape[0]) + 0.5)
        ax.set_yticklabels(df2_log2FC.index)
        if y_ticks_rot is None: y_ticks_rot=0
        plt.setp(ax.get_yticklabels(), rotation=y_ticks_rot, va='center', ha="right",rotation_mode="anchor",fontname=y_ticks_font,fontsize=y_ticks_size)
    
    else: # Horizontal orientation (Transpose)

        # Create figure
        fig, ax = plt.subplots(figsize=(figsize[0],figsize[1]))
        
        sns.heatmap(df2_log2FC.T, mask=mask_not_WT.T | mask_WT.T, # Data heatmap
                    cbar_kws=cbar_kws,
                    cmap=cmap, center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=cbar, square=sq, 
                    **kwargs)
        
        sns.heatmap(mask_not_WT.T, mask=~mask_not_WT.T, # Masked low-abundance values
                    cmap=[cmap_not_WT], center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=False, square=sq, 
                    **kwargs)

        sns.heatmap(mask_WT.T, mask=~mask_WT.T, # Masked WT values
                    cmap=[cmap_WT], center=center,
                    ax=ax, linecolor=edgecol, linewidths=lw, 
                    cbar=False, square=sq, 
                    **kwargs)

        # Format y axis
        if x_axis=='': ax.set_ylabel("AA",fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
        else: ax.set_ylabel(x_axis,fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font, labelpad=x_axis_pad)
        
        # Format x axis
        if y_axis=='': ax.set_xlabel("Change",fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad) # Add y axis label
        else: ax.set_xlabel(y_axis,fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font, labelpad=y_axis_pad)
        
        # Format y ticks
        ax.set_yticks(np.arange(df2_log2FC.shape[1]) + 0.5)
        ax.set_yticklabels(df2['DMS_Before_Number'].unique())
        if x_ticks_rot is None: x_ticks_rot=0
        plt.setp(ax.get_yticklabels(), rotation=x_ticks_rot, va="center", ha="right",rotation_mode="anchor",fontname=x_ticks_font,fontsize=x_ticks_size) 
        
        # Format x ticks
        ax.set_xticks(np.arange(df2_log2FC.shape[0]) + 0.5)
        ax.set_xticklabels(df2_log2FC.index)
        if y_ticks_rot is None: y_ticks_rot=45
        if y_ticks_rot==0: plt.setp(ax.get_xticklabels(), rotation=y_ticks_rot, ha="center", va="top", rotation_mode="anchor",fontname=y_ticks_font,fontsize=y_ticks_size)
        elif y_ticks_rot == 90: plt.setp(ax.get_xticklabels(), rotation=y_ticks_rot, ha="right", va="center", rotation_mode="anchor",fontname=y_ticks_font,fontsize=y_ticks_size)
        else: plt.setp(ax.get_xticklabels(), rotation=y_ticks_rot, ha="right",rotation_mode="anchor",fontname=y_ticks_font,fontsize=y_ticks_size)

    # Format title
    ax.set_title(title,fontsize=title_size,fontweight=title_weight,fontfamily=title_font)

    # Format cbar
    cbar = ax.collections[0].colorbar
    vmin, vmax = cbar.vmin, cbar.vmax
    if center is None: center = (vmin + vmax) / 2
    cbar.set_label(f"log2({FC})" if cbar_label is None else cbar_label, fontsize=cbar_label_size, fontweight=cbar_label_weight)
    cbar.set_ticks([vmin, center, vmax])
    cbar.ax.tick_params(labelsize=cbar_tick_size)
        
    # Set background to transparent
    ax.set_facecolor('white')

    # Save & show fig; return dataframe
    p.save_fig(file=file, dir=dir, fig=fig, dpi=dpi, icon='heat')
    if show:
        ext = file.split('.')[-1].lower() if file is not None else ''
        if ext not in ('html', 'json'):
            plt.show()
        else:
            mpld3.show(fig)