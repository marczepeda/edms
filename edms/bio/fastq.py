'''
Module: fastq.py
Author: Marc Zepeda
Created: 2024-08-05
Description: Fastq processing and analysis 

Usage:
[Supporting methods for sequences]
- fuzzy_substring_search: retuns a dataframe containing all substrings in 'text' that resemble 'pattern' within the Levenshtein 'max_distance'.

[Input/Output]
- revcom_fastqs(): write reverse complement of fastqs to a new directory
- unzip_fastqs(): Unzip gzipped fastqs and write to a new directory
- comb_fastqs(): Combines one or more (un)compressed fastqs files into a single (un)compressed fastq file

[Quantify epeg/ngRNA abundance]
- [Motif search: mU6,...]
    - count_motif(): returns a dataframe with the sequence motif location per read and abundance for every fastq file in a directory
    - plot_motif(): generate plots highlighting motif mismatches, locations, and sequences
- [Region/read alignments: spacer,..., & ngRNA-epegRNA]
    - plot_alignments(): generate line plots from fastq alignments dictionary
    - count_region(): align read region from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary
    - count_alignments(): align reads from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary

[Quantify edit outcomes]
- trim_filter(): trim and filter fastq sequence based on quality scores
- get_fastqs(): get fastq files from directory and store records in dataframes in a dictionary
- region(): gets DNA and AA sequence for records within flanks
- genotype(): assign genotypes to sequence records
- outcomes(): returns edit count & fraction per sample (tidy format)
- outcomes_desired(): groups desired edit count & fraction per sample (tidy format)
- genotyping(): quantify edit outcomes workflow

[Supporting methods for DMS plots]
- aa_props: dictionary of AA properties with citations (Generated by ChatGPT)
- edit_1(): split edit column to before, after, and amino acid number
- dms_cond(): returns DMS grid data in tidy format grouped by condition
- dms_comp(): returns comparison DMS grid dataframe in tidy format split by condition
- subscript(): returns dataframe with subscripts to tick labels

[Plot methods]
- scat(): creates scatter plot related graphs
- cat: creates category dependent graphs
- stack(): creates stacked bar plot
- heat(): creates heatmap
- vol(): creates volcano plot
'''

# Import packages
from Bio.Seq import Seq
from Bio import SeqIO
from Bio.Align import PairwiseAligner
import gzip
import os
import re
import datetime
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from adjustText import adjust_text
from collections import Counter
from pathlib import Path
from scipy.stats import ttest_ind
import Levenshtein

from ..gen import io
from ..gen import tidy as t
from ..gen import plot as p

# Get rid of warnings
import warnings
warnings.filterwarnings("ignore")

# Supporting methods for sequences
def fuzzy_substring_search(text: str, pattern: str, max_distance: int):
    """
    fuzzy_substring_search: retuns a dataframe containing all substrings in 'text' that resemble 'pattern' within the Levenshtein 'max_distance'.
    
    Parameters:
    text (str): text to search within
    pattern (str): text to search for
    max_distance (int): maximum Levenshtein distance of 'pattern' in text substring

    Dependencies: Levenshtein, pandas
    """
    # Initialize lists that will be stored in the output dataframe
    windows = []
    distances = []
    starts_i = []
    ends_i = []

    # Iterate through 'text' using windows with equal length to 'pattern'
    len_pat = len(pattern)
    for i in range(len(text) - len_pat + 1):
        window = text[i:i + len_pat]
        distance = Levenshtein.distance(window, pattern)
        if distance <= max_distance: # Save windows that resemble 'pattern' within the Levenshtein 'max_distance'
            windows.append(window)
            distances.append(distance)
            starts_i.append(int(i))
            ends_i.append(int(i + len_pat))

    # Return final dataframe
    return pd.DataFrame({'pattern': [pattern]*len(windows),
                         'window': windows,
                         'distance': distances,
                         'start_i': starts_i,
                         'end_i': ends_i})

# Input/Output
def revcom_fastqs(in_dir: str, out_dir: str):
    ''' 
    revcom_fastqs(): write reverse complement of fastqs to a new directory
    
    Parameters:
    in_dir (str): directory with fastq files
    out_dir (str): new directory with reverse complement fastq files
    
    Dependencies: Bio.SeqIO, gzip, os, & Bio.Seq.Seq
    '''
    io.mkdir(out_dir) # Ensure the output directory exists

    for filename in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory

        if filename.endswith(".fastq.gz"):
            input_fastq_gz = os.path.join(in_dir, filename)
            output_fastq_gz = os.path.join(out_dir, filename)
            print(f"Processing {filename}...")
            with gzip.open(input_fastq_gz, "rt") as infile, gzip.open(output_fastq_gz, "wt") as outfile:
                for record in SeqIO.parse(infile, "fastq"):
                    reverse_complement_seq = record.seq.reverse_complement() # Compute the reverse complement of the sequence
                    reverse_complement_record = record[:] # Create a new record with the reverse complement sequence
                    reverse_complement_record.seq = reverse_complement_seq # Write the new record to the output file
                    SeqIO.write(reverse_complement_record, outfile, "fastq")
            print(f"Saved reverse complement to {output_fastq_gz}")
        
        elif filename.endswith(".fastq"):
            input_fastq = os.path.join(in_dir, filename)
            output_fastq = os.path.join(out_dir, filename)
            print(f"Processing {filename}...")
            with open(input_fastq, "r") as infile, open(output_fastq, "w") as outfile:
                for record in SeqIO.parse(infile, "fastq"):
                    reverse_complement_seq = record.seq.reverse_complement() # Compute the reverse complement of the sequence
                    reverse_complement_record = record[:] # Create a new record with the reverse complement sequence
                    reverse_complement_record.seq = reverse_complement_seq # Write the new record to the output file
                    SeqIO.write(reverse_complement_record, outfile, "fastq")
            print(f"Saved reverse complement to {output_fastq_gz}")

def unzip_fastqs(in_dir: str, out_dir: str):
    ''' 
    unzip_fastqs(): Unzip gzipped fastqs and write to a new directory

    Parameters:
    in_dir (str): directory with compresesd fastq files
    out_dir (str): new directory with uncompressed fastq files
    
    Dependencies: gzip & os
    '''
    io.mkdir(out_dir) # Ensure the output directory exists

    for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
        print(f"Processing {in_file}...")
        if in_file.endswith(".fastq.gz"):
            with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                with open(os.path.join(out_dir,in_file.split('.fastq.gz')[0]+'.fastq'), 'wt') as out:
                    for line in handle:
                        out.write(line)

def comb_fastqs(in_dir: str, out_dir: str, out_file: str):
    ''' 
    comb_fastqs(): Combines one or more (un)compressed fastqs files into a single (un)compressed fastq file

    Parameters:
    in_dir (str): directory with fastq files
    out_dir (str): new directory with combined fastq file
    out_file (str): Name of output fastq file (Needs .fastq or .fastq.gz suffix)
    
    Dependencies: gzip & os
    '''
    io.mkdir(out_dir) # Ensure the output directory exists

    if out_file.endswith(".fastq.gz"):
        with gzip.open(os.path.join(out_dir,out_file), 'wt') as out:
            for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
                print(f"Processing {in_file}...")
                if in_file.endswith(".fastq.gz"):
                    with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                        for line in handle:
                            out.write(line)
                
                elif in_file.endswith(".fastq"):
                    with open(os.path.join(in_dir,in_file), 'r') as handle:
                        for line in handle:
                            out.write(line)
    
    elif out_file.endswith(".fastq"):
        with open(os.path.join(out_dir,out_file), 'wt') as out:
            for in_file in os.listdir(in_dir): # Find all .fastq.gz & .fastq files in the input directory
                print(f"Processing {in_file}...")
                if in_file.endswith(".fastq.gz"):
                    with gzip.open(os.path.join(in_dir,in_file), 'rt') as handle:
                        for line in handle:
                            out.write(line)
                
                elif in_file.endswith(".fastq"):
                    with open(os.path.join(in_dir,in_file), 'r') as f:
                        for line in handle:
                            out.write(line)

    else: print('out_file needs .fastq or .fastq.gz suffix')

# Quantify epeg/ngRNA abundance
### Motif search: mU6,...
def count_motif(fastq_dir: str, pattern: str, motif:str="motif", 
                max_distance:int=0, max_reads:int=0, meta: pd.DataFrame | str=None,
                out_dir:str=None, out_file:str=None):
    ''' 
    count_motif(): returns a dataframe with the sequence motif location with mismatches per read for every fastq file in a directory

    Parameters:
    fastq_dir (str): path to fastq directory
    pattern (str): search for this sequence motif
    motif (str, optional): motif name (Default: 'motif')
    max_distance (int, optional): max Levenstein distance for seq in fastq read (Default: 0)
    meta (DataFrame | str, optional): meta dataframe (or file path) must have 'fastq_file' column (Default: None)
    out_dir (str, optional): path to save directory (Default: None)
    out_file (str, optional): save file name (Default: None)

    Dependencies: pandas,gzip,os,Bio
    '''
    df = pd.DataFrame(columns=['fastq_file','read','motif','pattern','window','start_i','end_i','distance']) # Create a dataframe to store motif abundance
    for fastq_file in os.listdir(fastq_dir): # Find all .fastq.gz & .fastq files in the fastq directory
        
        print(f"Processing {fastq_file}...") # Keep track of sequence motifs & reads
        reads = pd.DataFrame() 

        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            with gzip.open(os.path.join(fastq_dir,fastq_file), 'rt') as handle:
                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    
                    # Find all substrings that resemble 'pattern' within the Levenshtein 'max_distance'.
                    read = fuzzy_substring_search(text=str(record.seq),pattern=pattern,max_distance=max_distance)
                    
                    # Retain the substring with the smallest Levenshtein distance to 'pattern'. 
                    if len(read)==0: # No substring
                        read.loc[0] = [pattern, "N"*len(pattern), -1, -1, -1]
                    else: # Found substring(s)
                        read = read[read['distance']==min(read['distance'])]
                        read = read.iloc[:1]
                    read['read'] = [r+1]
                    reads = pd.concat([reads,read]).reset_index(drop=True)

                    # Processing status and down sample to reduce computation
                    if len(reads)%10000==0: print(f"Processed {len(reads)} reads")
                    if (len(reads)+1>max_reads) & (max_reads!=0): break
                 
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            with open(os.path.join(fastq_dir,fastq_file), 'r') as handle:
                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads 

                    # Find all substrings that resemble 'pattern' within the Levenshtein 'max_distance'.
                    read = fuzzy_substring_search(text=str(record.seq),pattern=pattern,max_distance=max_distance)
                    
                    # Retain the substring with the smallest Levenshtein distance to 'pattern'. 
                    if len(read)==0: # No substring
                        read.loc[0] = [pattern, "N"*len(pattern), -1, -1, -1]
                    else: # Found substring(s)
                        read = read[read['distance']==min(read['distance'])] # smallest Levenshtein distance
                        if len(read)>1: read = read.iloc[:1] # Isolate first instance 
                    read['read'] = [r+1] # Add read index
                    reads = pd.concat([reads,read]).reset_index(drop=True)

                    # Processing status and down sample to reduce computation
                    if len(reads)%10000==0: print(f"Processed {len(reads)} reads")
                    if (len(reads)+1>max_reads) & (max_reads!=0): break
                 

        print(f'Completed {len(reads)} reads') # Append metadata
        reads['fastq_file'] = [fastq_file]*len(reads)
        reads['motif'] = [motif]*len(reads)
        df = pd.concat([df,reads]).reset_index(drop=True) # save to final dataframe
    
    # Improve dataframe column formatting 
    df = df.astype({'start_i': int,'end_i': int,'read': int,'distance':int})
    df['mismatches'] = [f">{max_distance}" if d==-1 else int(d) for d in df['distance']]
    df['location'] = [(start_i,end_i) if start_i!=-1 else "Absent" for (start_i,end_i) in t.zip_cols(df=df,cols=['start_i','end_i'])]

    # Merge with metadata
    if meta is not None:
        if type(meta)==str: # Get from file path if needed
            meta = io.get(pt=meta)
        if 'fastq_file' not in list(meta.columns): # Check for 'fastq_file' column
            raise(ValueError(f"meta needs 'fastq_file' column.\nDetected columns: {list(meta.columns)}"))
        else: # Merge on 'fastq_file' column
            df = pd.merge(left=meta,right=df,on='fastq_file')

    if out_dir is not None and out_file is not None: # Save dataframe (optional)
        io.save(dir=out_dir,file=out_file,obj=df)

    return df

def plot_motif(df: pd.DataFrame | str, out_dir: str=None, plot_suf='.pdf',numeric: str='count',
               id_col: str='fastq_file', id_axis: str='fastq', stack_figsize: tuple=(7,3), heat_figsize: tuple=(7,7),
               cutoff_frac:float=0.01):
    '''
    plot_motif(): generate plots highlighting motif mismatches, locations, and sequences
    
    Parameters:
    df (dataframe | str): count_motif() dataframe (or file path)
    out_dir (str, optional): output directory
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    numeric (str, optional): 'count' or 'fraction' can be the numeric column for plots (Default: 'count')
    id_col (str, optional): id column name (Default: 'fastq_file')
    id_axis (str, optional): replace id column name on plots (Default: 'fastq')
    stack_figsize (tuple, optional): stacked bar plot figure size (Default: (7,3))
    heat_figsize (tuple, optional): heatmap figure size (Default: (7,7))
    cutoff_frac (float, optional): y-axis values needs be greater than (e.g. 0.01) fraction

    Dependencies: count_motifs(),plot
    '''
    # Get dataframe from file path if needed
    if type(df)==str:
        df = io.get(pt=df)
    
    # Check numeric column
    if numeric not in ['count','fraction']:
        raise(ValueError(f"numeric can only be 'count' or 'fraction'; not {numeric}"))

    # Get value_counts() dataframes...
    # ...remove unwanted columns
    mismatches_cols = [c for c in list(df.columns) if c not in ['read','window','start_i','end_i','location']]
    locations_cols = [c for c in list(df.columns) if c not in ['read','window','distance','mismatches']]
    windows_cols = [c for c in list(df.columns) if c not in ['read','start_i','end_i','distance','mismatches','location']]

    # ...generate dataframes
    df_mismatches = df[mismatches_cols].value_counts().reset_index()
    df_locations = df[locations_cols].value_counts().reset_index()
    df_windows = df[windows_cols].value_counts().reset_index()
    
    # ...save (optional)
    if out_dir is not None:
        io.save(dir=out_dir,file=f"{df.iloc[0]['motif']}_mismatches.csv",obj=df_mismatches)
        io.save(dir=out_dir,file=f"{df.iloc[0]['motif']}_locations.csv",obj=df_locations)
        io.save(dir=out_dir,file=f"{df.iloc[0]['motif']}_windows.csv",obj=df_windows)

    # ...define cut() based on cutoff_frac
    def cut(df_vc: pd.DataFrame, col: str, off: bool=True):
        '''
        cut(): apply cutoff_fraction to value_counts() dataframe grouped by id

        Parameters:
        df_vc (dataframe): value_counts() dataframe
        col (str): column name that will be overwritten with cutoff_frac
        off (bool, optional): apply cutoff (Default: True)
        '''
        df_vc_cutoff = pd.DataFrame() # Initialize output dataframe
        for id in list(df_vc[id_col].value_counts().keys()): # Iterate through ids
            # Group by id and calculate fraction
            df_vc_id = df_vc[df_vc[id_col]==id]
            df_vc_id['fraction'] = df_vc_id['count']/sum(df_vc_id['count'])
            if off: # Apply cutoff_fract
                # Append greater than cutoff_frac
                df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id[df_vc_id['fraction']>=cutoff_frac]]).reset_index(drop=True)
                # Group less than cutoff_frac and append
                df_vc_id_other = df_vc_id[df_vc_id['fraction']<cutoff_frac].reset_index(drop=True)
                if df_vc_id_other.empty==False:
                    df_vc_id_other['count']=sum(df_vc_id_other['count'])
                    df_vc_id_other['fraction']=sum(df_vc_id_other['fraction'])
                    df_vc_id_other[col]=f'<{cutoff_frac*100}%'
                    df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id_other.iloc[:1]]).reset_index(drop=True)
            else: # Do not apply cutoff_fract
                df_vc_cutoff = pd.concat([df_vc_cutoff,df_vc_id]).reset_index(drop=True)  

        # Return output dataframe
        return df_vc_cutoff
    
    #...apply cut()
    df_mismatches = cut(df_vc=df_mismatches,col='mismatches',off=False)
    df_locations = cut(df_vc=df_locations,col='location')
    df_windows = cut(df_vc=df_windows,col='window')

    # Plots
    if numeric=='count':
        p.stack(df=df_mismatches,x=id_col,y=numeric,cols='mismatches', y_axis='Reads',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,cmap='tab20',
                dir=out_dir,file=f"{df.iloc[0]['motif']}_mismatches{plot_suf}")
        
        p.stack(df=df_locations,x=id_col,y=numeric,cols='location', y_axis='Reads',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,cmap='tab20',
                dir=out_dir,file=f"{df.iloc[0]['motif']}_locations{plot_suf}")
        
        p.heat(df=df_windows,x=id_col,y='window',vars='motif',vals=numeric,x_axis=id_axis,y_ticks_font='Courier New',
               figsize=heat_figsize,x_ticks_rot=45,title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}",
               dir=out_dir,file=f"{df.iloc[0]['motif']}_windows{plot_suf}")
    
    else: # fraction
        p.stack(df=df_mismatches,x=id_col,y=numeric,cols='mismatches', y_axis='Reads fraction',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,cmap='tab20',
                dir=out_dir,file=f"{df.iloc[0]['motif']}_mismatches{plot_suf}")
        
        p.stack(df=df_locations,x=id_col,y=numeric,cols='location', y_axis='Reads fraction',
                title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}", x_axis=id_axis,
                vertical=False,figsize=stack_figsize,cmap='tab20',
                dir=out_dir,file=f"{df.iloc[0]['motif']}_locations{plot_suf}")
        
        p.heat(df=df_windows,x=id_col,y='window',vars='motif',vals=numeric,x_axis=id_axis,y_ticks_font='Courier New',
               figsize=heat_figsize,x_ticks_rot=45,title=f"{df.iloc[0]['motif']}: {df.iloc[0]['pattern']}",
               dir=out_dir,file=f"{df.iloc[0]['motif']}_windows{plot_suf}",vals_dims=(0,1)) 
    
    # Return value_counts() dataframes
    return (df_mismatches,df_locations,df_windows)

### Region/read alignments: spacer,..., & ngRNA-epegRNA
def plot_alignments(fastq_alignments: dict, align_col: str, id_col: str,
                    out_dir: str, plot_suf='.pdf', show=False, **plot_kwargs):
    ''' 
    plot_alignments(): generate line plots from fastq alignments dictionary
    
    Parameters:
    fastq_alignments (dict): fastq alignments dictionary from count_region() or count_alignments()
    align_col (str): align column name in annotated library reference file
    id_col (str): id column name in annotated library reference file
    fastq_dir (str): directory with fastq files
    out_dir (str): directory for output files
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: pandas & plot
    '''
    for fastq_name,df_fastq in fastq_alignments.items():
        
        # Plot mismatch position per alignment
        print('Plot mismatch position per alignment')
        
        out_dir_fastq_name = os.path.join(out_dir,fastq_name)
        df_fastq_plot = pd.DataFrame()
        for align,id,mismatch_pos_per_alignment in t.zip_cols(df=df_fastq,cols=[align_col,id_col,'mismatch_pos_per_alignment']):
            df_fastq_plot_align = pd.DataFrame({align_col:[align]*len(mismatch_pos_per_alignment), # Obtain individual alignments
                                                id_col:[id]*len(mismatch_pos_per_alignment),
                                                'mismatch_pos':list(mismatch_pos_per_alignment.keys()),
                                                'mismatch_pos_per_alignment':list(mismatch_pos_per_alignment.values())})
            
            p.scat(typ='line',df=df_fastq_plot_align,x='mismatch_pos',y='mismatch_pos_per_alignment', # Plot mismatches for each alignment
                   title=f'{fastq_name} {id}',x_axis='Alignment Position',y_axis='Mismatches/Alignment',
                   dir=out_dir_fastq_name,file=f'{id.replace(".","_")}{plot_suf}',
                   show=show,**plot_kwargs)
            
            df_fastq_plot = pd.concat(objs=[df_fastq_plot,df_fastq_plot_align]).reset_index(drop=True) # Group alignment mismatches

        p.scat(typ='line',df=df_fastq_plot,x='mismatch_pos',y='mismatch_pos_per_alignment',cols=id_col, # Plot mismatches for each alignment
               title=f'{fastq_name}',x_axis='Alignment Position',y_axis='Mismatches/Alignment',
               dir=out_dir,file=f'{fastq_name}{plot_suf}',
               show=show,**plot_kwargs)

def count_region(df_ref: pd.DataFrame | str, align_col: str, id_col: str, fastq_dir: str,
                 df_motif5: pd.DataFrame | str, df_motif3: pd.DataFrame | str,
                 out_dir: str, match_score:int=1, mismatch_score:int=-4,
                 align_max:int=None, plot_suf:str='.pdf', show:bool=False, **plot_kwargs):
    ''' 
    count_region(): align read region from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary

    Parameters:
    df_ref (dataframe | str): annotated reference library dataframe (or file path)
    align_col (str): align column name in the annotated reference library
    id_col (str): id column name in the annotated reference library
    fastq_dir (str): directory with fastq files
    df_motif5 (dataframe | str): 5' motif dataframe (or file path)
    df_motif3 (dataframe | str): 3' motif dataframe (or file path)
    out_dir (str): directory for output files
    match_score (int, optional): match score for pairwise alignment (Default: 1)
    mismatch_score (int, optional): mismatch score for pairwise alignment (Default: -4)
    align_max (int, optional): max alignments per fastq file to save compute (Default: None)
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, Bio.PairwiseAligner, & plot_alignments()

    #### UPDATE: KEEP TRACK OF READ indices for future comparison #### ALSO DO FOR COUNT_ALIGNMENTS ####
    #### MAKE MORE MEMORY EFFICIENT AND SAVE COMPUTE ####
    #### ADD SAVE count_alignment_stats below ####
    #### Develop reporting standard ####
    '''
    # Intialize the aligner
    aligner = PairwiseAligner()
    aligner.mode = 'global'  # Use 'local' for local alignment
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score/2  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = mismatch_score/2  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = mismatch_score/2  # Penalty for extending a gap; applied to both strands

    # Get dataframes from file path if needed
    if type(df_ref)==str: 
        df_ref = io.get(df_ref)
    if type(df_motif5)==str: 
        df_ref = io.get(df_motif5)
    if type(df_motif3)==str: 
        df_ref = io.get(df_motif3)

    # Check dataframe for alignment and id columns
    if align_col not in df_ref.columns.tolist():
        raise Exception(f'Missing alignment column: {align_col}') 
    if id_col not in df_ref.columns.tolist():
        raise Exception(f'Missing id column: {id_col}')
    df_ref[align_col] = df_ref[align_col].str.upper() 

    # Check for start_i and end_i columns
    if 'end_i' not in df_motif5.columns.tolist():
        raise Exception('Missing column in df_motif5: end_i') 
    if 'start_i' not in df_motif3.columns.tolist():
        raise Exception('Missing column in df_motif3: start_i') 

    # Make fastqs dictionary
    fastqs = dict()
    count_region_stats = pd.DataFrame()
    for fastq_file in os.listdir(fastq_dir): # Iterate through fastq files
        
        # Get reads
        reads = 0 # Keep track of # of reads and missing regions
        missing5 = 0
        missing3 = 0
        seqs = [] # Store region sequenes from reads with motifs
        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            with gzip.open(os.path.join(fastq_dir,fastq_file), "rt") as handle:
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    reads += 1

                    # Obtain motif boundaries that define region
                    start_i = df_motif5.iloc[i]["end_i"]+1
                    end_i = df_motif5.iloc[i]["start_i"]
                    if (start_i!=0) | (end_i!=-1): seqs.append(record[start_i:end_i]) # Both motifs are present
                    elif (start_i==0) & (end_i==-1): # both motifs are missing
                        missing5 += 1
                        missing3 += 1
                    elif start_i==0: missing5 += 1 # 5' motif is missing
                    else: missing3 += 1 # 3' motif is missing
        
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            with open(os.path.join(fastq_dir,fastq_file), "r") as handle:    
                for i,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads & isolate region between motifs
                    reads += 1

                    # Obtain motif boundaries that define region
                    start_i = df_motif5.iloc[i]["end_i"]+1
                    end_i = df_motif5.iloc[i]["start_i"]
                    if (start_i!=0) | (end_i!=-1): seqs.append(record[start_i:end_i]) # Both motifs are present
                    elif (start_i==0) & (end_i==-1): # both motifs are missing
                        missing5 += 1
                        missing3 += 1
                    elif start_i==0: missing5 += 1 # 5' motif is missing
                    else: missing3 += 1 # 3' motif is missing
        
        else: # Not a fastq file
            continue
        
        # Update stats file with # of reads with(out) the region
        print(f'{fastq_name}:\t{reads} reads\t=>\t{len(seqs)} reads;\tmissing {missing5} motif5;\tmissing {missing3} motif3')
        count_region_stats = pd.concat([count_region_stats,
                                        pd.DataFrame({'file':[fastq_name],
                                                      'reads':[reads],
                                                      'reads_w_region':[len(seqs)],
                                                      'reads_wo_motif5':[missing5],
                                                      'reads_wo_motif3':[missing3]})])
        
        # Perform alignments
        print('Perform alignments')
        dc_alignments = {ref:0 for ref in df_ref[align_col]}
        dc_alignments_mismatch_num = {ref:0 for ref in df_ref[align_col]}
        dc_alignments_mismatch_pos = {ref:[] for ref in df_ref[align_col]}
        s=0
        for seq in seqs: # Iterate though sequences
            s+=1
            if align_max is not None:
                if s>align_max: break
            print(f'{s} out of {len(seqs)}')
            seq_alignments_scores = []
            seq_alignments_aligned = []
            for ref in df_ref[align_col]: # Iterate though reference sequences
                seq_alignment = aligner.align(ref, seq[0:len(ref)]) # trim ngs sequence to reference sequence & align
                seq_alignments_scores.append(seq_alignment[0].score) # Save highest alignment score
                seq_alignments_aligned.append(seq_alignment[0].aligned[0]) # Save alignment matches

            # Isolate maximum score alignment
            i = seq_alignments_scores.index(max(seq_alignments_scores))
            ref_i = df_ref.iloc[i][align_col]
            aligned_i = seq_alignments_aligned[i]
            dc_alignments[df_ref.iloc[i][align_col]] = dc_alignments[ref_i]+1

            # Find & quantify mismatches (Change zero-indexed to one-indexed)
            mismatch_pos = []
            if len(aligned_i) == 1: 
                (a1,b1) = aligned_i[0]
                if (a1==0)&(b1==len(ref_i)-1): mismatch_pos.extend([])
                elif a1==0: mismatch_pos.extend([k+1 for k in range(b1+1,len(ref_i))])
                elif b1==len(ref_i)-1: mismatch_pos.extend([k+1 for k in range(0,a1-1)])
                else: mismatch_pos.extend([j+1 for j in range(0,a1-1)] + [k+1 for k in range(b1+1,len(ref_i))])
            else:
                for j in range(len(aligned_i)-1):
                    (a1,b1) = aligned_i[j]
                    (a2,b2) = aligned_i[j+1]
                    if (j==0)&(a1!=0): mismatch_pos.extend([k+1 for k in range(0,a1-1)])
                    if (j==len(aligned_i)-2)&(b2!=len(ref_i)-1): mismatch_pos.extend([k+1 for k in range(b2+1,len(ref_i))])
                    mismatch_pos.extend([k+1 for k in range(b1+1,a2-1)])
            dc_alignments_mismatch_num[ref_i] = dc_alignments_mismatch_num[ref_i] + len(mismatch_pos)
            dc_alignments_mismatch_pos[ref_i] = dc_alignments_mismatch_pos[ref_i] + mismatch_pos

        # Calculate mismatch position fraction of alignments
        dc_alignments_mismatch_pos_fraction = dict()
        for (ref,mismatch_pos) in dc_alignments_mismatch_pos.items():
            dc_alignments_mismatch_pos_fraction[ref] = {pos:mismatch_pos.count(pos) for pos in range(1,len(ref)+1)}

        # Merge alignment dictionaries into a fastq dataframe
        print('Merge alignment dictionaries into a fastq dataframe')
        df_alignments = pd.DataFrame(dc_alignments.items(),columns=[align_col,'alignments'])
        df_alignments_mismatch_num = pd.DataFrame(dc_alignments_mismatch_num.items(),columns=[align_col,'mismatch_num'])
        df_alignments_mismatch_pos = pd.DataFrame(dc_alignments_mismatch_pos.items(),columns=[align_col,'mismatch_pos']) 
        df_fastq = pd.merge(left=df_ref,right=df_alignments,on=align_col)
        df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_num,on=align_col)
        df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_pos,on=align_col)
        
        # Calculate mismatch num & position per alignment
        print('Calculate mismatch num & position per alignment')
        mismatch_num_per_alignment_ls = []
        mismatch_pos_per_alignment_ls = []
        for (ref,mismatch_pos,mismatch_num,alignments) in t.zip_cols(df=df_fastq,cols=[align_col,'mismatch_pos','mismatch_num','alignments']):
            if alignments==0:
                mismatch_num_per_alignment_ls.append(0)
                mismatch_pos_per_alignment_ls.append({pos:0 for pos in range(1,len(ref)+1)})
            else:
                mismatch_num_per_alignment_ls.append(mismatch_num/alignments)
                mismatch_pos_per_alignment_ls.append({pos:mismatch_pos.count(pos)/alignments for pos in range(1,len(ref)+1)})
        df_fastq['mismatch_num_per_alignment'] = mismatch_num_per_alignment_ls
        df_fastq['mismatch_pos_per_alignment'] = mismatch_pos_per_alignment_ls
        
        # Save & append fastq dataframe to fastq dictionary
        print('Save & append fastq dataframe to fastq dictionary')
        io.save(dir=out_dir,file=f'alignment_{fastq_name}.csv',obj=df_fastq)
        fastqs[fastq_name]=df_fastq

        # Plot mismatch position per alignment
        plot_alignments(fastq_alignments={fastq_name:df_fastq}, align_col=align_col, id_col=id_col,
                        out_dir=out_dir, plot_suf=plot_suf, show=show, **plot_kwargs)
    
    # Save and return
    io.save(dir=out_dir,file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_count_region_stats.csv',obj=count_region_stats)
    return fastqs

def count_alignments(df_ref: pd.DataFrame | str, align_col: str, id_col: str, fastq_dir: str, 
                     out_dir: str, match_score:int=1, mismatch_score:int=-4,
                     align_max:int=None, plot_suf:str='.pdf', show:bool=False, **plot_kwargs):
    ''' 
    count_alignments(): align reads from fastq directory to annotated library with mismatches; plot and return fastq alignments dictionary
    
    Parameters:
    df_ref (dataframe | str): annotated reference library dataframe (or file path)
    align_col (str): align column name in the annotated reference library
    id_col (str): id column name in the annotated reference library
    fastq_dir (str): directory with fastq files
    out_dir (str): directory for output files
    match_score (int, optional): match score for pairwise alignment (Default: 1)
    mismatch_score (int, optional): mismatch score for pairwise alignment (Default: -4)
    align_max (int, optional): max alignments per fastq file to save compute (Default: None)
    plot_suf (str, optional): plot type suffix with '.' (Default: '.pdf')
    show (bool, optional): show plots (Default: False)
    **plot_kwargs (optional): plot key word arguments

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, Bio.PairwiseAligner, & plot_alignments()
    '''
    # Intialize the aligner
    aligner = PairwiseAligner()
    aligner.mode = 'global'  # Use 'local' for local alignment
    aligner.match_score = match_score  # Score for a match
    aligner.mismatch_score = mismatch_score/2  # Penalty for a mismatch; applied to both strands
    aligner.open_gap_score = mismatch_score/2  # Penalty for opening a gap; applied to both strands
    aligner.extend_gap_score = mismatch_score/2  # Penalty for extending a gap; applied to both strands

    # Get dataframe from file path if needed
    if type(df_ref)==str: 
        df_ref = io.get(df_ref)
    
    # Check dataframe for alignment and id columns
    if align_col not in df_ref.columns.tolist():
        raise Exception(f'Missing alignment column: {align_col}') 
    if id_col not in df_ref.columns.tolist():
        raise Exception(f'Missing id column: {id_col}')
    df_ref[align_col] = df_ref[align_col].str.upper() 
    
    # Make fastqs dictionary
    fastqs = dict()
    for fastq_file in os.listdir(fastq_dir): # Iterate through fastq files
        
        # Get reads
        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            with gzip.open(os.path.join(fastq_dir,fastq_file), "rt") as handle:
                seqs=[record.seq for record in SeqIO.parse(handle, "fastq")] # Parse reads
                fastq_name = fastq_file[:-len(".fastq.gz")]
                print(f'{fastq_name}:\t{len(seqs)} reads')
        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            with open(os.path.join(fastq_dir,fastq_file), "r") as handle:    
                seqs=[record.seq for record in SeqIO.parse(handle, "fastq")] # Parse reads
                fastq_name = fastq_file[:-len(".fastq")]
                print(f'{fastq_name}:\t{len(seqs)} reads')
        else: # Not a fastq file
            continue
        
        # Perform alignments
        print('Perform alignments')
        dc_alignments = {ref:0 for ref in df_ref[align_col]}
        dc_alignments_mismatch_num = {ref:0 for ref in df_ref[align_col]}
        dc_alignments_mismatch_pos = {ref:[] for ref in df_ref[align_col]}
        s=0
        for seq in seqs: # Iterate though sequences
            s+=1
            if align_max is not None:
                if s>align_max: break
            print(f'{s} out of {len(seqs)}')
            seq_alignments_scores = []
            seq_alignments_aligned = []
            for ref in df_ref[align_col]: # Iterate though reference sequences
                seq_alignment = aligner.align(ref, seq[0:len(ref)]) # trim ngs sequence to reference sequence & align
                seq_alignments_scores.append(seq_alignment[0].score) # Save highest alignment score
                seq_alignments_aligned.append(seq_alignment[0].aligned[0]) # Save alignment matches

            # Isolate maximum score alignment
            i = seq_alignments_scores.index(max(seq_alignments_scores))
            ref_i = df_ref.iloc[i][align_col]
            aligned_i = seq_alignments_aligned[i]
            dc_alignments[df_ref.iloc[i][align_col]] = dc_alignments[ref_i]+1

            # Find & quantify mismatches (Change zero-indexed to one-indexed)
            mismatch_pos = []
            if len(aligned_i) == 1: 
                (a1,b1) = aligned_i[0]
                if (a1==0)&(b1==len(ref_i)-1): mismatch_pos.extend([])
                elif a1==0: mismatch_pos.extend([k+1 for k in range(b1+1,len(ref_i))])
                elif b1==len(ref_i)-1: mismatch_pos.extend([k+1 for k in range(0,a1-1)])
                else: mismatch_pos.extend([j+1 for j in range(0,a1-1)] + [k+1 for k in range(b1+1,len(ref_i))])
            else:
                for j in range(len(aligned_i)-1):
                    (a1,b1) = aligned_i[j]
                    (a2,b2) = aligned_i[j+1]
                    if (j==0)&(a1!=0): mismatch_pos.extend([k+1 for k in range(0,a1-1)])
                    if (j==len(aligned_i)-2)&(b2!=len(ref_i)-1): mismatch_pos.extend([k+1 for k in range(b2+1,len(ref_i))])
                    mismatch_pos.extend([k+1 for k in range(b1+1,a2-1)])
            dc_alignments_mismatch_num[ref_i] = dc_alignments_mismatch_num[ref_i] + len(mismatch_pos)
            dc_alignments_mismatch_pos[ref_i] = dc_alignments_mismatch_pos[ref_i] + mismatch_pos

        # Calculate mismatch position fraction of alignments
        dc_alignments_mismatch_pos_fraction = dict()
        for (ref,mismatch_pos) in dc_alignments_mismatch_pos.items():
            dc_alignments_mismatch_pos_fraction[ref] = {pos:mismatch_pos.count(pos) for pos in range(1,len(ref)+1)}

        # Merge alignment dictionaries into a fastq dataframe
        print('Merge alignment dictionaries into a fastq dataframe')
        df_alignments = pd.DataFrame(dc_alignments.items(),columns=[align_col,'alignments'])
        df_alignments_mismatch_num = pd.DataFrame(dc_alignments_mismatch_num.items(),columns=[align_col,'mismatch_num'])
        df_alignments_mismatch_pos = pd.DataFrame(dc_alignments_mismatch_pos.items(),columns=[align_col,'mismatch_pos']) 
        df_fastq = pd.merge(left=df_ref,right=df_alignments,on=align_col)
        df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_num,on=align_col)
        df_fastq = pd.merge(left=df_fastq,right=df_alignments_mismatch_pos,on=align_col)
        
        # Calculate mismatch num & position per alignment
        print('Calculate mismatch num & position per alignment')
        mismatch_num_per_alignment_ls = []
        mismatch_pos_per_alignment_ls = []
        for (ref,mismatch_pos,mismatch_num,alignments) in t.zip_cols(df=df_fastq,cols=[align_col,'mismatch_pos','mismatch_num','alignments']):
            if alignments==0:
                mismatch_num_per_alignment_ls.append(0)
                mismatch_pos_per_alignment_ls.append({pos:0 for pos in range(1,len(ref)+1)})
            else:
                mismatch_num_per_alignment_ls.append(mismatch_num/alignments)
                mismatch_pos_per_alignment_ls.append({pos:mismatch_pos.count(pos)/alignments for pos in range(1,len(ref)+1)})
        df_fastq['mismatch_num_per_alignment'] = mismatch_num_per_alignment_ls
        df_fastq['mismatch_pos_per_alignment'] = mismatch_pos_per_alignment_ls
        
        # Save & append fastq dataframe to fastq dictionary
        print('Save & append fastq dataframe to fastq dictionary')
        io.save(dir=out_dir,file=f'alignment_{fastq_name}.csv',obj=df_fastq)
        fastqs[fastq_name]=df_fastq

        # Plot mismatch position per alignment
        plot_alignments(fastq_alignments={fastq_name:df_fastq}, align_col=align_col, id_col=id_col,
                        out_dir=out_dir, plot_suf=plot_suf, show=show, **plot_kwargs)
    
    return fastqs

# Quantify edit outcomes
def trim_filter(record,qall:int,qavg:int,qtrim:int,qmask:int,alls:int,avgs:int,trims:int,masks:int):
    '''
    trim_filter(): trim and filter fastq sequence based on quality scores
    
    Parameters:
    record: Bio.SeqIO fastq record
    qall (int): phred quality score threshold for all bases for a read to not be discarded
    qtrim (int): phred quality score threshold for trimming reads on both ends
    qavg (int): average phred quality score threshold for a read to not be discarded
    qmask (int): phred quality score threshold for base to not be masked to N
    alls (int): count of records that were dropped due to qall threshold
    avgs (int): count of records that were dropped due to qavg threshold
    trims (int): count of records that were trimmed due to qtrim threshold
    masks (int): count of records that had bases masked due to qmask threshold
    
    Dependencies: Bio.SeqIO, gzip, os, pandas, & Bio.Seq.Seq
    '''
    if all(score >= qall for score in record.letter_annotations['phred_quality']): # All threshold
        if np.mean(record.letter_annotations['phred_quality']) >= qavg: # Avg threshold
            
            quality_scores = record.letter_annotations['phred_quality'] # Set 5' & 3' trim indexes to the start and end
            trim_5 = 0 
            trim_3 = len(quality_scores)
            sequence = record.seq
            
            if qtrim!=0: # Save compute time if trim is not desired
                for i in range(len(quality_scores)): # Find 5' trim
                    if quality_scores[i] >= qtrim: break
                    trim_5 = i
                for i in reversed(range(len(quality_scores))): # Find 3' trim
                    if quality_scores[i] >= qtrim: break
                    trim_3 = i
                if (trim_5!=0)|(trim_3!=len(quality_scores)): trims += 1 # Trimmed read

            sequence = sequence[trim_5:trim_3] # Trim the sequence and quality scores
            quality_scores = quality_scores[trim_5:trim_3]

            
            bases = list(sequence) # Mask bases with 'N' threshold
            if masks !=0: # Save compute time if mask is not desired
                for i, qual in enumerate(quality_scores):
                    if qual < qmask: bases[i] = 'N'
            sequenceN = Seq('').join(bases) # Update the sequence with the modified version
            if Seq('N') in sequenceN: masks += 1

            return record.id,sequence,sequenceN,quality_scores,alls,avgs,trims,masks
    
        else: return None,None,None,None,alls,avgs+1,trims,masks # Avg threshold not met
    else: return None,None,None,None,alls+1,avgs,trims,masks # All threshold not met

def get_fastqs(dir: str,qall:int=10,qavg:int=30,qtrim:int=0,qmask:int=0,save:bool=True):
    ''' 
    get_fastqs(): get fastq files from directory and store records in dataframes in a dictionary
    
    Parameters:
    dir (str): directory with fastq files
    qall (int, optional): phred quality score threshold for all bases for a read to not be discarded (Q = -log(err))
    qtrim (int, optional): phred quality score threshold for trimming reads on both ends (Q = -log(err))
    qavg (int, optional): average phred quality score threshold for a read to not be discarded (Q = -log(err))
    qmask (int, optional): phred quality score threshold for base to not be masked to N (Q = -log(err))
    save (bool, optional): save reads statistics file to local directory (Default: True)

    Dependencies: Bio.SeqIO, gzip, os, pandas, Bio.Seq.Seq, & trim_filter()
    '''
    # Make fastqs dictionary
    fastqs = dict()
    if save == True: out = pd.DataFrame()
    for fastq_file in os.listdir(dir): # Iterate through fastq files
        reads = 0 
        alls = 0 # Keep track of reads & outcomes
        avgs = 0
        trims = 0
        masks = 0
        ids=[]
        seqs=[]
        seqsN=[]
        phred_scores=[]

        if fastq_file.endswith(".fastq.gz"): # Compressed fastq
            fastq_name = fastq_file[:-len(".fastq.gz")] # Get fastq name
            with gzip.open(os.path.join(dir,fastq_file), "rt") as handle:

                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    reads=r+1
                    record_id,record_seq,record_seqN,record_scores,alls,avgs,trims,masks = trim_filter(record,qall,qavg,qtrim,qmask,alls,avgs,trims,masks) # Trim & filter
                    if record_id is not None: # Save id, sequence, & quality scores
                        ids.append(record_id) 
                        seqs.append(record_seq)
                        seqsN.append(record_seqN)
                        phred_scores.append(record_scores)

        elif fastq_file.endswith(".fastq"): # Uncompressed fastq
            fastq_name = fastq_file[:-len(".fastq")] # Get fastq name
            with open(os.path.join(dir,fastq_file), "r") as handle:

                for r,record in enumerate(SeqIO.parse(handle, "fastq")): # Parse reads
                    reads=r+1
                    record_id,record_seq,record_seqN,record_scores,alls,avgs,trims,masks = trim_filter(record,qall,qavg,qtrim,qmask,alls,avgs,trims,masks) # Trim & filter
                    if record_id is not None: # Save id, sequence, sequence masked, & quality scores
                        ids.append(record_id) 
                        seqs.append(record_seq)
                        seqsN.append(record_seqN)
                        phred_scores.append(record_scores)
        
        else: # Not a fastq file
            continue

        fastqs[fastq_name]=pd.DataFrame({'id':ids, # Add dataframe to dictionary 
                                         'seq':seqs,
                                         'seqN':seqsN,
                                         'phred_scores':phred_scores})
        print(f'{fastq_name}:\t{reads} reads\t=>\t{len(fastqs[fastq_name])} reads (alls = {alls} & avgs = {avgs});\t{trims} trimmed reads;\t{masks} masked reads')
        if save==True: out = pd.concat([out,
                                        pd.DataFrame({'file': [fastq_name],
                                                      'reads': [reads],
                                                      'reads_filtered': [len(fastqs[fastq_name])],
                                                      'reads_dropped_all': [alls],
                                                      'reads_dropped_avg': [avgs],
                                                      'reads_trimmed': [trims],
                                                      'reads_masked': [masks]})])
    
    if save==True: io.save(dir='.',file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_get_fastqs.csv',obj=out)
    return fastqs

def region(fastqs: dict, flank5: str, flank3: str, save: bool=True, masks: bool=False):
    ''' 
    region(): gets DNA and AA sequence for records within flanks
    
    Parameters:
    fastqs (dict): dictionary from get_fastqs
    flank5 (str): top strand flanking sequence 5'
    flank3 (str): top strand flanking sequence 3'
    save (bool, optional): save reads statistics file to local directory (Default: True)
    masks (bool, optional): include masked sequence and translation (Default: False)
    
    Dependencies: pandas & Bio.Seq.Seq
    '''
    # Check flank lengths
    if (len(flank5)<9)|(len(flank3)<9): print('Warning: flank5 or flank3 less than 9.')

    # Remove fastq records that do not have flanks
    fastqs_1=dict()
    missing_flank5s = []
    missing_flank3s = []
    for file,fastq in fastqs.items():
        missing_flank5 = set()
        missing_flank3 = set()
        for i,seq in enumerate(fastq['seq']):
            if (seq.find(flank5)==-1):
                missing_flank5.add(i)
            if (seq.find(flank3)==-1): 
                missing_flank3.add(i)

        fastqs_1[file] = fastq.drop(sorted(missing_flank5.union(missing_flank3))).reset_index(drop=True)
        missing_flank5s.append(len(missing_flank5))
        missing_flank3s.append(len(missing_flank3))
     
    # Obtain nucleotide and AA sequences within flanks; remove fastq records with phred scores within flanks
    if save == True: out = pd.DataFrame()
    for j,(file,fastq) in enumerate(fastqs_1.items()):
        nuc=[]
        prot=[]
        if masks==True:
            nucN=[]
            protN=[]
        
        for i,seq in enumerate(fastq['seq']):
            nuc.append(seq[seq.find(flank5)+len(flank5):seq.find(flank3)])
            prot.append(Seq.translate(seq[seq.find(flank5)+len(flank5):seq.find(flank3)]))
            if masks==True:
                nucN.append(fastq.iloc[i]['seqN'][seq.find(flank5)+len(flank5):seq.find(flank3)])
                protN.append(Seq.translate(fastq.iloc[i]['seqN'][seq.find(flank5)+len(flank5):seq.find(flank3)]))
        
        fastqs_1[file]['nuc']=nuc
        fastqs_1[file]['prot']=prot
        if masks==True:
            fastqs_1[file]['nucN']=nuc
            fastqs_1[file]['protN']=protN
        
        print(f'{file}:\t{len(fastqs[file])} reads\t=>\t{len(fastqs_1[file])} reads;\tmissing {missing_flank5s[j]} flank5;\tmissing {missing_flank3s[j]} flank3')
        if save==True: out = pd.concat([out,
                                        pd.DataFrame({'file': [file],
                                                      'reads_filtered': [len(fastqs[file])],
                                                      'reads_w_flanks': [len(fastqs_1[file])],
                                                      'reads_wo_flank5': [missing_flank5s[j]],
                                                      'reads_wo_flank3': [missing_flank3s[j]]})
                                        ])
    
    if save==True: io.save(dir='.',file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_region.csv',obj=out)
    
    return fastqs_1

def genotype(fastqs: dict, res: int, wt: str, save: bool=True, masks: bool=False, keepX: bool=False):
    ''' 
    genotype(): assign genotypes to sequence records
    
    Parameters:
    fastqs (dict): dictionary from filter_fastqs
    res (int): first AA number
    wt (str, optional 2): expected wildtype nucleotide sequence (in frame AA; required unless pt is provided)
    save (bool, optional): save genotyped reads to local directory (Default: True)
    masks (bool, optional): include masked sequence and translation (Default: False)
    keepX (bool, optional): keep unknown translation (i.e., X) due to sequencing error (Default: False) 
    
    Dependencies: pandas & Bio.Seq.Seq

    Note: Need to add single indels eventually
    '''
    # Iterate through fastq files
    for file,fastq in fastqs.items():
        edit=[]
        if masks==True: editN=[]
        for i in range(len(fastq['prot'])):
            if len(wt)!=len(fastq.iloc[i]['nuc']): # Add single indels here
                edit.append('Indel')
                if masks==True: editN.append('Indel')
            elif Seq.translate(Seq(wt))==fastq.iloc[i]['prot']: 
                edit.append('WT')
                if masks==True: editN.append('WT')
            else:
                e = []
                if masks==True: eN = []
                
                for j, (a, b) in enumerate(zip(Seq.translate(Seq(wt)), fastq.iloc[i]['prot'])): # Find edits from sequence
                    if a != b: e.append(a+str(j+res)+b)
                if len(e)>1: edit.append(", ".join(e))
                elif len(e)==1: edit.append(e[0])
                else: edit.append('Unknown Edit')

                if masks==True:    
                    for j, (a, b) in enumerate(zip(Seq.translate(Seq(wt)), fastq.iloc[i]['protN'])): # Find edits from masked sequence
                        if (a != b)&(str(b)!='X')&(keepX==False): eN.append(a+str(j+res)+b)
                        elif (a != b)&(keepX==True): eN.append(a+str(j+res)+b)
                    if len(eN)>1: editN.append(", ".join(eN))
                    elif len(eN)==1: editN.append(eN[0])
                    else: editN.append('Masked Edit')
        
        fastqs[file]['edit']=edit
        if masks==True: fastqs[file]['editN']=editN
        print(f'{file}:\t{len(fastqs[file])} reads')
    
    if save==True: io.save(dir='.',file=f'{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}_genotype.csv',obj=t.reorder_cols(df=t.join(dc=fastqs,col='fastq_file'),cols=['fastq_file']))
    
    return fastqs

def outcomes(fastqs: dict, edit: str='edit'):
    ''' 
    outcomes(): returns edit count & fraction per sample (tidy format)

    Parameters:
    fastqs (dict): dictionary from genotype
    edit (str, optional): edit column name (Default: edit)
    
    Dependencies: pandas
    '''
    df = pd.DataFrame()
    for file,fastq in fastqs.items():
        temp=pd.DataFrame({'sample':[file]*len(fastq[edit].value_counts()),
                           edit:list(fastq[edit].value_counts().keys()),
                           'count':fastq[edit].value_counts(),
                           'fraction':fastq[edit].value_counts()/len(fastq[edit])})
        df=pd.concat([df,temp]).reset_index(drop=True)
    return df

def outcomes_desired(df: pd.DataFrame, desired_edits: list | str, sample_col: str='sample',
                     edit_col: str='edit', count_col: str='count',fraction_col: str='fraction'):
    ''' 
    outcomes_desired(): groups desired edit count & fraction per sample (tidy format)

    Parameters:
    df (DataFrame): dataframe with edit count & fraction per sample (tidy format)
    desired_edits (list or str): list of desired edits (list of str) or desired edits column name (str)
    sample_col (str, optional): sample column name (Default: sample)
    edit_col (str, optional): edit column name (Default: edit)
    count_col (str, optional): count column name (Default: count)
    fraction_col (str, optional): fraction column name (Default: fraction)

    Dependencies: pandas
    '''
    if isinstance(desired_edits, list): desired_edits_col = None # Determine if desired edits is a list or str
    elif isinstance(desired_edits, str): desired_edits_col = desired_edits
    else: TypeError(f'desired_edits = {desired_edits} was not a list or str.')

    df_desired = pd.DataFrame()
    for sample in df[sample_col].value_counts().keys(): # Iterate through samples
        df_sample = df[df[sample_col]==sample].reset_index(drop=True)

        if desired_edits_col: 
            desired_edits = df_sample.iloc[0][desired_edits_col] # Get desired edits list for each sample if the column name was provided
            if isinstance(desired_edits, str): desired_edits = [desired_edits]
        
        i_desired = [] # Store desired edit & corresponding counts & fractions
        count_desired = []
        fraction_desired = []

        for i,(edit,count,fraction) in enumerate(t.zip_cols(df=df_sample,cols=[edit_col,count_col,fraction_col])):
            
            if ', ' in edit: # Search for desired edit within multiple edit outcomes
                edits = edit.split(', ')
                for edit in edits:
                    if edit in desired_edits:
                        i_desired.append(i)
                        count_desired.append(count)
                        fraction_desired.append(fraction)
                        break

            else: # Search for desired within single edit outcome
                if edit in desired_edits:
                    i_desired.append(i)
                    count_desired.append(count)
                    fraction_desired.append(fraction)

        df_sample = df_sample.drop(index=i_desired) # Remove desired edits & combine into 'Desired' edit
        other_cols = [col for col in df_sample.columns if col not in [edit_col,count_col,fraction_col]]
        df_sample_desired = df_sample.iloc[0][other_cols].to_frame().T.reset_index(drop=True)
        df_sample_desired[edit_col] = ['Desired']
        df_sample_desired[count_col] = [sum(count_desired)]
        df_sample_desired[fraction_col] = [sum(fraction_desired)]
        df_sample = pd.concat(objs=[df_sample,df_sample_desired]).reset_index(drop=True)
        df_desired = pd.concat(objs=[df_desired,df_sample]).reset_index(drop=True)

    return df_desired

def genotyping(in_dir: str, sequence: str, res: int, desired_edits: list = None,
               out_dir: str=None, out_file: str=None, **kwargs):
    ''' 
    genotying(): quantify edit outcomes workflow
    
    Parameters:
    in_dir (str): directory with fastq files
    sequence (str): sequence formatted flank5(genotype region)flank3
    res (int): first AA number in genotype region
    desired_edits (list, optional): list of desired edits (Default: None)
    out_dir (str, optional): output directory (Default: None)
    out_file (str, optional): output file (Default: None)

    **kwargs:
    qall (int, optional): phred quality score threshold for all bases for a read to not be discarded (Q = -log(err))
    qtrim (int, optional): phred quality score threshold for trimming reads on both ends (Q = -log(err))
    qavg (int, optional): average phred quality score threshold for a read to not be discarded (Q = -log(err))
    qmask (int, optional): phred quality score threshold for base to not be masked to N (Q = -log(err))
    save (bool, optional): save reads statistics and genotype file to local directory (Default: True)
    masks (bool, optional): include masked sequence and translation (Default: False)
    keepX (bool, optional): keep unknown translation (i.e., X) due to sequencing error (Default: False) 
    
    Dependencies: get_fastq(), region(), genotype(), outcomes()
    '''
    # Check sequence and obtain flank5(genotype region)flank3
    if '(' not in sequence or ')' not in sequence:
        raise(ValueError(f'Missing "(" or ")" in sequence:\n{sequence}'))
    flank5 = sequence.split('(')[0]
    wt = sequence.split('(')[1].split(')')[0]
    flank3 = sequence.split(')')[1]

    # Split **kwargs
    get_fastqs_kw = ['qall','qtrim','qavg','qmask','save'] # get_fastq()
    region_kw = ['save','masks'] # region()
    genotype_kw = ['save','masks','keepX'] # genotype()
    outcomes_kw = ['edit'] # outcomes()

    get_fastqs_kwargs = {k:kwargs[k] for k in get_fastqs_kw if k in kwargs}
    region_kwargs = {k:kwargs(k) for k in region_kw if k in kwargs}
    genotype_kwargs = {k:kwargs(k) for k in genotype_kw if k in kwargs}
    outcomes_kwargs = {k:kwargs(k) for k in outcomes_kw if k in kwargs}
    
    # Quantify edit outcomes workflow
    dc = get_fastqs(get_fastqs_kwargs,dir=in_dir)
    dc = region(region_kwargs,fastqs=dc,flank5=flank5,flank3=flank3)
    dc = genotype(genotype_kwargs,fastqs=dc,res=res,wt=wt)
    df = outcomes(outcomes_kwargs,fastqs=dc)
    if desired_edits is not None: 
        df_desired = outcomes_desired(outcomes_kwargs,df=df,desired_edits=desired_edits)

    # Save and return edit outcomes dataframe
    if out_dir is not None and out_file is not None: # Save dataframe (optional)
        io.save(dir=out_dir,file=out_file,obj=df)
        if desired_edits is not None: 
            io.save(dir=out_dir,file=f"{'.'.join(out_file.split('.')[:-1])}_desired.{out_file.split('.')[-1]}",obj=df_desired)
    return df


# Supporting methods for DMS plots
''' aa_props: dictionary of AA properties with citations (Generated by ChatGPT)
    
    Sources:
    Hydrophobicity: https://resources.qiagenbioinformatics.com/manuals/clcgenomicsworkbench/650/Hydrophobicity_scales.html
    Weight: from Bio.Data import IUPACData (protein_weights)
    Polarity: https://web.expasy.org/protscale/pscale/PolarityGrantham.html
'''
aa_props = {
    'E': {'name': 'Glutamic acid', 'hydrophobicity': -3.5, 'weight': 147.1, 'polarity': 12.3, 'charge': 'negative'},
    'D': {'name': 'Aspartic acid', 'hydrophobicity': -3.5, 'weight': 133.1, 'polarity': 13.0, 'charge': 'negative'},
    'R': {'name': 'Arginine', 'hydrophobicity': -4.5, 'weight': 174.2, 'polarity': 10.5, 'charge': 'positive'},
    'H': {'name': 'Histidine', 'hydrophobicity': -3.2, 'weight': 155.2, 'polarity': 10.4, 'charge': 'positive'},
    'K': {'name': 'Lysine', 'hydrophobicity': -3.9, 'weight': 146.2, 'polarity': 11.3, 'charge': 'positive'},
    'F': {'name': 'Phenylalanine', 'hydrophobicity': 2.8, 'weight': 165.2, 'polarity': 5.2, 'charge': 'neutral'},
    'Y': {'name': 'Tyrosine', 'hydrophobicity': -1.3, 'weight': 181.2, 'polarity': 6.2, 'charge': 'neutral'},
    'W': {'name': 'Tryptophan', 'hydrophobicity': -0.9, 'weight': 204.2, 'polarity': 5.4, 'charge': 'neutral'},
    'S': {'name': 'Serine', 'hydrophobicity': -0.8, 'weight': 105.1, 'polarity': 9.2, 'charge': 'neutral'},
    'Q': {'name': 'Glutamine', 'hydrophobicity': -3.5, 'weight': 146.2, 'polarity': 10.5, 'charge': 'neutral'},
    'T': {'name': 'Threonine', 'hydrophobicity': -0.7, 'weight': 119.1, 'polarity': 8.6, 'charge': 'neutral'},
    'N': {'name': 'Asparagine', 'hydrophobicity': -3.5, 'weight': 132.1, 'polarity': 11.6, 'charge': 'neutral'},
    'C': {'name': 'Cysteine', 'hydrophobicity': 2.5, 'weight': 121.2, 'polarity': 5.5, 'charge': 'neutral'},
    'P': {'name': 'Proline', 'hydrophobicity': -1.6, 'weight': 115.1, 'polarity': 8.0, 'charge': 'neutral'},
    'A': {'name': 'Alanine', 'hydrophobicity': 1.8, 'weight': 89.1, 'polarity': 8.1, 'charge': 'neutral'},
    'G': {'name': 'Glycine', 'hydrophobicity': -0.4, 'weight': 75.1, 'polarity': 9.0, 'charge': 'neutral'},
    'M': {'name': 'Methionine', 'hydrophobicity': 1.9, 'weight': 149.2, 'polarity': 5.7, 'charge': 'neutral'},
    'V': {'name': 'Valine', 'hydrophobicity': 4.2, 'weight': 117.1, 'polarity': 5.9, 'charge': 'neutral'},
    'I': {'name': 'Isoleucine', 'hydrophobicity': 4.5, 'weight': 131.2, 'polarity': 5.2, 'charge': 'neutral'},
    'L': {'name': 'Leucine', 'hydrophobicity': 3.8, 'weight': 131.2, 'polarity': 4.9, 'charge': 'neutral'},
    '*': {'name': 'Stop', 'hydrophobicity': None, 'weight': None, 'polarity': None, 'charge': None}
}

def edit_1(df: pd.DataFrame,col='edit'):
    ''' 
    edit_1(): split edit column to before, after, and amino acid number
    
    Parameters:
    df (dataframe): fastq outcomes dataframe
    col (str, optional): edit column name
    
    Dependencies: pandas
    '''
    df_1 = df[(df[col].str.contains(',')==False)&(df[col]!='WT')&(df[col]!='Indel')] # Isolate single AA changes
    df_1['before']=df_1[col].str[0] # Split edit information
    df_1['after']=df_1[col].str[-1]
    df_1['number']=df_1[col].str[1:-1].astype(int)
    return df_1.reset_index(drop=True)

def dms_cond(df: pd.DataFrame, cond: str, wt:str, res: int, sample='sample', edit='edit', psuedocount=0):
    ''' 
    dms_cond(): returns DMS grid data in tidy format grouped by condition
    
    Parameters:
    df (dataframe): fastq outcomes dataframe
    cond (str): Condition column name for grouping fastq outcomes dataframe
    wt (str): Expected wildtype nucleotide sequence (in frame AA)
    res (int): First AA number
    sample (str, optional): Sample column name for splicing fastq outcomes dataframe (Default: 'sample')
    edit (str, optional): Edit column name within fastq outcomes dataframe (Default: 'edit')
    psuedocount (int, optional): psuedocount to avoid log(0) & /0 (Default: 0)
    
    Dependencies: Bio.Seq.Seq, pandas, numpy, tidy, edit_1(), & aa_props
    '''
    wt_prot = Seq(wt).translate(table=1) # Obtain WT protein sequence
    wt_nums = np.arange(res,res+len(wt_prot))
    print('Isolate single aa change fastq outcomes')
    dc=t.split(edit_1(df),sample) # Isolate single aa change fastq outcomes and split by sample
    
    print('Fill with DMS grid data for each sample:')
    dc2=dict() # Fill with DMS grid data in tidy format split by sample
    for key_sample,df_sample in dc.items():
        print(key_sample)
        wt_fastq = df[(df['edit']=='WT')&(df[sample]==key_sample)] # Obtain WT fastq outcome
        df_sample_DMS=pd.DataFrame(columns=wt_fastq.columns) # Fill with DMS grid data in tidy format
        
        for num in wt_nums: # Iterate through WT protein sequence
            vals=dict() # Create dictionary with all amino acid changes for a given residue
            
            # Add metadata that is the same for all genotypes
            meta = [x for x in df_sample.columns if x not in [edit,'count','fraction','before','after','number']]
            for m in meta: 
                vals[m]=[wt_fastq[m].to_list()[0]]*len(list(aa_props.keys()))
            
            # Create all amino acid changes
            vals['before']=[wt_prot[num-res]]*len(list(aa_props.keys()))
            vals['number']=[num]*len(list(aa_props.keys()))
            vals['after']=list(aa_props.keys())
            vals[edit]=[vals['before'][i]+str(num)+vals['after'][i] for i in range(len(vals['after']))]

            # Fill in counts (+ psuedocount) for amino acid changes, WT, and none
            counts=[]
            num_mut = df_sample[df_sample['number']==num]
            for a in vals['after']:
                if a == wt_prot[num-res]: counts.append(wt_fastq['count'].to_list()[0]+psuedocount) # Wild type
                elif a in num_mut['after'].to_list(): counts.append(num_mut[num_mut['after']==a]['count'].to_list()[0]+psuedocount) # Amino acid change present
                else: counts.append(psuedocount) # Amino acid change absent
            vals['count']=counts
            sum_counts = sum(vals['count'])
            vals['fraction']=[count/sum_counts for count in vals['count']]

            df_sample_DMS = pd.concat([df_sample_DMS,pd.DataFrame(vals)]).reset_index(drop=True) # Append residue DMS data
        
        df_sample_DMS['number']=df_sample_DMS['number'].astype(int) # Set number as type int
        df_sample_DMS['count']=df_sample_DMS['count'].astype(int) # Set count as type int for plotting

        df_sample_DMS[sample] = [key_sample]*df_sample_DMS.shape[0]
        dc2[key_sample]=df_sample_DMS # Append sample DMS data

    print('Group samples by condition:')
    dc3=t.split(t.join(dc2,sample),cond) # Join samples back into 1 dataframe & split by condition
    df_cond_stat = pd.DataFrame()
    for key_cond,df_cond in dc3.items(): # Iterate through conditions
        print(key_cond)
        edit_ls = []
        fraction_avg_ls = []
        fraction_ls = []
        count_avg_ls = []
        before_ls = []
        after_ls = []
        number_ls = []
        for e in df_cond[edit]: # iterate through edits
            df_cond_edit = df_cond[df_cond[edit]==e]
            edit_ls.append(e)
            fraction_avg_ls.append(sum(df_cond_edit['fraction'])/len(df_cond_edit['fraction']))
            fraction_ls.append(df_cond_edit['fraction'].tolist())
            count_avg_ls.append(sum(df_cond_edit['count'])/len(df_cond_edit['count']))
            before_ls.append(df_cond_edit.iloc[0]['before'])
            after_ls.append(df_cond_edit.iloc[0]['after'])
            number_ls.append(df_cond_edit.iloc[0]['number'])
        df_cond_stat = pd.concat([df_cond_stat,
                                  pd.DataFrame({'edit':edit_ls,
                                                'before':before_ls,
                                                'after':after_ls,
                                                'number':number_ls,
                                                'fraction_ls':fraction_ls,
                                                'fraction_avg':fraction_avg_ls,
                                                'count_avg':count_avg_ls,
                                                cond:[key_cond]*len(number_ls)})])
    return df_cond_stat.drop_duplicates(subset=['edit','Description']).reset_index(drop=True)

def dms_comp(df: pd.DataFrame, cond: str, cond_comp: str, wt:str, res: int, sample='sample', edit='edit', psuedocount=1):
    ''' 
    dms_comp(): returns comparison DMS grid dataframe in tidy format split by condition
    
    Parameters:
    df (dataframe): fastq outcomes dataframe
    cond (str): Condition column name for grouping fastq outcomes dataframe
    cond_comp (str): Condition for comparison group
    wt (str): Expected wildtype nucleotide sequence (in frame AA)
    res (int): First AA number
    sample (str, optional): Sample column name for splicing fastq outcomes dataframe (Default: 'sample')
    edit (str, optional): Edit column name within fastq outcomes dataframe (Default: 'edit')
    psuedocount (int, optional): psuedocount to avoid log(0) & /0 (Default: 1)
    
    Dependencies: Bio.Seq.Seq, pandas, numpy, tidy, edit_1(), dms_cond(), & aa_props
    '''
    df_cond_stat = dms_cond(df,cond,wt,res,sample,edit,psuedocount) # Execute dms_cond()

    # Fold change & p-value relative comparison group
    print(f'Compute FC & pval relative to {cond_comp}:')
    df_stat = pd.DataFrame()
    df_comp = df_cond_stat[df_cond_stat[cond]==cond_comp] # Isolate comparison group
    df_other = df_cond_stat[df_cond_stat[cond]!=cond_comp] # From other groups
    for e in set(df_other[edit].tolist()): # iterate through edits
        print(f'{e}')
        df_other_edit = df_other[df_other[edit]==e]
        df_comp_edit = df_comp[df_comp[edit]==e]
        df_other_edit['fraction_avg_compare'] = [df_comp_edit.iloc[0]['fraction_avg']]*df_other_edit.shape[0]
        df_other_edit['count_avg_compare'] = [df_comp_edit.iloc[0]['count_avg']]*df_other_edit.shape[0]
        df_other_edit['FC'] = df_other_edit['fraction_avg']/df_comp_edit.iloc[0]['fraction_avg']
        ttests = [ttest_ind(other_fraction_ls,df_comp_edit.iloc[0]['fraction_ls']) 
                                 for other_fraction_ls in df_other_edit['fraction_ls']]
        df_other_edit['pval'] = [ttest[1] for ttest in ttests]
        df_other_edit['tstat'] = [ttest[0] for ttest in ttests]
        df_stat = pd.concat([df_stat,df_other_edit])
    df_stat['compare'] = [cond_comp]*df_stat.shape[0]
    return df_stat[[edit,'before','after','number','FC','pval','tstat','fraction_avg','fraction_avg_compare','count_avg','count_avg_compare',cond,'compare']].sort_values(by=['number','after']).reset_index(drop=True)

def subscript(df: pd.DataFrame,tick='before',tick_sub='number'):
    ''' 
    subscript(): returns dataframe with subscripts to tick labels
    
    Parameters:
    df (dataframe): dataframe
    tick (str, optional): new tick label column name
    tick_sub (str, optional): previous numeric tick label that will become a subscript

    Dependencies: pandas
    '''
    ticks = []
    labels = []
    for (t,ts) in set(zip(df[tick],df[tick_sub])):
        ticks.append(ts)
        labels.append('$\\mathrm{'+t+'_{'+str(ts)+'}}$')
    return pd.DataFrame({'tick':ticks,'label':labels}).sort_values(by='tick').reset_index(drop=True)

# Plot methods
def scat(typ: str,df: pd.DataFrame,x: str,y: str,cols=None,cols_ord=None,stys=None,cutoff=0.01,cols_exclude=None,
         file=None,dir=None,palette_or_cmap='colorblind',edgecol='black',
         figsize=(10,6),title='',title_size=18,title_weight='bold',title_font='Arial',
         x_axis='',x_axis_size=12,x_axis_weight='bold',x_axis_font='Arial',x_axis_scale='linear',x_axis_dims=(0,100),x_ticks_rot=0,x_ticks_font='Arial',x_ticks=[],
         y_axis='',y_axis_size=12,y_axis_weight='bold',y_axis_font='Arial',y_axis_scale='linear',y_axis_dims=(0,100),y_ticks_rot=0,y_ticks_font='Arial',y_ticks=[],
         legend_title='',legend_title_size=12,legend_size=9,legend_bbox_to_anchor=(1,1),legend_loc='upper left',legend_items=(0,0),show=True,
         **kwargs):
    '''
    scat(): creates scatter plot related graphs

    Parameters:
    typ (str): plot type (scat, line, line_scat)
    df (dataframe): pandas dataframe
    x (str): x-axis column name
    y (str): y-axis column name
    cols (str, optional): color column name
    cols_ord (list, optional): color column values order
    stys (str, optional): styles column name
    cols_exclude (list | str, optional): color column values exclude
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    palette_or_cmap (str, optional): seaborn color palette or matplotlib color map
    edgecol (str, optional): point edge color
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_scale (str, optional): x-axis scale linear, log, etc.
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_scale (str, optional): y-axis scale linear, log, etc.
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y_ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    show (bool, optional): show plot (Default: True)
    
    Dependencies: os, matplotlib, seaborn, & plot
    '''
    # Omit data smaller than cutoff or excluded
    df_cut=df[df[y]>=cutoff]
    df_other=df[df[y]<cutoff]
    for sample in list(df_other['sample'].value_counts().keys()):
        df_temp = df_other[df_other['sample']==sample]
        df_temp['fraction']=sum(df_temp['fraction'])
        df_temp['edit']='Other'
        df_cut = pd.concat([df_cut,df_temp.iloc[0].to_frame().T])

    # Omit excluded
    if type(cols_exclude)==list: 
        for exclude in cols_exclude: df_cut=df_cut[df_cut[cols]!=exclude]
    elif type(cols_exclude)==str: df_cut=df_cut[df_cut[cols]!=cols_exclude]

    # Sort data by genotype position
    if cols_ord==None:
        genotypes = list(df_cut[cols].value_counts().keys())
        positions = list()
        for geno in genotypes:
            numbers = re.findall(r'\d+\.?\d*', geno)
            if len(numbers)==0: positions.append(100000) # Places WT and Indel at the end
            else: positions.append(sum([int(n) for n in numbers])/len(numbers))
        assign = pd.DataFrame({'positions':positions,
                               'genotypes':genotypes})
        cols_ord = list(assign.sort_values(by='positions')['genotypes'])

    p.scat(typ=typ,df=df_cut,x=x,y=y,cols=cols,cols_ord=cols_ord,cols_exclude=cols_exclude,
           file=file,dir=dir,palette_or_cmap=palette_or_cmap,edgecol=edgecol,
           figsize=figsize,title=title,title_size=title_size,title_weight=title_weight,title_font=title_font,
           x_axis=x_axis,x_axis_size=x_axis_size,x_axis_weight=x_axis_weight,x_axis_font=x_axis_font,x_axis_scale=x_axis_scale,x_axis_dims=x_axis_dims,x_ticks_rot=x_ticks_rot,x_ticks_font=x_ticks_font,x_ticks=x_ticks,
           y_axis=y_axis,y_axis_size=y_axis_size,y_axis_weight=y_axis_weight,y_axis_font=y_axis_font,y_axis_scale=y_axis_scale,y_axis_dims=y_axis_dims,y_ticks_rot=y_ticks_rot,y_ticks_font=y_ticks_font,y_ticks=y_ticks,
           legend_title=legend_title,legend_title_size=legend_title_size,legend_size=legend_size,legend_bbox_to_anchor=legend_bbox_to_anchor,legend_loc=legend_loc,legend_items=legend_items,show=show, 
           **kwargs)

def cat(typ: str,df: pd.DataFrame,x: str,y: str,errorbar=None,cols=None,cols_ord=None,cutoff=0.01,cols_exclude=None,
        file=None,dir=None,palette_or_cmap='colorblind',edgecol='black',lw=1,
        figsize=(10,6),title='',title_size=18,title_weight='bold',title_font='Arial',
        x_axis='',x_axis_size=12,x_axis_weight='bold',x_axis_font='Arial',x_axis_scale='linear',x_axis_dims=(0,1),x_ticks_rot=0,x_ticks_font='Arial',x_ticks=[],
        y_axis='',y_axis_size=12,y_axis_weight='bold',y_axis_font='Arial',y_axis_scale='linear',y_axis_dims=(0,1),y_ticks_rot=0,y_ticks_font='Arial',y_ticks=[],
        legend_title='',legend_title_size=12,legend_size=9,legend_bbox_to_anchor=(1,1),legend_loc='upper left',legend_items=(0,0),show=True,
        **kwargs):
    ''' 
    cat: creates category dependent graphs
    
    Parameters:
    typ (str): plot type (bar, box, violin, swarm, strip, point, count, bar_swarm, box_swarm, violin_swarm)
    df (dataframe): pandas dataframe
    x (str, optional): x-axis column name
    y (str, optional): y-axis column name
    cols (str, optional): color column name
    cols_ord (list, optional): color column values order
    cols_exclude (list | str, optional): color column values exclude
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    palette_or_cmap (str, optional): seaborn color palette or matplotlib color map
    edgecol (str, optional): point edge color
    lw (int, optional): line width
    errorbar (str, optional): error bar type (sd)
    errwid (int, optional): error bar line width
    errcap (int, optional): error bar cap line width
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_scale (str, optional): x-axis scale linear, log, etc.
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_scale (str, optional): y-axis scale linear, log, etc.
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y_ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    show (bool, optional): show plot (Default: True)
    
    Dependencies: os, matplotlib, seaborn, & plot
    '''
    # Omit data smaller than cutoff or excluded
    df_cut=df[df[y]>=cutoff]
    df_other=df[df[y]<cutoff]
    for sample in list(df_other['sample'].value_counts().keys()):
        df_temp = df_other[df_other['sample']==sample]
        df_temp['fraction']=sum(df_temp['fraction'])
        df_temp['edit']='Other'
        df_cut = pd.concat([df_cut,df_temp.iloc[0].to_frame().T])
    
    # Omit excluded
    if type(cols_exclude)==list: 
        for exclude in cols_exclude: df_cut=df_cut[df_cut[cols]!=exclude]
    elif type(cols_exclude)==str: df_cut=df_cut[df_cut[cols]!=cols_exclude]

    # Sort data by genotype position
    if cols_ord==None:
        genotypes = list(df_cut[cols].value_counts().keys())
        positions = list()
        for geno in genotypes:
            numbers = re.findall(r'\d+\.?\d*', geno)
            if len(numbers)==0: positions.append(100000) # Places WT and Indel at the end
            else: positions.append(sum([int(n) for n in numbers])/len(numbers))
        assign = pd.DataFrame({'positions':positions,
                               'genotypes':genotypes})
        cols_ord = list(assign.sort_values(by='positions')['genotypes'])

    p.cat(typ=typ,df=df_cut,x=x,y=y,errorbar=errorbar,cols=cols,cols_ord=cols_ord,cols_exclude=cols_exclude,
          file=file,dir=dir,palette_or_cmap=palette_or_cmap,edgecol=edgecol,lw=lw,
          figsize=figsize,title=title,title_size=title_size,title_weight=title_weight,title_font=title_font,
          x_axis=x_axis,x_axis_size=x_axis_size,x_axis_weight=x_axis_weight,x_axis_font=x_axis_font,x_axis_scale=x_axis_scale,x_axis_dims=x_axis_dims,x_ticks_rot=x_ticks_rot,x_ticks_font=x_ticks_font,x_ticks=x_ticks,
          y_axis=y_axis,y_axis_size=y_axis_size,y_axis_weight=y_axis_weight,y_axis_font=y_axis_font,y_axis_scale=y_axis_scale,y_axis_dims=y_axis_dims,y_ticks_rot=y_ticks_rot,y_ticks_font=y_ticks_font,y_ticks=y_ticks,
          legend_title=legend_title,legend_title_size=legend_title_size,legend_size=legend_size,legend_bbox_to_anchor=legend_bbox_to_anchor,legend_loc=legend_loc,legend_items=legend_items,show=show, 
          **kwargs)

def stack(df: pd.DataFrame,x='sample',y='fraction',cols='edit',cutoff=0.01,cols_ord=[],x_ord=[],
          file=None,dir=None,cmap='Set2',errcap=4,vertical=True,
          figsize=(10,6),title='Editing Outcomes',title_size=18,title_weight='bold',title_font='Arial',
          x_axis='',x_axis_size=12,x_axis_weight='bold',x_axis_font='Arial',x_ticks_rot=0,x_ticks_font='Arial',
          y_axis='',y_axis_size=12,y_axis_weight='bold',y_axis_font='Arial',y_ticks_rot=0,y_ticks_font='Arial',
          legend_title='',legend_title_size=12,legend_size=12,
          legend_bbox_to_anchor=(1,1),legend_loc='upper left',legend_ncol=1,show=True,space_capitalize=True,**kwargs):
    ''' 
    stack(): creates stacked bar plot

    Parameters:
    df (dataframe): pandas dataframe
    x (str, optional): x-axis column name
    y (str, optional): y-axis column name
    cols (str, optional): color column name
    cutoff (float, optional): y-axis values needs be greater than (e.g. 0.01)
    cols_ord (list, optional): color column values order
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    cmap (str, optional): matplotlib color map
    errcap (int, optional): error bar cap line width
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-ticks font
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: re, os, pandas, numpy, matplotlib.pyplot & plot
    '''
    # Omit smaller than cutoff and convert it to <cutoff
    df_cut=df[df[y]>=cutoff]
    df_other=df[df[y]<cutoff]
    for sample in list(df_other['sample'].value_counts().keys()):
        df_temp = df_other[df_other['sample']==sample]
        df_temp['fraction']=sum(df_temp['fraction'])
        df_temp['edit']=f'<{cutoff}'
        df_cut = pd.concat([df_cut,df_temp.iloc[:1]])

    # Sort pivot table columns by genotype position
    if cols_ord==[]:
        genotypes = list(df_cut[cols].value_counts().keys())
        positions = list()
        for geno in genotypes:
            numbers = re.findall(r'\d+\.?\d*', geno)
            if len(numbers)==0: positions.append(100000) # Places WT and Indel at the end
            else: positions.append(sum([int(n) for n in numbers])/len(numbers))
        assign = pd.DataFrame({'positions':positions,
                               'genotypes':genotypes})
        cols_ord = list(assign.sort_values(by='positions')['genotypes'])
    
    # Make stacked barplot
    p.stack(df=df_cut,x=x,y=y,cols=cols,cutoff=0,cols_ord=cols_ord,x_ord=x_ord,
            file=file,dir=dir,cmap=cmap,errcap=errcap,vertical=vertical,
            figsize=figsize,title=title,title_size=title_size,title_weight=title_weight,title_font=title_font,
            x_axis=x_axis,x_axis_size=x_axis_size,x_axis_weight=x_axis_weight,x_axis_font=x_axis_font,x_ticks_rot=x_ticks_rot,x_ticks_font=x_ticks_font,
            y_axis=y_axis,y_axis_size=y_axis_size,y_axis_weight=y_axis_weight,y_axis_font=y_axis_font,y_ticks_rot=y_ticks_rot,y_ticks_font=y_ticks_font,
            legend_title=legend_title,legend_title_size=legend_title_size,legend_size=legend_size,
            legend_bbox_to_anchor=legend_bbox_to_anchor,legend_loc=legend_loc,legend_ncol=legend_ncol,show=show,space_capitalize=space_capitalize,**kwargs)

def heat(df: pd.DataFrame, cond: str,x='number',y='after',vals='fraction_avg',vals_dims:tuple=None,
         file=None,dir=None,edgecol='black',lw=1,annot=False,cmap="bone_r",sq=True,cbar=True,
         title='',title_size=12,title_weight='bold',title_font='Arial',figsize=(20,7),
         x_axis='',x_axis_size=12,x_axis_weight='bold',x_axis_font='Arial',x_ticks_rot=45,x_ticks_font='Arial',
         y_axis='',y_axis_size=12,y_axis_weight='bold',y_axis_font='Arial',y_ticks_rot=0,y_ticks_font='Arial',
         show=True,space_capitalize=True,**kwargs):
    ''' 
    heat(): creates heatmap
    
    Parameters:
    df (dataframe): tidy-formatted DMS dataframe (dms_cond() or dms_comp())
    x (str, optional): x-axis column name (AA residues number column)
    y (str, optional): y-axis column name (AA change column)
    vals (str, optional): values column name
    vals_dims (tuple, optional): vals minimum and maximum formatted (vmin, vmax; Default: None)
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    edgecol (str, optional): point edge color
    lw (int, optional): line width
    annot (bool, optional): annotate values
    cmap (str, optional): matplotlib color map
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    figsize (tuple, optional): figure size per subplot
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_ticks_font (str, optional): x-ticks font
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y-ticks font
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: matplotlib, seaborn, pandas, & aa_props
    '''
    # Find min and max values in the dataset for normalization
    if vals_dims is None:
        vmin = df[vals].values.min()
        vmax = df[vals].values.max()
    else:
        vmin = vals_dims[0]
        vmax = vals_dims[1]

    # Make DMS grids
    print('Make DMS grids')
    dc=t.split(df,cond) # Split by condition
    dc2={key:pd.pivot(df_cond,columns=x,index=y,values=vals).astype(float).reindex(list(aa_props.keys())) 
         for key,df_cond in dc.items()} # Generate pivot tables
    
    # Create a single figure with multiple heatmap subplots
    print('Create a single figure with multiple heatmap subplots')
    fig, axes = plt.subplots(nrows=len(list(dc2.keys())),ncols=1,figsize=(figsize[0],figsize[1]*len(list(dc2.keys()))),sharex=False,sharey=True)
    if isinstance(axes, np.ndarray)==False: axes = np.array([axes]) # Make axes iterable if there is only 1 heatmap
    for (ax, key) in zip(axes, list(dc2.keys())):
        print(f'{key}')
        sns.heatmap(dc2[key],annot=annot,cmap=cmap,ax=ax,linecolor=edgecol,linewidths=lw,cbar=cbar,square=sq,vmin=vmin,vmax=vmax, **kwargs)
        if len(list(dc2.keys()))>1: ax.set_title(key,fontsize=title_size,fontweight=title_weight,fontfamily=title_font)  # Add title to subplot
        else: ax.set_title(title,fontsize=title_size,fontweight=title_weight,fontfamily=title_font)
        if x_axis=='': 
            if space_capitalize: ax.set_xlabel(p.re_un_cap(x),fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font) # Add x axis label
            else: ax.set_xlabel(x,fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font) # Add x axis label
        else: ax.set_xlabel(x_axis,fontsize=x_axis_size,fontweight=x_axis_weight,fontfamily=x_axis_font)
        if y_axis=='': 
            if space_capitalize: ax.set_ylabel(p.re_un_cap(y),fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font) # Add y axis label
            else: ax.set_ylabel(y,fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font) # Add y axis label
        else: ax.set_ylabel(y_axis,fontsize=y_axis_size,fontweight=y_axis_weight,fontfamily=y_axis_font)
        ax.set_xticklabels(subscript(dc[key])['label'].to_list()) # Change x ticks to have subscript format
        # Format x ticks
        if (x_ticks_rot==0)|(x_ticks_rot==90): plt.setp(ax.get_xticklabels(), rotation=x_ticks_rot, ha="center",rotation_mode="anchor",fontname=x_ticks_font) 
        else: plt.setp(ax.get_xticklabels(), rotation=x_ticks_rot, ha="right",rotation_mode="anchor",fontname=x_ticks_font) 
        # Format y ticks
        plt.setp(ax.get_yticklabels(), rotation=y_ticks_rot, va='center', ha="right",rotation_mode="anchor",fontname=y_ticks_font)
        ax.set_facecolor('white')  # Set background to transparent

    # Save & show fig
    if file is not None and dir is not None:
        io.mkdir(dir) # Make output directory if it does not exist
        plt.savefig(fname=os.path.join(dir, file), dpi=600, bbox_inches='tight', format=f'{file.split(".")[-1]}')
    if show: plt.show()

def vol(df: pd.DataFrame,x: str,y: str,size:str=None,size_dims:tuple=None,include_wt=False,
        FC_threshold=2,pval_threshold=0.05,file=None,dir=None,color='lightgray',alpha=0.5,edgecol='black',vertical=True,
        figsize=(10,6),title='',title_size=18,title_weight='bold',title_font='Arial',
        x_axis='',x_axis_size=12,x_axis_weight='bold',x_axis_font='Arial',x_axis_dims=(0,0),x_ticks_rot=0,x_ticks_font='Arial',x_ticks=[],
        y_axis='',y_axis_size=12,y_axis_weight='bold',y_axis_font='Arial',y_axis_dims=(0,0),y_ticks_rot=0,y_ticks_font='Arial',y_ticks=[],
        legend_title='',legend_title_size=12,legend_size=9,legend_bbox_to_anchor=(1,1),legend_loc='upper left',
        legend_items=(0,0),legend_ncol=1,display_size=True,display_labels=True,return_df=True,show=True,space_capitalize=True,
        **kwargs):
    ''' 
    vol(): creates volcano plot
    
    Parameters:
    df (dataframe): pandas dataframe
    x (str): x-axis column name (FC)
    y (str): y-axis column name (pval)
    cols (str, optional): color column name
    size (str, optional): size column name
    size_dims (tuple, optional): (minimum,maximum) values in size column (Default: None)
    include_wt (bool, optional): include wildtype (Default: False)
    FC_threshold (float, optional): fold change threshold (Default: 2; log2(2)=1)
    pval_threshold (float, optional): p-value threshold (Default: 0.05; -log10(0.05)=1.3)
    file (str, optional): save plot to filename
    dir (str, optional): save plot to directory
    color (str, optional): matplotlib color for nonsignificant values
    alpha (float, optional): transparency for nonsignificant values (Default: 0.5)
    edgecol (str, optional): point edge color
    vertical (bool, optional): vertical orientation; otherwise horizontal (Default: True)
    figsize (tuple, optional): figure size
    title (str, optional): plot title
    title_size (int, optional): plot title font size
    title_weight (str, optional): plot title bold, italics, etc.
    title_font (str, optional): plot title font
    x_axis (str, optional): x-axis name
    x_axis_size (int, optional): x-axis name font size
    x_axis_weight (str, optional): x-axis name bold, italics, etc.
    x_axis_font (str, optional): x-axis font
    x_axis_dims (tuple, optional): x-axis dimensions (start, end)
    x_ticks_rot (int, optional): x-axis ticks rotation
    x_axis_font (str, optional): x-axis font
    x_ticks (list, optional): x-axis tick values
    y_axis (str, optional): y-axis name
    y_axis_size (int, optional): y-axis name font size
    y_axis_weight (str, optional): y-axis name bold, italics, etc.
    y_axis_font (str, optional): y-axis font
    y_axis_dims (tuple, optional): y-axis dimensions (start, end)
    y_ticks_rot (int, optional): y-axis ticks rotation
    y_ticks_font (str, optional): y_ticks font
    y_ticks (list, optional): y-axis tick values
    legend_title (str, optional): legend title
    legend_title_size (str, optional): legend title font size
    legend_size (str, optional): legend font size
    legend_bbox_to_anchor (tuple, optional): coordinates for bbox anchor
    legend_loc (str): legend location
    legend_ncol (tuple, optional): # of columns
    display_size (bool, optional): display size on plot (Default: True)
    display_labels (bool, optional): display labels for significant values (Default: True)
    return_df (bool, optional): return dataframe (Default: True)
    show (bool, optional): show plot (Default: True)
    space_capitalize (bool, optional): use re_un_cap() method when applicable (Default: True)
    
    Dependencies: os, matplotlib, seaborn, pandas, & edit_1()
    '''
    # Strings with subscripts
    log2 = 'log\u2082'
    log10 = 'log\u2081\u2080'
    
    # Log transform data
    df[f'{log2}({x})'] = [np.log10(xval)/np.log10(2) for xval in df[x]]
    df[f'-{log10}({y})'] = [-np.log10(yval) for yval in df[y]]
    
    # Organize data by significance
    signif = []
    for (log2FC,log10P) in zip(df[f'{log2}({x})'],df[f'-{log10}({y})']):
        if (np.abs(log2FC)>1)&(log10P>-np.log10(0.05)): signif.append('FC & p-value')
        elif (np.abs(log2FC)<=1)&(log10P>-np.log10(0.05)): signif.append('p-value')
        elif (np.abs(log2FC)>1)&(log10P<=-np.log10(0.05)): signif.append('FC')
        else: signif.append('NS')
    df['Significance']=signif
    #signif_order = ['NS','FC','p-value','FC & p-value']

    # Organize data by conservation (changed from)
    basic = ['R','K', 'H']
    acidic = ['D','E']
    polar = ['S', 'T', 'N', 'Q', 'Y', 'C']
    nonpolar = ['A','V','L','I','M','F','W','P','G']
    change = []

    df = edit_1(df)
    for (before,after) in zip(df['before'],df['after']):
        if (before in basic)&(after not in basic): change.append('Basic')
        elif (before in acidic)&(after not in acidic): change.append('Acidic')
        elif (before in polar)&(after not in polar): change.append('Polar')
        elif (before in nonpolar)&(after not in nonpolar): change.append('Nonpolar')
        else: change.append('Conserved')
    df['Change'] = change

    sty_order = ['Conserved','Basic','Acidic','Polar','Nonpolar']
    mark_order = ['D','^','v','<','>']

    # Remove wildtype
    if include_wt==False:
        wt_i = [i for i,(before,after) in enumerate(t.zip_cols(df=df,cols=['before','after'])) if before == after]
        df = df.drop(wt_i,axis=0).reset_index(drop=True)

    # Organize data by abundance
    sizes=(1,100)
    if size_dims is not None: df = df[(df[size]>=size_dims[0])&(df[size]<=size_dims[1])]

    # Set dimensions
    if x_axis_dims==(0,0): x_axis_dims=(min(df[f'{log2}({x})']),max(df[f'{log2}({x})']))
    if y_axis_dims==(0,0): y_axis_dims=(0,max(df[f'-{log10}({y})']))

    # Generate figure
    fig, ax = plt.subplots(figsize=figsize)
    
    if vertical: # orientation
        # with significance boundraries
        plt.vlines(x=-np.log10(FC_threshold)/np.log10(2), ymin=y_axis_dims[0], ymax=y_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)
        plt.vlines(x=np.log10(FC_threshold)/np.log10(2), ymin=y_axis_dims[0], ymax=y_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)
        plt.hlines(y=-np.log10(pval_threshold), xmin=x_axis_dims[0], xmax=x_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)
    
        # with data
        if display_size==False: size=None
        sns.scatterplot(data=df[df['Significance']!='FC & p-value'], x=f'{log2}({x})', y=f'-{log10}({y})', 
                        edgecolor=edgecol, color=color, alpha=alpha, style='Change',
                        style_order=sty_order, markers=mark_order, 
                        size=size, sizes=sizes,
                        ax=ax, **kwargs)
        sns.scatterplot(data=df[(df['Significance']=='FC & p-value')&(df[f'{log2}({x})']<0)], 
                        x=f'{log2}({x})', y=f'-{log10}({y})', 
                        hue=f'{log2}({x})',
                        edgecolor=edgecol, palette='Blues_r', style='Change',
                        style_order=sty_order, markers=mark_order,
                        size=size, sizes=sizes, legend=False,
                        ax=ax, **kwargs)
        sns.scatterplot(data=df[(df['Significance']=='FC & p-value')&(df[f'{log2}({x})']>0)], 
                        x=f'{log2}({x})', y=f'-{log10}({y})', 
                        hue=f'{log2}({x})',
                        edgecolor=edgecol, palette='Reds', style='Change',
                        style_order=sty_order, markers=mark_order,
                        size=size, sizes=sizes, legend=False,
                        ax=ax, **kwargs)
        
        # with labels
        if display_labels:
            df_signif = df[df['Significance']=='FC & p-value']
            adjust_text([plt.text(x=df_signif.iloc[i][f'{log2}({x})'], 
                                  y=df_signif.iloc[i][f'-{log10}({y})'],
                                  s=edit) for i,edit in enumerate(df_signif['edit'])])
        
        # Set x axis
        if x_axis=='': x_axis=f'{log2}({x})'
        plt.xlabel(x_axis, fontsize=x_axis_size, fontweight=x_axis_weight,fontfamily=x_axis_font)
        if x_ticks==[]: 
            if (x_ticks_rot==0)|(x_ticks_rot==90): plt.xticks(rotation=x_ticks_rot,ha='center',fontfamily=x_ticks_font)
            else: plt.xticks(rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font)
        else: 
            if (x_ticks_rot==0)|(x_ticks_rot==90): plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot, ha='center',fontfamily=x_ticks_font)
            else: plt.xticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,ha='right',fontfamily=x_ticks_font)

        # Set y axis
        if y_axis=='': y_axis=f'-{log10}({y})'
        plt.ylabel(y_axis, fontsize=y_axis_size, fontweight=y_axis_weight,fontfamily=y_axis_font)

        if y_ticks==[]: plt.yticks(rotation=y_ticks_rot,fontfamily=y_ticks_font)
        else: plt.yticks(ticks=y_ticks,labels=y_ticks,rotation=y_ticks_rot,fontfamily=y_ticks_font)

    else: # Horizontal orientation
        # with significance boundraries
        plt.hlines(y=-np.log10(FC_threshold)/np.log10(2), xmin=y_axis_dims[0], xmax=y_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)
        plt.hlines(y=np.log10(FC_threshold)/np.log10(2), xmin=y_axis_dims[0], xmax=y_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)
        plt.vlines(x=-np.log10(pval_threshold), ymin=x_axis_dims[0], ymax=x_axis_dims[1], colors='k', linestyles='dashed', linewidth=1)

        # with data
        if display_size==False: size=None
        sns.scatterplot(data=df[df['Significance']!='FC & p-value'], y=f'{log2}({x})', x=f'-{log10}({y})', 
                        edgecolor=edgecol, color=color, alpha=alpha, style='Change',
                        style_order=sty_order, markers=mark_order, 
                        size=size, sizes=sizes,
                        ax=ax, **kwargs)
        sns.scatterplot(data=df[(df['Significance']=='FC & p-value')&(df[f'{log2}({x})']<0)], 
                        y=f'{log2}({x})', x=f'-{log10}({y})', 
                        hue=f'{log2}({x})',
                        edgecolor=edgecol, palette='Blues_r', style='Change',
                        style_order=sty_order, markers=mark_order,
                        size=size, sizes=sizes, legend=False,
                        ax=ax, **kwargs)
        sns.scatterplot(data=df[(df['Significance']=='FC & p-value')&(df[f'{log2}({x})']>0)], 
                        y=f'{log2}({x})', x=f'-{log10}({y})', 
                        hue=f'{log2}({x})',
                        edgecolor=edgecol, palette='Reds', style='Change',
                        style_order=sty_order, markers=mark_order,
                        size=size, sizes=sizes, legend=False,
                        ax=ax, **kwargs)
        
        # with labels
        if display_labels:
            df_signif = df[df['Significance']=='FC & p-value']
            adjust_text([plt.text(y=df_signif.iloc[i][f'{log2}({x})'], 
                                  x=df_signif.iloc[i][f'-{log10}({y})'],
                                  s=edit) for i,edit in enumerate(df_signif['edit'])])
        
        # Set x axis
        if y_axis=='': y_axis=f'-{log10}({y})'
        plt.xlabel(y_axis, fontsize=y_axis_size, fontweight=y_axis_weight,fontfamily=y_axis_font)
        if y_ticks==[]: 
            if (y_ticks_rot==0)|(y_ticks_rot==90): plt.xticks(rotation=y_ticks_rot,ha='center',fontfamily=y_ticks_font)
            else: plt.xticks(rotation=y_ticks_rot,ha='right',fontfamily=y_ticks_font)
        else: 
            if (y_ticks_rot==0)|(y_ticks_rot==90): plt.xticks(ticks=y_ticks,labels=y_ticks,rotation=y_ticks_rot, ha='center',fontfamily=y_ticks_font)
            else: plt.xticks(ticks=y_ticks,labels=y_ticks,rotation=y_ticks_rot,ha='right',fontfamily=y_ticks_font)

        # Set y axis
        if x_axis=='': x_axis=f'{log2}({x})'
        plt.ylabel(x_axis, fontsize=x_axis_size, fontweight=x_axis_weight,fontfamily=x_axis_font)

        if x_ticks==[]: plt.yticks(rotation=x_ticks_rot,fontfamily=x_ticks_font)
        else: plt.yticks(ticks=x_ticks,labels=x_ticks,rotation=x_ticks_rot,fontfamily=x_ticks_font)

    # Set title
    if title=='' and file is not None: 
        if space_capitalize: title=p.re_un_cap(".".join(file.split(".")[:-1]))
        else: ".".join(file.split(".")[:-1])
    plt.title(title, fontsize=title_size, fontweight=title_weight, family=title_font)

    # Move legend to the right of the graph
    if legend_items==(0,0): ax.legend(title=legend_title,title_fontsize=legend_title_size,fontsize=legend_size,
                                        bbox_to_anchor=legend_bbox_to_anchor,loc=legend_loc,ncol=legend_ncol)
    else: 
        handles, labels = ax.get_legend_handles_labels()
        ax.legend(title=legend_title,title_fontsize=legend_title_size,fontsize=legend_size,
                  bbox_to_anchor=legend_bbox_to_anchor,loc=legend_loc,ncol=legend_ncol, # Move right of the graph
                  handles=handles[legend_items[0]:legend_items[1]],labels=labels[legend_items[0]:legend_items[1]]) # Only retains specified labels

    # Save & show fig; return dataframe
    if file is not None and dir is not None:
        io.mkdir(dir) # Make output directory if it does not exist
        plt.savefig(fname=os.path.join(dir, file), dpi=600, bbox_inches='tight', format=f'{file.split(".")[-1]}')
    if show: plt.show()
    if return_df: return df     